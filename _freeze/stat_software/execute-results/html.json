{
  "hash": "53c7fe81970ebf54cae24b247397fa34",
  "result": {
    "engine": "knitr",
    "markdown": "# Statistical Software {#sec-stat_software}\n\n\n\nIn the past, researchers had to manually code their own statistical analyses, which was tedious and error-prone. Today, statistical software simplifies this process dramatically. Researchers shift their focus from the technical complexities of computation to understanding statistical logic and applying analyses correctly.\n\nThis section offers guidance on selecting appropriate statistical programming language, walks through the set-up process, and introduces the basics of conducting statistical analyses using modern tools.\n\n## Statistical software options: {#sec-options}\n\nR, Python, and Stata are the 3 most commonly used languages.\n\n|  |  |  |  |\n|------------------|------------------|------------------|-------------------|\n|  | **R** | **Python** | **Stata** |\n| **Cost** | Free | Free | Requires License |\n| **IDE** | RStudio/Posit | Many, Visual Code is good | Built in editor |\n| **Strengths** | Best libraries for epidemiology, trial statistics. | Best libraries for text processing, machine learning, AI integration | Simple syntax; powerful quasi-experimental/meta-analysis packages. Used by U of U MSCI. |\n| **Weakness** | Clunky syntax; many 'dialects' | Overkill for many, complex development environment | Clunkiest machine learning, explainable programing, cost. |\n| <a class='glossary'>Explainable programming<span class='def'>Writing analytic code in a clear, structured, and understandable way so that readers (including reviewers, collaborators, and future researchers) can easily follow and replicate the steps of your analysis. In quantitative disciplines such as mathematics and computer science, providing such code is already standard practice. Medicine is beginning to adopt this approach, recognizing its importance for transparency and reproducibility.</span></a> | Quarto | Jupyter, Quarto | Not native (though can use Jupyter) |\n\nThere are a few other language options (SPSS, SAS, Julia, etc.), but they are omitted for brevity (generally, not the best modern options)\n\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Explainable programming </td>\n   <td style=\"text-align:left;\"> Writing analytic code in a clear, structured, and understandable way so that readers (including reviewers, collaborators, and future researchers) can easily follow and replicate the steps of your analysis. In quantitative disciplines such as mathematics and computer science, providing such code is already standard practice. Medicine is beginning to adopt this approach, recognizing its importance for transparency and reproducibility. </td>\n  </tr>\n</tbody>\n</table>\n\n\n\n## How to install {#sec-install}\n\nChoose the tab for the language(s) you plan to use:\n\n### Instructions:\n\n::: panel-tabset\n## R\n\n|  |  |  |  |\n|------------------|------------------|------------------|-------------------|\n| 1: | Install R Language | <https://cran.r-project.org/> | This installs the base programming language |\n| 2: | Install RStudio | <https://posit.co/downloads/> | RStudio is an <a class='glossary'>IDE<span class='def'>Integrated Development Environment, a program that allows for writing, running, and debugging code within a single program.</span></a> (integrated development environment) that allows you to write, execute, and debug code from within a single program. |\n| 3: | Install Quarto (formerly Markdown) | <https://quarto.org/docs/get-started/> | Facilitates sharing and explaining your code. |\n\n## Python\n\n<table style=\"width:94%;\">\n<colgroup>\n<col style=\"width: 3%\" />\n<col style=\"width: 17%\" />\n<col style=\"width: 42%\" />\n<col style=\"width: 30%\" />\n</colgroup>\n<tbody>\n<tr>\n<td><p>1:</p></td>\n<td><p>Install Python Language and dependencies</p></td>\n<td><p><a href=\"https://github.com/conda-forge/miniforge?tab=readme-ov-file\">https://github.com/conda-forge/miniforge?tab=readme-ov-file</a></p></td>\n<td><p>This (mini-forge) installs the base Python programming language, the things it depends on, and many useful packages</p></td>\n</tr>\n<tr>\n<td><p>2:</p></td>\n<td><p>Create an environment</p></td>\n<td><p>Execute the following commands in a terminal:</p>\n<p><code>mamba create -n stats python=3.12 numpy pandas scipy statsmodels scikit-learn lifelines pingouinmatplotlib seaborn plotnine pyreadstatjupyterlab ipykernel jupytext --channel conda-forge</code></p>\n<p><code>conda activate stats</code></p></td>\n<td><p>This sets up an environment (controls the versions and packages that are used).</p></td>\n</tr>\n<tr>\n<td><p>3:</p></td>\n<td><p>Install Visual Code</p></td>\n<td><p><a href=\"https://code.visualstudio.com/download\">https://code.visualstudio.com/download</a></p></td>\n<td><p>Visual Code is an IDE (integrated development environment) that allows you to write, execute, and debug code from within a single program.</p></td>\n</tr>\n</tbody>\n</table>\n\n\\*Note: there are many <a class='glossary'>IDE<span class='def'>Integrated Development Environment, a program that allows for writing, running, and debugging code within a single program.</span></a>. Visual Code is a classic one with a lot of functionality, though there are AI enabled ones (e.g. Cursor) that may be more helpful depending on how much programming you plan to do and whether you want to bother with the added complexity (discussed more in @sec-llm).\n\nYou can also use Quarto for explainable programming in Python - but Jupyter is a more common workflow so we focus on that.\n\n## Stata\n\n|  |  |  |  |\n|------------------|------------------|------------------|-------------------|\n| 1: | Get a product key | if U of U Trainee, contact me | This verifies you or your institutions' purchase |\n| 2: | Install STATA | <https://www.stata.com/install-guide/> | Includes language and <a class='glossary'>IDE<span class='def'>Integrated Development Environment, a program that allows for writing, running, and debugging code within a single program.</span></a> (integrated development environment) |\n:::\n\n## Packages {#sec-packages}\n\nFor common statistical analyses in any of these languages, specialized *packages* already exist that handle these tasks efficiently. Whenever you find yourself manually calculating or coding a statistical procedure, consider that someone likely has already written reliable, tested code that will perform the analysis faster and more accurately. You'll want to use these packages whenever possible.\n\n\n\nFirst, a few terms: <a class='glossary'>function<span class='def'>A reusable piece of code that performs a specific task. Examples include calculating the mean of a dataset or running logistic regression.</span></a>, <a class='glossary'>arguments<span class='def'>Inputs provided to a function so it can perform its task. For instance, a function calculating a mean needs a dataset, while logistic regression requires data, the outcome variable, and predictor variables.</span></a>, <a class='glossary'>package<span class='def'>A curated collection of functions designed to accomplish related tasks. Programming languages come pre-installed with basic packages, but you’ll often download additional packages to access specialized functions. Each language provides straightforward ways to locate and install new packages.</span></a>\n\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> definition </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> arguments </td>\n   <td style=\"text-align:left;\"> Inputs provided to a function so it can perform its task. For instance, a function calculating a mean needs a dataset, while logistic regression requires data, the outcome variable, and predictor variables. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> function </td>\n   <td style=\"text-align:left;\"> A reusable piece of code that performs a specific task. Examples include calculating the mean of a dataset or running logistic regression. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> package </td>\n   <td style=\"text-align:left;\"> A curated collection of functions designed to accomplish related tasks. Programming languages come pre-installed with basic packages, but you’ll often download additional packages to access specialized functions. Each language provides straightforward ways to locate and install new packages. </td>\n  </tr>\n</tbody>\n</table>\n\n\n\n### Finding Packages\n\n::: panel-tabset\n## R\n\n|  |  |\n|---------------------------|------------------------------|\n| Where to find packages? | <https://cran.r-project.org/web/views/> |\n| Command to install packages | `install.packages(`\\``package_name`\\``)` |\n| How to access documentation file? | `?package_name` or `?command_name` |\n\n## Python\n\n<table style=\"width:78%;\">\n<colgroup>\n<col style=\"width: 24%\" />\n<col style=\"width: 53%\" />\n</colgroup>\n<tbody>\n<tr>\n<td><p>Where to find packages?</p></td>\n<td><ol type=\"1\">\n<li><p><a href=\"https://anaconda.org/conda-forge\">https://anaconda.org/conda-forge</a></p></li>\n<li><p><a href=\"https://pypi.org/\">https://pypi.org/</a></p></li>\n<li><p><a href=\"https://github.com/topics/python\">https://github.com/topics/python</a></p></li>\n</ol></td>\n</tr>\n<tr>\n<td><p>Command to install packages</p></td>\n<td><p><code>mamba install package_name</code> (rarely, packages that have not been compiled on conda-forge will require <code>pip install package_name</code> to be used)</p></td>\n</tr>\n<tr>\n<td><p>How to access documentation file?</p></td>\n<td><p>The project page on <a href=\"https://pypi.org/\">https://pypi.org/</a> or <a href=\"https://github.com/\">https://github.com/</a></p></td>\n</tr>\n</tbody>\n</table>\n\n## Stata\n\n|                                   |                            |\n|-----------------------------------|----------------------------|\n| Where to find packages?           | `findit package_name`      |\n| Command to install packages       | `ssc install package_name` |\n| How to access documentation file? | `help package_name`        |\n:::\n\n## Reproducible Research {#sec-reproducible}\n\nAs mentioned before, <a class='glossary'>Explainable programming<span class='def'>Writing analytic code in a clear, structured, and understandable way so that readers (including reviewers, collaborators, and future researchers) can easily follow and replicate the steps of your analysis. In quantitative disciplines such as mathematics and computer science, providing such code is already standard practice. Medicine is beginning to adopt this approach, recognizing its importance for transparency and reproducibility.</span></a> is an increasingly important idea because programming errors frequently lead to erroneous research results. Understandable analytic code is important for co-authors to understand and verify what you've done, as well as facilitating replication and improvements.\n\nImportantly (and, as will be discussed in @sec-data), it's not just the statistical design choices that can have an influence on the observed outcomes. Researchers must decide what data should be included, how the data should be cleaned, whether missing values will be imputed (and if so, how). All this occurs before talk of statistical tests, regressions, or presentation occur.\n\nBy some estimates, variation in how these pre-processing tasks are done accounts for more variability in findings than the design choices most readers focus on. Accordingly, the data processing code that is shared should include the entire process, from data cleaning to figure generation.\n\n### How to document and share code \n\n::::::::: panel-tabset\n## R\n\nIn R, the easiest way to put together understandable and sharable code is to use Quarto documents (this website is actually is generated in Quarto).\n\nQuarto creates notebooks that include segments that can contain text and pictures with other sections that contain executable R code. The segments containing R code can be executed in subsequent order - variables are shared between the boxes - thus allowing a long analytic pipeline to be broken up into small understandable chunks.\n\n::: callout-tip\nTo create a new Quarto notebook, go to File-\\>New-\\>Quarto Document (\\*.qmd). Leave all the options in their default. **Try it to make the below notebook.**\n:::\n\nFor example, a hypothetical analysis might go something like:\n\n#### Example Simulation of Coin Flips:\n\nOur aim is to simulate the number of heads seen if a coin is flipped 50 times.\n\nFirst, we create a function that simulates a fair coin flip: 50% probability heads (coded as 1), 50% probability of tails (coded as 0)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## one fair-coin flip: 1 = Heads, 0 = Tails\nflip_coin <- function(n = 1) {\n  sample(c(0, 1), size = n, replace = TRUE)\n}\n```\n:::\n\n\nThen, we write a program that repesents a single simulation: the coin is flipped 50 times\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## run one experiment of 50 flips and return #Heads\nsimulate_50 <- function() {\n  sum(flip_coin(50))\n}\n\n## quick demo\nset.seed(42)   # reproducible example\nsimulate_50()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 27\n```\n\n\n:::\n:::\n\n\nThen, we perform 1000 simulations and store the results in an array, which we then present summary statistics on.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)                       # reproducible simulations\nn_sims  <- 1000                    # how many experiments?\nresults <- replicate(n_sims, simulate_50())\n\nsummary(results)                   # five-number summary\nhist(results,\n     breaks = 20,\n     main   = \"Distribution of heads in 1 000 × 50-flip experiments\",\n     xlab   = \"Number of heads\") \n```\n\n::: {.cell-output-display}\n![](stat_software_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=100%}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  14.00   22.00   25.00   24.74   27.00   35.00 \n```\n\n\n:::\n:::\n\n\n#### Nuts and Bolts\n\nObviously, this example is a bit contrived, but is meant to show how the text and code can be interleaved to make the resulting output easy to understand. This quarto file can then be shared with collaborators (or posted on a code sharing site, such as github - provided none of the protected health information is hard coded into the text.\n\n::: callout-tip\nAs an exercise, see if you can create your own Quarto markdown file, and extend the analysis to generate the interquartile range for the mean number of Heads in 50 flips.\n:::\n\n## Python\n\nIn Python, the easiest way to put together understandable and sharable code is to use Jupyter notebooks.\n\nJupyter (like Quarto) creates notebooks that include segments that can contain text and pictures with other sections that contain executable Python code. The segments containing Python code can be executed in subsequent order - variables are shared between the boxes - thus allowing a long analytic pipeline to be broken up into small understandable chunks.\n\n::: callout-tip\nTo create a new Jupyter notebook, go to File -\\> New Jupyter Notebook (\\*.ipynb). **Try it to make the below notebook.**\n:::\n\nFor example, a hypothetical analysis might go something like:\n\n#### Example Simulation of 50 Coin Flips\n\nOur aim is to simulate the number of heads seen if a coin is flipped 50 times.\n\nFirst, we create a function that simulates a fair coin flip: 50% probability heads (coded as 1), 50% probability of tails (coded as 0)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport random\n\ndef flip_coin(n: int = 1) -> list[int]:\n    \"\"\"Return a list of 0 / 1 draws; 1 = heads, 0 = tails.\"\"\"\n    return random.choices([0, 1], k=n)\n```\n:::\n\n\nThen, we write a program that repesents a single simulation: the coin is flipped 50 times\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\ndef simulate_50() -> int:\n    \"\"\"Flip a coin 50 × and return the number of heads.\"\"\"\n    return sum(flip_coin(50))\n\nrandom.seed(42)          # reproducible example\nsimulate_50()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n25\n```\n\n\n:::\n:::\n\n\nThen, we perform 1000 simulations and store the results in an array, which we then present summary statistics on.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrandom.seed(42)\n\nn_sims  = 1000\nresults = [simulate_50() for _ in range(n_sims)]\n\n# summary statistics\nprint(f\"min  : {min(results)}\")\nprint(f\"max  : {max(results)}\")\nprint(f\"mean : {np.mean(results):.2f}\")\nprint(f\"sd   : {np.std(results, ddof=1):.2f}\")\n\n# visualise\nplt.hist(results, bins=20, edgecolor=\"black\")\nplt.title(\"Heads in 1 000 × 50-flip experiments\")\nplt.xlabel(\"Number of heads\")\nplt.ylabel(\"Frequency\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmin  : 13\nmax  : 36\nmean : 24.93\nsd   : 3.48\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](stat_software_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n#### Nuts and Bolts\n\nObviously, this example is a bit contrived, but is meant to show how the text and code can be interleaved to make the resulting output easy to understand. This Jupyter notebook can then be shared with collaborators (or posted on a code sharing site, such as github - provided none of the protected health information is hard coded into the text.\n\n::: callout-tip\nAs an exercise, see if you can create your own Jupyter notebook file, and extend the analysis to generate the interquartile range for the mean number of Heads in 50 flips.\n:::\n\n## Stata\n\nStata does not have a native notebook-style way to share code, but instead you have \\*.do files that contain a script of the relevant commands, and you can add comments using either \\# , \\*, or //\\* \\*//\\*\n\n::: callout-tip\nTo create a new STATA do file, go to File -\\> New -\\> Do File (\\*.do). **Try it to make the below script.**\n:::\n\nStata Code: \\*.do files dont have an easy way to execute within the web page, so you'll have to run the below script on your own machine to see the output.\n\n``` stata\n*-----------------------------------------------------*\n* 1.  Set the number of simulations                  *\n*-----------------------------------------------------*\nlocal nsims   = 1000      // how many experiments?\nlocal nflips  = 50        // flips per experiment\n\nset obs `nsims'           // create `nsims' empty observations\n\n* Optional: reproducibility\nset seed 42\n\n*-----------------------------------------------------*\n* 2.  Simulate 50 fair-coin flips for each experiment *\n*     A \"head\" is coded 1 when runiform() < .5        *\n*-----------------------------------------------------*\nforvalues j = 1/`nflips' {\n    generate byte flip`j' = runiform() < .5\n}\n\n*-----------------------------------------------------*\n* 3.  Count heads in each experiment                  *\n*-----------------------------------------------------*\negen heads = rowtotal(flip1-flip`nflips')\n\n*-----------------------------------------------------*\n* 4.  Inspect the sampling distribution               *\n*-----------------------------------------------------*\nsummarize heads\n\nhistogram heads , ///\n    width(1) start(0.5)          /// each bin spans one integer count\n    title(\"Heads in `nsims' × `nflips'-flip experiments\") ///\n    xlabel(0(5)50)               /// tick every 5 heads; adjust as desired\n    ylabel(, angle(horizontal))\n```\n\n![](images/clipboard-1761817781.png)\n\n#### Nuts and Bolts\n\nObviously, this example is a bit contrived, but is meant to show how the text in comments and code can be interleaved to make the resulting output easy to understand. This do file can then be shared with collaborators (or posted on a code sharing site, such as github - provided none of the protected health information is hard coded into the text.\n\n::: callout-tip\nAs an exercise, see if you can create your own Stata do file, and extend the analysis to generate the interquartile range for the mean number of Heads in 50 flips.\n:::\n:::::::::\n\n## Reproducible Research {#sec-reproducible}\n\nFrom a perspective of scientific rigor, sharing should also include the individual patient data to allow researchers to directly replicate or modify the reported analyses. However, despite research participant support for data sharing, privacy concerns and research ethics concerns generally do not permit the sharing of data, even if it has been pseudonomized. This is an active space where \"ideal\" and \"actual\" are far apart... but the current takeaway is that individual patient data should not ben shared unless that was explicitly part of the IRB authorization.\n\nFurther reading (R focus): <https://raps-with-r.dev/intro.html>\n\n## An Example: Meta-analyzing the effect of corticosteroids in CAP {#sec-steroids_cap}\n\nLet's use meta-analysis as a quick example of how to put this all into action.\n\nA meta-analysis typically accompanies a systematic review to comprehensively capture relevant studies on a question. Systematic reviews are complex, so we’ll skip the details here. Instead, we’ll perform a meta-analysis using a prepared spreadsheet of all known steroids-for-CAP studies (let me know if any studies are missing).\n\nDownload the data here: [Steroid CAP Trials Spreadsheet](https://github.com/reblocke/statistics_sandbox/blob/1b87fb4e65da11a7f9541ea9c7f93b7e3947a13a/Steroids%20PNa/Steroids%20PNa%20MA.xls)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# ---- display the table ------------------------------------------------------\nkable(steroids_pna, caption = \"Steroids PNa meta-analysis data\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Steroids PNa meta-analysis data\n\n|study             | year| chest_ma| int_death| int_alive| pla_death| pla_alive|\n|:-----------------|----:|--------:|---------:|---------:|---------:|---------:|\n|Wagner            | 1956|        0|         1|        51|         1|        60|\n|McHardy           | 1972|        0|         3|        37|         9|        77|\n|Marik             | 1993|        0|         1|        13|         3|        13|\n|Confalonieri      | 2005|        0|         0|        23|         7|        16|\n|El Ghamrawy       | 2006|        0|         3|        14|         6|        11|\n|Mikami            | 2007|        0|         1|        15|         0|        15|\n|Snijders          | 2010|        0|         6|        98|         6|       103|\n|Melvis            | 2011|        0|        17|       134|        19|       134|\n|Ferrandez-Serrano | 2011|        0|         1|        27|         1|        27|\n|Sabry             | 2011|        0|         2|        38|         6|        34|\n|Nafae             | 2013|        0|         4|        56|         6|        14|\n|Blum              | 2015|        0|        16|       386|        13|       387|\n|Torres            | 2015|        0|         8|        53|        11|        48|\n|Lloyd             | 2019|        0|        69|       338|        63|       362|\n|Wittermans        | 2021|        0|         3|       206|         7|       196|\n|ESCAPe Study      | 2023|        0|        47|       239|        50|       227|\n|Dequin            | 2023|        0|        25|       375|        47|       348|\n|REMAP-CAP         | 2025|        1|        78|       443|        12|       110|\n\n\n:::\n:::\n\n\nNow, we'll cover how you'd meta-analyze these studies in each language as an exercise:\n\n### Example Code\n\n::: panel-tabset\n## R\n\nIn R, here's the steps needed to perform the meta-analysis:\n\nFirst, set your working directory to wherever you downloaded the file. Then, we'll need to make sure we have the right package to read in the spreadsheet\n\nWe'll need the 'readxl' package to read in the \\*.xls file <https://cran.r-project.org/web/packages/readxl/index.html>\n\nThen, we write code to import the spreadsheet with study data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(readxl)                           # install.packages(\"readxl\") if needed\n\n## --- OPTION A – read from a local copy ------------------------------------\n# setwd(\"~/your_dir\")                       # uncomment and edit this line\n# dat <- read_xls(\"Steroids PNa MA.xls\", sheet = \"Sheet1\") |>\n#        janitor::clean_names()\n\n## --- OPTION B – read directly from the GitHub raw file             ----\nurl  <- \"https://raw.githubusercontent.com/reblocke/statistics_sandbox/\" |>\n        paste0(\"1b87fb4e65da11a7f9541ea9c7f93b7e3947a13a/\",\n                \"Steroids%20PNa/Steroids%20PNa%20MA.xls\")\n\ntmp  <- tempfile(fileext = \".xls\")          # download to a temp file\ndownload.file(url, tmp, mode = \"wb\")\n\ndat  <- read_xls(tmp, sheet = \"Sheet1\") |>\n        janitor::clean_names()                       # lower-case, snake_case\n\n# Inspect the key columns\ndat |>\n  dplyr::select(study, year,\n                int_death, int_alive,\n                pla_death, pla_alive) |>\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 18 × 6\n   study              year int_death int_alive pla_death pla_alive\n   <chr>             <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n 1 Wagner             1956         1        51         1        60\n 2 McHardy            1972         3        37         9        77\n 3 Marik              1993         1        13         3        13\n 4 Confalonieri       2005         0        23         7        16\n 5 El Ghamrawy        2006         3        14         6        11\n 6 Mikami             2007         1        15         0        15\n 7 Snijders           2010         6        98         6       103\n 8 Melvis             2011        17       134        19       134\n 9 Ferrandez-Serrano  2011         1        27         1        27\n10 Sabry              2011         2        38         6        34\n11 Nafae              2013         4        56         6        14\n12 Blum               2015        16       386        13       387\n13 Torres             2015         8        53        11        48\n14 Lloyd              2019        69       338        63       362\n15 Wittermans         2021         3       206         7       196\n16 ESCAPe Study       2023        47       239        50       227\n17 Dequin             2023        25       375        47       348\n18 REMAP-CAP          2025        78       443        12       110\n```\n\n\n:::\n:::\n\n\nNext, we'll need to get a package to do the meta-analysis. Here's a list of relevant packages: <https://cran.r-project.org/web/views/MetaAnalysis.html>\n\nThe 'meta' package looks good. Try using \\`install.packages('meta')\\` to install it, then you can you can access the documentation using \\`?meta\\`\n\nNow, we'll perform a random-effects meta-analysis (Using DL variance stimator)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(meta)                             # install.packages(\"meta\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: metadat\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading 'meta' package (version 8.1-0).\nType 'help(meta)' for a brief overview.\n```\n\n\n:::\n\n```{.r .cell-code}\nm <- metabin(\n  event.e     = int_death,\n  n.e         = int_death + int_alive,\n  event.c     = pla_death,\n  n.c         = pla_death + pla_alive,\n  studlab     = paste(study, year),\n  data        = dat,\n  sm          = \"OR\",      # odds ratio\n  method.tau  = \"DL\",      # <-- DerSimonian–Laird estimator\n  method.random.ci        = FALSE\n)\n```\n:::\n\n\nLastly, we'll need to plot the result as a Forest Plot on a log scale (always log scale with ratio variables)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nforest(\n  m,\n  comb.fixed  = FALSE,\n  comb.random = TRUE,\n  print.tau2  = FALSE,\n  backtransf  = TRUE,\n  xlab        = \"Odds Ratio (death)\",\n  leftlabs    = c(\"Study (year)\", \"Steroid\", \"Placebo\"),\n  xlog        = TRUE,\n  at          = c(1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32),\n  label.e     = \"Steroid\",\n  label.c     = \"Placebo\",\n  col.diamond = \"navy\",\n  overall.lty = 2,\n  ## keep the default right-hand columns\n  # rightcols = FALSE,          # <-- delete this line\n  main        = \"Steroids OR for Death, Random-Effects Meta-analysis\"\n)\n```\n\n::: {.cell-output-display}\n![](stat_software_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Python\n\nIn Python, here's the steps needed to perform the meta-analysis:\n\nFirst, set your working directory to wherever you downloaded the file. Then, we'll need to make sure we have the right package to read in the spreadsheet\n\nThen, we write code to import the spreadsheet with study data using pandas\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport pandas as pd\n\n# --- OPTION A: read directly from disk -------------------------------\n# df = pd.read_excel(\"Steroids PNa MA.xls\", sheet_name=\"Sheet1\")\n\n# --- OPTION B:  For this demo we fetch the raw file from GitHub (same content as R block)\nurl = (\"https://raw.githubusercontent.com/reblocke/statistics_sandbox/\"\n       \"1b87fb4e65da11a7f9541ea9c7f93b7e3947a13a/\"\n       \"Steroids%20PNa/Steroids%20PNa%20MA.xls\")\ndf = pd.read_excel(url)           # GitHub serves raw file\n\n# clean column names to snake_case like janitor::clean_names()\ndf.columns = (df.columns\n                .str.strip()\n                .str.lower()\n                .str.replace(\" \", \"_\"))\n\n# quick sanity check\ndf.head(18)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                study  year  chest_ma  ...  int_alive  pla_death  pla_alive\n0              Wagner  1956         0  ...         51          1         60\n1             McHardy  1972         0  ...         37          9         77\n2               Marik  1993         0  ...         13          3         13\n3        Confalonieri  2005         0  ...         23          7         16\n4         El Ghamrawy  2006         0  ...         14          6         11\n5              Mikami  2007         0  ...         15          0         15\n6            Snijders  2010         0  ...         98          6        103\n7              Melvis  2011         0  ...        134         19        134\n8   Ferrandez-Serrano  2011         0  ...         27          1         27\n9               Sabry  2011         0  ...         38          6         34\n10              Nafae  2013         0  ...         56          6         14\n11               Blum  2015         0  ...        386         13        387\n12             Torres  2015         0  ...         53         11         48\n13              Lloyd  2019         0  ...        338         63        362\n14         Wittermans  2021         0  ...        206          7        196\n15       ESCAPe Study  2023         0  ...        239         50        227\n16             Dequin  2023         0  ...        375         47        348\n17          REMAP-CAP  2025         1  ...        443         12        110\n\n[18 rows x 7 columns]\n```\n\n\n:::\n:::\n\n\nNext, we'll need to get a package to do the meta-analysis. We'll use the statsmodels (statsmodels.stats.meta_analysis) package https://www.statsmodels.org/dev/examples/notebooks/generated/metaanalysis1.html\n\nNow, we'll perform a random-effects meta-analysis (Using DL variance estimator - which takes some manual preparation work).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport numpy as np\nfrom statsmodels.stats.meta_analysis import (\n    effectsize_2proportions,    # log-odds-ratio + variance   \n    combine_effects             # pooling with DL            \n)\n\n# --- 2. per-study log(OR) & variance with 0.5 correction --------------------\na = df[\"int_death\"].to_numpy(dtype=float)\nb = df[\"int_alive\"].to_numpy(dtype=float)\nc = df[\"pla_death\"].to_numpy(dtype=float)\nd = df[\"pla_alive\"].to_numpy(dtype=float)\n\n# continuity correction if any cell is zero in that study\ncc = ((a == 0) | (b == 0) | (c == 0) | (d == 0)).astype(float) * 0.5\na += cc; b += cc; c += cc; d += cc\n\nlog_or = np.log((a * d) / (b * c))\nvar_or = 1 / a + 1 / b + 1 / c + 1 / d          # variance of log(OR)\n\n# --- 3. DerSimonian-Laird random-effects pooling ---------------------------\nres = combine_effects(log_or, var_or, method_re=\"dl\")   # DL estimator\n\nprint(\"Current statsmodels version:\", importlib.metadata.version(\"statsmodels\"))\n\nsf = res.summary_frame()          # still log-OR\nsf[\"eff\"]    = np.exp(sf[\"eff\"])\nsf[\"ci_low\"] = np.exp(sf[\"ci_low\"])\nsf[\"ci_upp\"] = np.exp(sf[\"ci_upp\"])\nprint(sf.round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent statsmodels version: 0.14.4\n                     eff  sd_eff  ci_low  ci_upp   w_fe   w_re\n0                  1.176   1.427   0.072  19.285  0.005  0.010\n1                  0.694   0.696   0.177   2.714  0.020  0.036\n2                  0.333   1.219   0.031   3.638  0.006  0.013\n3                  0.047   1.495   0.002   0.878  0.004  0.009\n4                  0.393   0.814   0.080   1.936  0.014  0.028\n5                  3.000   1.672   0.113  79.499  0.003  0.007\n6                  1.051   0.594   0.328   3.369  0.027  0.046\n7                  0.895   0.356   0.446   1.796  0.075  0.092\n8                  1.000   1.440   0.059  16.822  0.005  0.010\n9                  0.298   0.850   0.056   1.578  0.013  0.026\n10                 0.167   0.711   0.041   0.672  0.019  0.035\n11                 1.234   0.380   0.586   2.600  0.066  0.085\n12                 0.659   0.506   0.245   1.774  0.037  0.059\n13                 1.173   0.190   0.808   1.702  0.264  0.150\n14                 0.408   0.697   0.104   1.599  0.020  0.036\n15                 0.893   0.223   0.576   1.383  0.191  0.137\n16                 0.494   0.258   0.297   0.819  0.142  0.123\n17                 1.614   0.328   0.849   3.069  0.089  0.100\nfixed effect       0.861   0.098   0.711   1.042  1.000    NaN\nrandom effect      0.785   0.145   0.590   1.044    NaN  1.000\nfixed effect wls   0.861   0.121   0.678   1.092  1.000    NaN\nrandom effect wls  0.785   0.146   0.590   1.045    NaN  1.000\n```\n\n\n:::\n:::\n\n\nLastly, we'll need to plot the result as a Forest Plot on a log scale (always log scale with ratio variables)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# draw the default forest plot (no extra kwargs)\nfig = res.plot_forest(use_exp=True)\n\n# post-process the axis with Matplotlib\nax = fig.axes[0]          # the single Axes returned by plot_forest\nax.set_xscale(\"log\")\nax.set_xlim(0.03125, 32)\nax.set_xlabel(\"Odds Ratio (death)\")\nax.set_xticks([1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32])\nax.set_xticklabels(\n    [\"1/32\", \"1/16\", \"1/8\", \"1/4\", \"1/2\",\n     \"1\", \"2\", \"4\", \"8\", \"16\", \"32\"]\n)\n\nplt.title(\"Steroids OR for Death, Random-Effects Meta-analysis (DL)\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(0.03125, 32)\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](stat_software_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Stata\n\nAs before, Stata does not have a native notebook-style way to execute code within this webpage, so you'll have to run the below script on your own machine to see the output.\n\nHere's the steps:\n\n``` stata\n\n*-----------------------------------------------------*\n* 1. Set your_dir to wherever you downloaded the doc  *\n*-----------------------------------------------------*\ncd your_dir\n\n*-----------------------------------------------------*\n* 2. Import the Spreadsheet with Study Data           *\n*-----------------------------------------------------*\nclear\nimport excel \"Steroids PNa MA.xls\", sheet(\"Sheet1\") firstrow case(lower)\nlist study year int_death int_alive pla_death pla_alive\nlabel variable study \"Study\"\n\n*-----------------------------------------------------*\n* 3. Meta-analyze the studies using REML              *\n*-----------------------------------------------------*\nmeta esize int_death int_alive pla_death pla_alive, random(reml) esize(lnor) studylabel(study year) \n\n*-----------------------------------------------------*\n* 4. Create a forest plot with the result             *\n*-----------------------------------------------------*\nmeta forestplot, eform  nullrefline(lcolor(gs3)) esrefline(lcolor(gs3) lpattern(dash_dot)) title(\"Steroids OR for Death, Random-Effects Meta-analysis\")  xsize(11) ysize(7) xscale(log range(0.01 64)) xlabel(0.03125 \"1/32\" 0.0625 \"1/16\" 0.125 \"1/8\" 0.25 \"1/4\" 0.5 \"1/2\" 1 \"1\" 2 \"2\" 4 \"4\" 8 \"8\" 16 \"16\" 32 \"32\")\n```\n\n![](images/clipboard-1219197948.png)\n:::\n\n## Large Language Models (LLMs) {#sec-llm}\n\nRandomized trials (!) show the professional coders are more productive when using LLMs to assist with code. Novices see larger relative gains because LLMs supply boiler-plate, interface syntax, and “first drafts” of code.\n\nHowever, LLMs can hallucinate. They can also be tricky to use. Here's some guidance that holds for any of the frontier company models (OpenAI: chatGPT, Anthropic Claude, Google Gemini)\n\n### Minimal viable set up:\n\n| Step | Why | Tool |\n|------------------------|------------------------|------------------------|\n| Use your local <a class='glossary'>IDE<span class='def'>Integrated Development Environment, a program that allows for writing, running, and debugging code within a single program.</span></a> with a plain chat tab or desktop app. | Allows you to (quality)control inputs and outputs - which is harder to do with an integrated IDE (e.g. Cursor) or agentic models. | Built-in browser tab or official desktop app |\n| Do NOT upload data to LLMs | The companies do not have Business Access Agreements with institutions, and this generally violates the consent/IRB authorization of most studies. | Keep data local, only share schemas (descriptions of the variables) or mock data. |\n| Sanity test all LLM-generated code | Hallucinations exist, and often task specifications are subtly wrong | You can writing coding tests (so called \"Unit Tests\", but should also visualize and consider all code (this is true when not using LLMs) |\n| Give the LLM examples of what you want | LLMs are VERY good at understanding what existing code does - and can often modify much better than create de-novo | Use things like: here's the code that generates this figure - modify it so that title is larger. |\n| Give the LLM specific instructions, often with steps | The more \"context\" you give the LLM for what you want, the more likely the associations it follows will be relevant | Detailed instructions in the prompt. More on prompt engineering is available here: <https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview> |\n\n::: callout-important\n**Never put PHI into a commercial LLM interface**. Not even 'publicly available' data like MIMIC (which has terms of use that forbid this). Most companies market their LLMs as being able to act as data-analysts on your behalf (meaning, you give it the data and it analyzes it for you). Don't do this. [https://pmc.ncbi.nlm.nih.gov/articles/PMC10937180/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10937180/?utm_source=chatgpt.com)\n\nInstead, use the LLM to help you write statistical code that you then run on your machine.\n:::\n\n| **Mistake** | **Consequence** | **Quick fix** |\n|------------------------|------------------------|------------------------|\n| Skipping LLM entirely | Slower learning curve | Use it for boiler-plate, explaining things, first drafts |\n| Blind trust | Silent bugs ≈ misleading science | Unit tests, peer-review, benchmark against known outputs |\n| Vague prompt | Generic, unusable code | Include language, package, data schema, desired output |\n| Manual debugging | Time sink | Feed the *exact* error message back to the model |\n| Pasting PHI | Compliance breach | Use synthetic or sampled data; keep true identifiers offline |\n\n## Local (and other) resources {#sec-utah_resources}\n\n-   One Data Science Hub Workshops: <https://utah-data-science-hub.github.io/education_archived.html>  \n\n-   Request CTSI help: <https://ctsi.utah.edu/cores-and-services/triad> \n\n## TODO List: {.unnumbered}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# find citation on how frequently errors occur in scientific programming. \n# find citation for data cleaning influence on stability of findings\n```\n:::\n\n",
    "supporting": [
      "stat_software_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}