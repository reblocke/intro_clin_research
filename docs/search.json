[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Clinical Research",
    "section": "",
    "text": "Overview\nThis workbook provides a practical, step-by-step guide for medical students, residents, and fellows embarking on a clinical research project.\nThe focus is specifically on clinical epidemiology and health services research, reflecting both my expertise and the suitability of these areas for clinician-researchers (discussed further in What Type of Project Should You Do?).\nWhile numerous excellent resources exist, this guide uniquely emphasizes:\nIdeally, this guide serves as a complement to active mentorship, where a dedicated mentor can address specific questions and contextual challenges. However, as detailed in What Kind of Mentor(s) Do You Need?, mentorship can vary significantly in quality and depth. Some mentors might lack content-specific expertise (methodologists are unfortunately rare and highly sought-after) or sufficient involvement, making self-directed resources particularly valuable for trainees eager to independently advance their research skills.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "instructions.html",
    "href": "instructions.html",
    "title": "1  How to Use this Book",
    "section": "",
    "text": "1.1 Setup",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "instructions.html#sec-setup",
    "href": "instructions.html#sec-setup",
    "title": "1  How to Use this Book",
    "section": "",
    "text": "1.1.1 Install booktem\n# install.packages(\"devtools\")\ndevtools::install_github(\"debruine/booktem\")\n\n1.1.2 Quarto Options\nThe file _quarto.yml contains various options that you can set to change the format and look of your book.\n\n1.1.2.1 Language Options\nThere is some default text for things like the “authors” list and “table of contents” that might need translations. Set the lang key to the 2-letter language code for your language.\nYou can make a custom translation by translating the values in the include/_language.yml file.\nlang: en\n# language: include/_language.yml\n\n1.1.2.2 Project Options\nThe project key defines the inputs and outputs for the book (quarto reference).\n\n\n\n\n\n\nproject key\n\n\n\n\n\nproject:\n  type: book\n  output-dir: docs\n  resources: resources \n\n\n\nThe output-dir key defines the directory where the rendered web files will be saved. This is set to docs in order to be compatible with GitHub Pages, but you can change this if you are working with a different repository that expects the web files to be in a different directory.\nThe resources key specifies a directory that is copied verbatim to the output directory. This is where you should put, for example, data files that you want to make accessible online (sometimes they don’t automatically copy over when linked).\n\n1.1.2.3 Book Options\nThe book key defines options that affect the look and function of the book (quarto reference).\n\n\n\n\n\n\nbook key\n\n\n\n\n\nbook:\n  title: Book\n  subtitle: ~\n  author: ~\n  doi: ~\n  license: CC-BY 4.0\n  description: ~\n  cover-image: images/logos/logo.png\n  image: images/logos/logo.png\n  favicon: images/logos/logo.png\n  cookie-consent: false\n  google-analytics: ~\n  page-navigation: true\n  search: true\n  # comments:\n  #   hypothesis:\n  #     theme: clean\n  #     openSidebar: false\n  downloads: ~\n  sharing: ~\n  sidebar:\n    title: ~\n    logo: ~\n    search: true\n    contents: ~\n    style: floating\n    background: ~\n    foreground: ~\n    border: true\n    alignment: left\n    collapse-level: 3\n    pinned: true\n    header: \"\"\n    footer: \"\"\n  margin-header: ~\n  page-footer:\n    left: ~\n    right: ~\n  chapters:\n  - index.qmd\n  - instructions.qmd\n  appendices:\n  - references.qmd\n\n\n\n\n1.1.2.4 html Options\nThe format key defines options for specific formats, such as html or pdf. We’ll only be using html here (quarto reference).\n\n\n\n\n\n\nformat:html key\n\n\n\n\n\nformat:\n  html:\n    theme:\n      light:\n      - flatly\n      - include/light.scss\n      dark:\n      - darkly\n      - include/dark.scss\n    css:\n    - https://use.fontawesome.com/releases/v5.13.0/css/all.css\n    - include/booktem.css\n    - include/glossary.css\n    - include/style.css\n    df-print: kable\n    code-link: true\n    code-fold: false\n    code-line-numbers: true\n    code-overflow: wrap\n    code-copy: hover\n    highlight-style: a11y\n    mainfont: ~\n    monofont: ~\n    include-after-body: [include/script.js]\n\n\n\n\n1.1.3 Crossrefs\nSection links must start with sec- and look like this: Section 1.1.5.\n## Section Title {#sec-section-title}\n\nInternal links look like this: @sec-section-title\nFigure links must start with fig- and look like this: Figure 1.1.\n\n\n\n\n\n\n\nFigure 1.1: A histogram of a Poisson distribution with lambda = 3\n\n\n\n\nTable links must start with tbl- and look like this: Table 1.1.\n\n\n\nTable 1.1: The authors of this book\n\n\n\n\n\nfirst_name\nlast_name\n\n\n\nLisa\nDeBruine\n\n\nDaniël\nLakens\n\n\n\n\n\n\n\n\n\nSee the quarto documentation for more information.\n\n1.1.4 References\nZotero export - keep updated\n\n1.1.5 Snippets\nSnippets in RStudio provide shortcuts to syntax. For example, in an RMarkdown document, type “r” and shift-tab to expand a code chunk.\nYou can add your own snippets. Under the Tools menu, choose Edit Code Snippets... and paste the following text into the end of the appropriate sections.\n\n1.1.5.1 Markdown\nsnippet gls\n    r glossary(\"${1:term}\")\n    \nsnippet gls2\n    r glossary(\"${1:term}\", \"${2:display}\")\n    \nsnippet h1\n    # ${1:title} {#sec-${2:ref}}\n    \nsnippet h2\n    ## ${1:title} {#sec-${2:ref}}\n    \nsnippet h3\n    ### ${1:title} {#sec-${2:ref}}\n    \nsnippet h4\n    #### ${1:title} {#sec-${2:ref}}\n    \nsnippet h5\n    ##### ${1:title} {#sec-${2:ref}}\n\n1.1.6 Customize\n\n1.1.6.1 Page Footer\nThe default footer includes license YEAR, author, and github and twitter icons, but you can customize this in the _quarto.yml file under page-footer:. See the quarto documentation for more options. See the available icons at https://icons.getbootstrap.com/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "instructions.html#sec-layout",
    "href": "instructions.html#sec-layout",
    "title": "1  How to Use this Book",
    "section": "\n1.2 Layout",
    "text": "1.2 Layout\n\n1.2.1 Conventions\nThis book will use the following conventions:\n\nCode: list(number = 1, letter = \"A\")\n\nFile paths: data/sales.csv\n\nMenu/interface options: Tools &gt; Global Options… &gt; Pane Layout\n\nR Packages: tidyverse\n\nGlossary items: alphaThe threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\n\nCitations: Wickham et al. (2022)\n\nInternal links: Section 1.2.1\n\nExternal links: Mastering Shiny\n\nMac-specific: Cmd-Shift-F10\n\nWindows-specific: Ctl-Shift-F10\n\n\nA list of mac and windows keyboard shortcuts.\n\n1.2.2 Figures\nIt is best practice to set a custom ggplot theme, then each subsequent plot will use that theme. You can put this code in R/my_setup.R after loading ggplot2.\nStart with a built-in theme and then add any tweaks with the theme() function.\n\nlibrary(ggplot2)\n\nmy_theme &lt;- theme_minimal(base_size = 16) + \n            theme(panel.background = element_rect(fill = \"red\", \n                                                  color = \"black\", \n                                                  size = 5),\n                  panel.grid = element_blank())\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\ntheme_set(my_theme)\n\n\nggplot(midwest, aes(popdensity, percollege)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Population Density\", y = \"Percent College Educated\")\n\n\n\n\n\n\nFigure 1.2: Demographic information of midwest counties from 2000 US census\n\n\n\n\n\n1.2.3 Tables\n\nhead(beaver1)\n\n\n\nBeavers\n\nday\ntime\ntemp\nactiv\n\n\n\n346\n840\n36.33\n0\n\n\n346\n850\n36.34\n0\n\n\n346\n900\n36.35\n0\n\n\n346\n910\n36.42\n0\n\n\n346\n920\n36.55\n0\n\n\n346\n930\n36.69\n0\n\n\n\n\n\n\n\n1.2.4 Callout boxes\nSee the quarto reference for more options.]{.aside}\n\n\n\n\n\n\nNote\n\n\n\n.callout-note: Informational asides.\n\n\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\ncolapse = “true”: Expanded!\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n.callout-tip: Tips\n\n\n\n\n\n\n\n\nWarning\n\n\n\n.callout-warning: Notes to warn you about something.\n\n\n\n\n\n\n\n\nCaution\n\n\n\n.callout-caution: Notes about things that could cause serious errors.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n.callout-important: Notes about things that are important.\n\n\n\n1.2.5 Code and Output\n\n# code chunks\npaste(\"Code\", \"Output\", 1, sep = \" \")\n\n[1] \"Code Output 1\"\n\n\n\n\n\nFilename or header\n\n# code chunks with filename\na &lt;- 1\n\n\n\n\n```{r, fig.width = 2, fig.height = 2}\n# code chunks with visible headers\nhist(rnorm(100000))\n```\n\n\n## Markdown Example\n\n* Inline code: `r nrow(iris)`\n* *Italics*\n* **Bold**\n* [Linked text](https://psyteachr.github.io)\n\n1.2.6 Fonts",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "instructions.html#sec-extras",
    "href": "instructions.html#sec-extras",
    "title": "1  How to Use this Book",
    "section": "\n1.3 Extras",
    "text": "1.3 Extras\n\n1.3.1 Glossary\nBooks are set up with lightweight glossary functions from the glossary package.\n\n# code in R/my_setup.R to initialise the glossary on each page\nlibrary(glossary)\nglossary_path(\"include/glossary.yml\")\nglossary_popup(\"click\") # \"click\", \"hover\" or \"none\"\n\nEdit the file glossary.yml with your glossary terms like this:\nalpha: |\n  The threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\np-value: |\n  The probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\nLook up a term from the glossary file with glossary(\"alpha\"): alphaThe threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\nDisplay a different value for the term with glossary(\"alpha\", \"$\\\\alpha$\"): \\(\\alpha\\)The threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\nUse an inline definition instead of the glossary file with glossary(\"beta\", def = \"The second letter of the Greek alphabet\"): betaThe second letter of the Greek alphabet\nJust show the definition with glossary(\"p-value\", show = \"def\"): The probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\nShow the table of terms defined on this page with glossary_table():\n\n\n\n\nterm\ndefinition\n\n\n\nalpha\nThe threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\n\n\nbeta\nThe second letter of the Greek alphabet\n\n\np-value\nThe probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\n\n\n\n\n\n\n1.3.2 FontAwesome\nThe fontAwesome quarto extension allows you to use the free icons with syntax like:\n{{&lt; fa dragon &gt;}}\n{{&lt; fa brands github size=5x title=\"(github logo)\" &gt;}}\nTo install it, just run this code in the Terminal pane of RStudio (not the Console pane).\nquarto install extension quarto-ext/fontawesome\n\n\n\n\nWickham, H., Bryan, J., & Barrett, M. (2022). Usethis: Automate package and project setup. https://CRAN.R-project.org/package=usethis",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "webexercises.html",
    "href": "webexercises.html",
    "title": "Appendix B — Webexercises",
    "section": "",
    "text": "B.1 Example Question types\nThis template shows how instructors can easily create interactive web documents that students can use in self-guided learning. The webexercises package provides a number of functions that you use in inline R code or through code chunk options to create HTML widgets (text boxes, pull down menus, buttons that reveal hidden content). Examples are given below. Render this file to HTML to see how it works.\nNOTE: To use the widgets in the compiled HTML file, you need to have a JavaScript-enabled browser.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "webexercises.html#example-questions",
    "href": "webexercises.html#example-questions",
    "title": "Appendix B — Webexercises",
    "section": "",
    "text": "B.1.1 Fill-In-The-Blanks (fitb())\nCreate fill-in-the-blank questions using fitb(), providing the answer as the first argument.\n\n2 + 2 is \n\n\nYou can also create these questions dynamically, using variables from your R session.\n\nThe square root of 64 is: \n\n\nThe blanks are case-sensitive; if you don’t care about case, use the argument ignore_case = TRUE.\n\nWhat is the letter after D? \n\n\nIf you want to ignore differences in whitespace use, use the argument ignore_ws = TRUE (which is the default) and include spaces in your answer anywhere they could be acceptable.\n\nHow do you load the tidyverse package? \n\n\nYou can set more than one possible correct answer by setting the answers as a vector.\n\nType a vowel: \n\n\nYou can use regular expressions to test answers against more complex rules.\n\nType any 3 letters: \n\n\nB.1.2 Multiple Choice (mcq())\n\n“Never gonna give you up, never gonna: \nlet you go\nturn you down\nrun away\nlet you down”\n“I \nbless the rains\nguess it rains\nsense the rain down in Africa” -Toto\n\nB.1.3 True or False (torf())\n\nTrue or False? You can permute values in a vector using sample(). \nTRUE\nFALSE\n\n\nB.1.4 Longer MCQs (longmcq())\nWhen your answers are very long, sometimes a drop-down select box gets formatted oddly. You can use longmcq() to deal with this. Since the answers are long, It’s probably best to set up the options inside an R chunk with echo=FALSE.\nWhat is a p-value?\n\nthe probability that the null hypothesis is truethe probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is truethe probability of making an error in your conclusion\n\nWhat is true about a 95% confidence interval of the mean?\n\n95% of the data fall within this rangeif you repeated the process many times, 95% of intervals calculated in this way contain the true meanthere is a 95% probability that the true mean lies within this range",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "webexercises.html#checked-sections",
    "href": "webexercises.html#checked-sections",
    "title": "Appendix B — Webexercises",
    "section": "\nB.2 Checked sections",
    "text": "B.2 Checked sections\nCreate sections with the class webex-check to add a button that hides feedback until it is pressed. Add the class webex-box to draw a box around the section (or use your own styles).\n\nI am going to learn a lot: \nTRUE\nFALSE\n\nWhat is a p-value?\n\nthe probability that the null hypothesis is truethe probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is truethe probability of making an error in your conclusion",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "webexercises.html#hidden-solutions-and-hints",
    "href": "webexercises.html#hidden-solutions-and-hints",
    "title": "Appendix B — Webexercises",
    "section": "\nB.3 Hidden solutions and hints",
    "text": "B.3 Hidden solutions and hints\nYou can fence off a solution area that will be hidden behind a button using hide() before the solution and unhide() after, each as inline R code. Pass the text you want to appear on the button to the hide() function.\nIf the solution is a code chunk, instead of using hide() and unhide(), simply set the webex.hide chunk option to TRUE, or set it to the string you wish to display on the button.\nRecreate the scatterplot below, using the built-in cars dataset.\n\n\n\n\n\n\n\n\n\n\nI need a hint\n\nSee the documentation for plot() (?plot)\n\n\n\n\n\nClick here to see the solution\n\nplot(cars$speed, cars$dist)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Wickham, H., Bryan, J., & Barrett, M. (2022). Usethis: Automate\npackage and project setup. https://CRAN.R-project.org/package=usethis",
    "crumbs": [
      "Appendices",
      "References"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Appendix A — Other helpful resources",
    "section": "",
    "text": "A.1 Statiatical Programming:\nHere are some other helpful resources that still need to be organized and triaged.\nResources…. Here are a few references that might be helpful for learning R: University of Utah Resource https://uofudelphi-r-23-08-21.netlify.app/ More in depth resource/book https://r4ds.hadley.nz/ and slightly more advanced: https://rap4mads.eu/03-functional-programming.html style guide for how to name things https://style.tidyverse.org/syntax.html",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Other helpful resources</span>"
    ]
  },
  {
    "objectID": "project_selection.html",
    "href": "project_selection.html",
    "title": "1  How to Select a Project",
    "section": "",
    "text": "1.1 What Type of Project Should You Do?\nThere are a variety of inter-related considerations that influence the choice of a research/academic project: choosing a topic, choosing a mentor, and choosing a particular study question. Each of these will greatly influence the the experience, requirements, and skills obtained.\nThere are types of research projects that a trainee could do, but some are better than others. My opinionated guidance is:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "project_selection.html#sec-setup",
    "href": "project_selection.html#sec-setup",
    "title": "1  How to Select a Project",
    "section": "",
    "text": "1.1.1 Install booktem\n# install.packages(\"devtools\")\ndevtools::install_github(\"debruine/booktem\")\n\n1.1.2 Quarto Options\nThe file _quarto.yml contains various options that you can set to change the format and look of your book.\n\n1.1.2.1 Language Options\nThere is some default text for things like the “authors” list and “table of contents” that might need translations. Set the lang key to the 2-letter language code for your language.\nYou can make a custom translation by translating the values in the include/_language.yml file.\nlang: en\n# language: include/_language.yml\n\n1.1.2.2 Project Options\nThe project key defines the inputs and outputs for the book (quarto reference).\n\n\n\n\n\n\nproject key\n\n\n\n\n\nproject:\n  type: book\n  output-dir: docs\n  resources: resources \n\n\n\nThe output-dir key defines the directory where the rendered web files will be saved. This is set to docs in order to be compatible with GitHub Pages, but you can change this if you are working with a different repository that expects the web files to be in a different directory.\nThe resources key specifies a directory that is copied verbatim to the output directory. This is where you should put, for example, data files that you want to make accessible online (sometimes they don’t automatically copy over when linked).\n\n1.1.2.3 Book Options\nThe book key defines options that affect the look and function of the book (quarto reference).\n\n\n\n\n\n\nbook key\n\n\n\n\n\nbook:\n  title: Book\n  subtitle: ~\n  author: ~\n  doi: ~\n  license: CC-BY 4.0\n  description: ~\n  cover-image: images/logos/logo.png\n  image: images/logos/logo.png\n  favicon: images/logos/logo.png\n  cookie-consent: false\n  google-analytics: ~\n  page-navigation: true\n  search: true\n  # comments:\n  #   hypothesis:\n  #     theme: clean\n  #     openSidebar: false\n  downloads: ~\n  sharing: ~\n  sidebar:\n    title: ~\n    logo: ~\n    search: true\n    contents: ~\n    style: floating\n    background: ~\n    foreground: ~\n    border: true\n    alignment: left\n    collapse-level: 3\n    pinned: true\n    header: \"\"\n    footer: \"\"\n  margin-header: ~\n  page-footer:\n    left: ~\n    right: ~\n  chapters:\n  - index.qmd\n  - instructions.qmd\n  appendices:\n  - references.qmd\n\n\n\n\n1.1.2.4 html Options\nThe format key defines options for specific formats, such as html or pdf. We’ll only be using html here (quarto reference).\n\n\n\n\n\n\nformat:html key\n\n\n\n\n\nformat:\n  html:\n    theme:\n      light:\n      - flatly\n      - include/light.scss\n      dark:\n      - darkly\n      - include/dark.scss\n    css:\n    - https://use.fontawesome.com/releases/v5.13.0/css/all.css\n    - include/booktem.css\n    - include/glossary.css\n    - include/style.css\n    df-print: kable\n    code-link: true\n    code-fold: false\n    code-line-numbers: true\n    code-overflow: wrap\n    code-copy: hover\n    highlight-style: a11y\n    mainfont: ~\n    monofont: ~\n    include-after-body: [include/script.js]\n\n\n\n\n1.1.3 Crossrefs\nSection links must start with sec- and look like this: Section 1.1.5.\n## Section Title {#sec-section-title}\n\nInternal links look like this: @sec-section-title\nFigure links must start with fig- and look like this: Figure 1.1.\n\n\n\n\n\n\n\nFigure 1.1: A histogram of a Poisson distribution with lambda = 3\n\n\n\n\nTable links must start with tbl- and look like this: Table 1.1.\n\n\n\nTable 1.1: The authors of this book\n\n\n\n\n\nfirst_name\nlast_name\n\n\n\nLisa\nDeBruine\n\n\nDaniël\nLakens\n\n\n\n\n\n\n\n\n\nSee the quarto documentation for more information.\n\n1.1.4 References\nZotero export - keep updated\n\n1.1.5 Snippets\nSnippets in RStudio provide shortcuts to syntax. For example, in an RMarkdown document, type “r” and shift-tab to expand a code chunk.\nYou can add your own snippets. Under the Tools menu, choose Edit Code Snippets... and paste the following text into the end of the appropriate sections.\n\n1.1.5.1 Markdown\nsnippet gls\n    r glossary(\"${1:term}\")\n    \nsnippet gls2\n    r glossary(\"${1:term}\", \"${2:display}\")\n    \nsnippet h1\n    # ${1:title} {#sec-${2:ref}}\n    \nsnippet h2\n    ## ${1:title} {#sec-${2:ref}}\n    \nsnippet h3\n    ### ${1:title} {#sec-${2:ref}}\n    \nsnippet h4\n    #### ${1:title} {#sec-${2:ref}}\n    \nsnippet h5\n    ##### ${1:title} {#sec-${2:ref}}\n\n1.1.6 Customize\n\n1.1.6.1 Page Footer\nThe default footer includes license YEAR, author, and github and twitter icons, but you can customize this in the _quarto.yml file under page-footer:. See the quarto documentation for more options. See the available icons at https://icons.getbootstrap.com/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "project_selection.html#sec-layout",
    "href": "project_selection.html#sec-layout",
    "title": "1  How to Select a Project",
    "section": "\n1.4 Layout",
    "text": "1.4 Layout\n\n1.4.1 Conventions\nThis book will use the following conventions:\n\nCode: list(number = 1, letter = \"A\")\n\nFile paths: data/sales.csv\n\nMenu/interface options: Tools &gt; Global Options… &gt; Pane Layout\n\nR Packages: tidyverse\n\nGlossary items: alpha\n\nCitations: Wickham et al. (2022)\n\nInternal links: Section 1.4.1\n\nExternal links: Mastering Shiny\n\nMac-specific: Cmd-Shift-F10\n\nWindows-specific: Ctl-Shift-F10\n\n\nA list of mac and windows keyboard shortcuts.\n\n1.4.2 Figures\nIt is best practice to set a custom ggplot theme, then each subsequent plot will use that theme. You can put this code in R/my_setup.R after loading ggplot2.\nStart with a built-in theme and then add any tweaks with the theme() function.\n\nlibrary(ggplot2)\n\nmy_theme &lt;- theme_minimal(base_size = 16) + \n            theme(panel.background = element_rect(fill = \"red\", \n                                                  color = \"black\", \n                                                  size = 5),\n                  panel.grid = element_blank())\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\ntheme_set(my_theme)\n\n\nggplot(midwest, aes(popdensity, percollege)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Population Density\", y = \"Percent College Educated\")\n\n\n\n\n\n\nFigure 1.2: Demographic information of midwest counties from 2000 US census\n\n\n\n\n\n1.4.3 Tables\n\nhead(beaver1)\n\n\n\nBeavers\n\nday\ntime\ntemp\nactiv\n\n\n\n346\n840\n36.33\n0\n\n\n346\n850\n36.34\n0\n\n\n346\n900\n36.35\n0\n\n\n346\n910\n36.42\n0\n\n\n346\n920\n36.55\n0\n\n\n346\n930\n36.69\n0\n\n\n\n\n\n\n\n1.4.4 Callout boxes\nSee the quarto reference for more options.]{.aside}\n\n\n\n\n\n\nNote\n\n\n\n.callout-note: Informational asides.\n\n\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\ncolapse = “true”: Expanded!\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n.callout-tip: Tips\n\n\n\n\n\n\n\n\nWarning\n\n\n\n.callout-warning: Notes to warn you about something.\n\n\n\n\n\n\n\n\nCaution\n\n\n\n.callout-caution: Notes about things that could cause serious errors.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n.callout-important: Notes about things that are important.\n\n\n\n1.4.5 Code and Output\n\n# code chunks\npaste(\"Code\", \"Output\", 1, sep = \" \")\n\n[1] \"Code Output 1\"\n\n\n\n\n\nFilename or header\n\n# code chunks with filename\na &lt;- 1\n\n\n\n\n```{r, fig.width = 2, fig.height = 2}\n# code chunks with visible headers\nhist(rnorm(100000))\n```\n\n\n## Markdown Example\n\n* Inline code: `r nrow(iris)`\n* *Italics*\n* **Bold**\n* [Linked text](https://psyteachr.github.io)\n\n\n\n\nWickham, H., Bryan, J., & Barrett, M. (2022). Usethis: Automate package and project setup. https://CRAN.R-project.org/package=usethis",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "project_selection.html#sec-extras",
    "href": "project_selection.html#sec-extras",
    "title": "1  How to Select a Project",
    "section": "\n1.5 Extras",
    "text": "1.5 Extras\n\n1.5.1 Glossary\nBooks are set up with lightweight glossary functions from the glossary package.\n\n# code in R/my_setup.R to initialise the glossary on each page\nlibrary(glossary)\nglossary_path(\"include/glossary.yml\")\nglossary_popup(\"click\") # \"click\", \"hover\" or \"none\"\n\nEdit the file glossary.yml with your glossary terms like this:\nalpha: |\n  The threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\np-value: |\n  The probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\nLook up a term from the glossary file with glossary(\"alpha\"): alpha\nDisplay a different value for the term with glossary(\"alpha\", \"$\\\\alpha$\"): \\(\\alpha\\)\nUse an inline definition instead of the glossary file with glossary(\"beta\", def = \"The second letter of the Greek alphabet\"): betaThe second letter of the Greek alphabet\nJust show the definition with glossary(\"p-value\", show = \"def\"):\nShow the table of terms defined on this page with glossary_table():\n\n\n\n\nterm\ndefinition\n\n\n\nalpha\n\n\n\nbeta\nThe second letter of the Greek alphabet\n\n\np-value\n\n\n\n\n\n\n\n1.5.2 FontAwesome\nThe fontAwesome quarto extension allows you to use the free icons with syntax like:\n\n\nTo install it, just run this code in the Terminal pane of RStudio (not the Console pane).\nquarto install extension quarto-ext/fontawesome\n\n\n\n\nWickham, H., Bryan, J., & Barrett, M. (2022). Usethis: Automate package and project setup. https://CRAN.R-project.org/package=usethis",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "project_selection.html#sec-type",
    "href": "project_selection.html#sec-type",
    "title": "1  How to Select a Project",
    "section": "",
    "text": "Clinical Epidemiology or Health Services research (broadly defined as observational research on patterns of disease, medical care, and outcomes). Generally advisable because projects can be sufficiently small, short-timelne (needed for fellows/residents), and clinically relevant… and thus can be more useful in terms of understanding the application of research to clinical care, even if you end up doing a non-research job. Thus, this is the primary focus of this module. However, it is not the only option to consider.\n\n\n\n\n\n\n\nOther types of research you might consider:\n\n\n\n\n\n\nSurvey research - this is the most tractable flavor of ‘prospective’ research for a trainee, as the regulatory requirements and data-accrual rate is amenable to trainee timelines. Often times, this can involved mixed-methods (where quantification is paired with interviews to understand qualitatively what factors influence or lead to observed outcomes)\nMedical Education scholarship - undestandably, many trainees are interested in ways to improve the trainee experience. And there is lots of room for improvement. However, educational scholarship really needs rigorous assessment to generate generalizable knowledge. Thus, good educational research ends up overlapping substantially with the concepts discussed here.trainees who want to be medical educators should learn research skills because curricular evaluation is an increasingly important component of education (and still should be more important than it is)\nBasic science - I do not cover this much here because I’m not qualified to. My impression is that it’s harder to have a successful basic science project as a trainee, but some motivated folks do manage. If you’re basic science committed or curious, fellowship is the time to try. (nothing against basic science per-se.. some of this may pertain, but I have no first hand experience on how to do that well.\nSystematic Review and Meta-analysis - these are a suprisingly large amount of work, and require a team to complete. If pursuing, I would recommend to do as part of a structured experience (e.g. a course) or with a group who has experience with each of the roles (ie. a librarian, duplicate data assessors/extractors, etc.). The main risk of this sort of project is that it’s easy to get scooped, there are many groups competing to meta-analyze new data when it comes, and it’s hard to think of good research gaps that are answerable with available studies but haven’t been aswered (this is, at it’s core, the crux of doing a SRMA)\nOther reviews - ideally, if doing this you’d want to know you have a mentor with an “in” for getting it published - whether that’s an invitation or gravitas in the relevant field.\nSecondary (retrospective) analyses of existing data-sets (e.g. from prospective cohorts or previously completed trials)\nRandomized trials are the gold standard workhorse of medical evidence, but are generally not feasible for most trainees to substantively contribute to, owing to the time required, regulatory burden, and resources required. Challenges associated with prospective research: likely relevant to this: https://www.atsjournals.org/doi/full/10.34197/ats-scholar.2022-0130PS",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Introduction to Clinical Research",
    "section": "About",
    "text": "About\nI am an Assistant Professor of Research in the Department of Critical Care at Intermountain Medical Center. I am core faculty in the University of Utah Pulmonary and Critical Care Medicine Fellowship Program and Internal Medicine Residency. Academically, I focus on hypercapnic respiratory failure epidemiology, clinical reasoning, and research informatics. More information and current projects at reblocke.github.io\n\n# TODO:  \n# Shorten a bit for 1h session (I think not)\n# Change structure to intentionally start people off installing, then progress\n# Trouble shoot regression and chi2 equivalence... may be easier with linear regression.\n# Create an example of using chatgpt to troubleshoot an error",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "project_selection.html#sec-question",
    "href": "project_selection.html#sec-question",
    "title": "1  How to Select a Project",
    "section": "\n1.3 What makes an interesting (good) research question?",
    "text": "1.3 What makes an interesting (good) research question?\nSay you’ve decided you’re going to do clinical research, and you’re going to work with a mentor to identify a suitable project that gets you excited and your mentor(s) can support.\nIt’s worth considering a few things before delving into an investigation on a particular sutdy question:\nFirst, you must have a way to get the needed data in order to answer the question. Collecting\nConsider your research edge: do you have access to data that know one else can get? if so, you can do research that no one else can.\nDo you have an idea that no-one else has had? (believe it or not, there is a large role for clearer thinking on topics.. so the fact that no-one has done it yet doesn’t necessarily mean that no one could do it)\nDo you have a new way of looking at the same data as others? e.g. either new methodologies or hypotheses that can be re-evaluated in light of subsequent evidence. Consider\n\n1.3.1 What type of Question?\nMost research is inferential: attempting to support inferences about causes and effects. Even most research that claims to be about associations is actually about cause and effect . In some ways this makes sense - we want to understand cause and effect so that we can intervene. However, when working with observational data (ie. not running an experiment - such as a trial), it’s very hard to meet all the assumptions required to identify a causal effect (see (sec?)-***)\nDescriptive epidemiology: [ ] find the paper\nHowever, an alternative objective is to just describe something, i.e. descriptive epidemiology. How often does something happen? What is the ultimate outcome for patients who face a particular situation? What is the base rate of occurence of a diagnosis? The assumptions required to support such a question are often much more believable than establishing cause and effect.\nA researcher will often face a dilemma - do we attempt to use the available data to support a potentially dubious argument about cause and effect? Or robustly describe something? As a matter of personal preference - it seems to me usually better to go with the descriptive (ie. more strongly supported) Question\n\n1.3.2 Exploratory vs Confirmatory Research\nYou can’t propose and confirm a theory based on the same data.\nOne way of framing this issue is categorizing research as either exploratory or confirmatory. In exploratory research, you can look through the data and see what relationships are there. You can look at as many potential relationships as you want and see which ones are interesting. However, when you find one - you have to keep in mind that you’ve had many opportunities to find a relationship, and thus metrics based on controllign the false positive rate (such as p-values) aren’t valid.\nIf, however, you already have a theory and you plan to test it in your data - this is confirmatory research. For reasons we’ll explain in the (intro_stat?) section, you should pre-specify your theory, analysis, and criteria for success before looking at the data. Pre-registration is the recommended way to commit to an analysis plan.\nThis is a high bar that most research doesn’t meet. That’s OK, but you just have to be transparent about it.\n[ ] Exploratory research: https://www.tandfonline.com/doi/full/10.1080/02640414.2025.2486871\n\n1.3.3 ‘Bullshit science’\nThere is a lot of low quality medical science that gets done by trainees. By this, I mean research that cannot answer the question it seeks to investigate, either due to issues with study design, data, or analysis. This is not judgement on the trainees (I’ve been there), but a statement about the end-product - which either makes a compelling argument or it doesn’t.\nMethodologists have long recognized ([ ] Bland Altman) the reason for the excess of low-quality medical research: incentives. There’s much less reward for critiquing research than there is in doing it, and you’re not likely to make friends in close-knit research communities by poking wholes in your colleagues work. For trainees particularly, research is correctly perceived as the only way to have a compelling application to the best training programs (whether one ultimately wants to do research or not). This leads to a commodification of research, where the number of posters, presentations, and manuscripts are tallied in an application, rather than judged on whether there was a novel and rigorous contribution.\nThis is, of course, a waste. Trainees doing bullshit research to check boxes both dilutes scientific norms and we’d be better off if we decoupled research from applications. Thankfully, there’s plenty of real research that should be done. So, while it may take a bit more effort and activation, you can always decline to do bullshit research. If you ever think to yourself “I don’t really care what this research finds”, you should consider why you’re going through the process at all. Life is too short to waste your time doing bullshit science.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "stat_software.html",
    "href": "stat_software.html",
    "title": "2  Statistical Software",
    "section": "",
    "text": "2.1 Statistical software options:\nIn the past, researchers had to manually code their own statistical analyses, which was tedious and error-prone. Today, statistical software simplifies this process dramatically. Researchers shift their focus from the technical complexities of computation to understanding statistical logic and applying analyses correctly.\nThis section offers guidance on selecting appropriate statistical programming language, walks through the set-up process, and introduces the basics of conducting statistical analyses using modern tools.\nR, Python, and Stata are the 3 most commonly used languages.\nThere are a few other language options (SPSS, SAS, Julia, etc.), but they are omitted for brevity (generally, not the best modern options)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#statistical-software-options",
    "href": "stat_software.html#statistical-software-options",
    "title": "2  Statistical Software",
    "section": "",
    "text": "R\nStata\nPython\n\n\nCost\nFree\nRequires License\nFree\n\n\nIDE\nRStudio (Posit)\nBuilt in editor\nMany (Visual Code best)\n\n\nStrengths\nBest epi / trials libraries for helpful functions\nSimple functionality; powerful quasi-experimental/Meta-analysis. U of U MSCI uses.\nBest NLP, machine learning libraries\n\n\nWeakness\nClunky syntax; many ‘dialects’\nSimple syntax\nModerately Complex Syntax\n\n\nExplainable Programming*\nQuarto\nNot native (can use Jupyter)\nJupyter, Quarto\n\n\n\n\n\n2.1.1 How to install\nInstructions for how to do the multiple tabs: https://x.com/rlmcelreath/status/1793641224941007261?s=46&t=5eJ6uoTQrbbYTlHIOnRYRg\nIn quarto just use\n\n\nInstallation Instructions\nFor python\n\n\n\n## For R\n\n## For Stata\n\n\nStep 1:\nInstall R Language\nhttps://cran.r-project.org/\n\n\n\nStep 2:\nInstall RStudio\nhttps://posit.co/downloads/\nRStudio is an IDE (development environment)\n\n\nStep 3:\nInstall Quarto (formerly Markdown)\nhttps://quarto.org/docs/get-started/\nFacilitates sharing and explaining your code. Will soon be standard in medical science.\n\n\nStep 4:\nDownload this document\nhttps://github.com/reblocke/fellow_stats\n“fellow_stats.qmd”\n\n\n\nGet a Key -\nDownload the DMG\nInstall\n\n\na\nb\nc\n\n\n\nhttps://quarto.org/docs/interactive/layout.html\nThis page is made using Quarto.\nPackages\nOther people have mostly done all the analyses you’ll want to do:\n\nCurated lists of relevant packages: https://cran.r-project.org/web/views/\n\n`install.packages( )` will install the packages\n`?package`  or `?command` will bring up the documentation\n\nExample: Say you want to do a meta-analysis.\nRelevant packages: https://cran.r-project.org/web/views/MetaAnalysis.html\nThe ‘meta’ package looks good. Try using `install.packages(‘meta’)` to install it, then you can you can access the documentation using `?meta`\nLet’s try an example… I extracted data on all of the trials comparing high O2 to low O2 targets and uploaded to github.\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nhead(data_sheet)\nauthors &lt;- select(data_sheet, author)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nauthor\nyear\ndoi\nnum_randomized\nnum_patients\nnum_high_o2\nhigh_o2_died\nhigh_o2_alive\nnum_low_o2\nlow_o2_died\nlow_o2_alive\ntarget\noutcome\npopulation\n\n\n\nOxygen-ICU\nGirardis\n2016\n10.1001/jama.2016.11993\n460\n432\n216\n74.0\n142.0\n216\n52.000\n164.000\neither\nin-hosp\nAll\n\n\nCLOSE\nPanwar\n2016\n10.1164/rccm.201505-1019OC\n104\n103\n51\n19.0\n32.0\n52\n21.000\n31.000\nspo2\n90d\nAll\n\n\nHYPER2S\nAsfar\n2017\n10.1016/S2213-2600(17)30046-2\n442\n434\n217\n104.0\n113.0\n217\n90.000\n127.000\nsao2 vs fio2\n90d\nSeptic Shock\n\n\nLang2018\nLang\n2018\n10.1111/aas.13093\n65\n65\n38\n9.0\n29.0\n27\n8.000\n19.000\nfio2\n6m\nTBI\n\n\nCOMACARE\nJakkula\n2018\n10.1007/s00134-018-5453-9\n123\n120\n59\n20.0\n39.0\n61\n18.000\n43.000\npao2\n30d\nOHCA\n\n\nICU-ROX\nMackle\n2020\n10.1056/NEJMoa1903297\n1000\n965\n484\n157.3\n326.7\n481\n166.907\n314.093\nspo2\n90d\nunknown\n\n\n\n\n\n\nNow, let’s meta-analyze it:\n\nlibrary(meta)\n\nLoading required package: metadat\n\n\nLoading 'meta' package (version 8.1-0).\nType 'help(meta)' for a brief overview.\n\n#metabin takes events, total (rather than events, nonevents)\nm_ex1 &lt;- meta::metabin(low_o2_died, num_low_o2, high_o2_died, num_high_o2, data = data_sheet, studlab = paste(name, author, year), sm = \"OR\")\nmeta::forest(m_ex1, comb.random = FALSE, lab.c = \"High Oxygen\", lab.e = \"Low Oxygen\", label.left = \"Favors Low O2\", label.right = \"Favors High O2\")\n\nWarning: Use argument 'label.e' instead of 'lab.e' (deprecated).\n\n\nWarning: Use argument 'label.c' instead of 'lab.c' (deprecated).\n\n\n\n\n\n\n\n\nAnd if you want to get really cutting edge, you can do a trial sequential analysis (TSA) on it:\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.2     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: Unknown or uninitialised column: `order`.\nUnknown or uninitialised column: `order`.\n\n\nWarning in RTSA(type = \"analysis\", data = rtsa_df, outcome = \"RR\", mc = 0.9, :\nNB. The required information size adjusted by Diversity (D^2). This might cause\nan under-powered analysis. Consider changing the argument `random_adj` from\n`D2` (default) to `tau2`.\n\n\n$study\n[1] \"character\"\n\n$author\n[1] \"character\"\n\n$year\n[1] \"numeric\"\n\n$doi\n[1] \"character\"\n\n$num_randomized\n[1] \"numeric\"\n\n$num_patients\n[1] \"numeric\"\n\n$nI\n[1] \"numeric\"\n\n$eI\n[1] \"numeric\"\n\n$high_o2_alive\n[1] \"numeric\"\n\n$nC\n[1] \"numeric\"\n\n$eC\n[1] \"numeric\"\n\n$low_o2_alive\n[1] \"numeric\"\n\n$target\n[1] \"character\"\n\n$outcome\n[1] \"character\"\n\n$population\n[1] \"character\"\n\n$study\ninteger(0)\n\n$author\ninteger(0)\n\n$year\ninteger(0)\n\n$doi\ninteger(0)\n\n$num_randomized\ninteger(0)\n\n$num_patients\ninteger(0)\n\n$nI\ninteger(0)\n\n$eI\ninteger(0)\n\n$high_o2_alive\ninteger(0)\n\n$nC\ninteger(0)\n\n$eC\ninteger(0)\n\n$low_o2_alive\ninteger(0)\n\n$target\ninteger(0)\n\n$outcome\ninteger(0)\n\n$population\ninteger(0)\n\n\n\nplot(an_rtsa)\n\nWarning in geom_segment(aes(x = 0, xend = max(sma_timing, na.rm = T), y = y_val1, : All aesthetics have length 1, but the data has 13 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, xend = 0, y = y_val1, yend = y_val2)): All aesthetics have length 1, but the data has 13 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\nMeaning, we’ve passed futility (at 90% power) for a 10% relative risk reduction a few trials ago. Cool.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#reproducible-research",
    "href": "stat_software.html#reproducible-research",
    "title": "2  Statistical Software",
    "section": "\n2.5 Reproducible Research",
    "text": "2.5 Reproducible Research\nFrom a perspective of scientific rigor, sharing should also include the individual patient data to allow researchers to directly replicate or modify the reported analyses. However, despite research participant support for data sharing, privacy concerns and research ethics concerns generally do not permit the sharing of data, even if it has been pseudonomized. This is an active space where “ideal” and “actual” are far apart… but the current takeaway is that individual patient data should not ben shared unless that was explicitly part of the IRB authorization.\nFurther reading (R focus): https://raps-with-r.dev/intro.html",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#vibe-coding-and-related-concepts.",
    "href": "stat_software.html#vibe-coding-and-related-concepts.",
    "title": "2  Statistical Software",
    "section": "\n2.3 ‘Vibe Coding’ and related concepts.",
    "text": "2.3 ‘Vibe Coding’ and related concepts.\n\nGPT usage.\n\nLarge Language Models: Options:\n\nOpenAI Chat GPT (requires subscription for best performance; custom GPTs)\nGithub CoPilot (programming specific)\nMicrosoft CoPilot - access to GPT4 = free through University of Utah\n\nCopilot:\n\nVisit bing.com/chat.\nSelect “sign in with a work or school account” under the Sign in icon in the upper right corner of the page.\nEnter your unid@umail.utah.edu and uNID password.\nComplete Duo two-factor authentication.\nThe conversation is protected when a green shield appears in the upper right corner next to your username. It is critical to verify that the green shield is present for all conversations.\n\nPrompt Engineering:\n\nhave the GPT take the persona that you want\nspell out the chain of thougt that you want the GPT to take (either multiple steps in 1 prompt or several prompts building on one another works)\nGive examples or specifications of what you want done. [this is particularly useful because the documents you give it can form a context and examples]. \n\nHow I used GPT4 creating this workbook:\n\n\nThe prompt I used to create the above example.\n\nU of U Resources - \n\nOne Data Science Hub Workshops: https://utah-data-science-hub.github.io/education_archived.html  \nRequest CTSI help: https://ctsi.utah.edu/cores-and-services/triad \nIntuitive Biostatistics by Harvey Motulsky - https://a.co/d/4NCk2bS",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "3  Data",
    "section": "",
    "text": "3.1 Getting Data\nYes - this really is that important to deserve its own chapter.\nwhat are some relevant (to PCCM) data-sets.\nDatabases compiled by NLM https://www.datasetcatalog.nlm.nih.gov/index.html\nThe NHLBI Pooled Cohorts Study harmonized spirometry data from nine U.S. population-based cohorts: 9 Prospective US Cohorts from NHLBI  includig spirometry:  (from Am J Epidemiol. 2018;187(11):2265–2278 )\nhttps://academic-oup-com.ezproxy.lib.utah.edu/aje/article/187/11/2265/5047150\nARIC, Atherosclerosis Risk in Communities\nCARDIA, Coronary Artery Risk Development in Young Adults\nCHS, Cardiovascular Health Study;\nFHS-O, Framingham Heart Study—Offspring Cohort\nHABC, Health, Aging and Body Composition\nHCHS/SOL, Hispanic Community Health Study/Study of Latinos\nJHS, Jackson Heart Study\nMESA, Multi-Ethnic Study of Atherosclerosis;\nSHS, Strong Heart Study.\nSpiromics\nCOPDGene\nTriNetX\nPinc AI Healthcare\nSleep: Sleepdata.org\nNCHS: National Center for Health Statistics Datasets:: https://www.cdc.gov/nchs/nhis/nhis_questionnaires.htm\nhttps://nhis.ipums.org/nhis/aboutIPUMSNHIS.shtml   &lt;— documentation for the NCHS datasets, and an integration of several years with weightings.\nReference for sampling designs - https://stats.oarc.ucla.edu/other/mult-pkg/faq/faq-choosing-the-correct-analysis-for-various-survey-designs/  ; https://stats.oarc.ucla.edu/stata/seminars/survey-data-analysis-in-stata-17/\nNHIS—list of variables pertinent to respiratory health: https://nhis.ipums.org/nhis/userNotes_HP2020.shtml#group14\n— can be linked with the National Death Index\nNational Inpatient Sample Data elements: https://hcup-us.ahrq.gov/db/nation/nis/nisdde.jsp\nNHANES\nMIMIC - III/IV\nEICU - https://www.nature.com/articles/sdata2018178\nSicDB - https://link.springer.com/article/10.1007/s00134-023-07046-3 Salzburg “SICdb (1.0.4) contains 27,386 admissions from 4 different intensive care units (ICUs) at 1 single tertiary care institution of the Department of Anesthesiology and Intensive Care Medicine at the Salzburger Landesklinik (SALK) and Paracelsus Medical University (PMU) between 2013 and 2021.” 1-per-minute. https://www.sicdb.com/\n—-&gt; comment on anonymization https://link.springer.com/article/10.1007/s00134-023-07153-1\nUPDB ***\nNHLBI BioData Catalyst - https://academic-oup-com.ezproxy.lib.utah.edu/jamia/article/30/7/1293/7165700?utm_source=etoc&utm_campaign=jamia&utm_medium=email&nbd=41184264570&nbd_source=campaigner - includes TOPmed, COVID data-sets. Idea = a place for researchers to store these resources\nEDW.\nResearch Networks: (get Limited dataset)\n-PCORNET (can access broad network) - need to submit an IRB to them. Dr. Hess is local contact\n-ACT (smaller version of PCORNET)\n-Clinithink\n-TriNetX\n-Epic Cosmos\nData Science Services (since ~2016) - handles query with research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#formatting",
    "href": "data.html#formatting",
    "title": "3  Data",
    "section": "\n3.2 Formatting",
    "text": "3.2 Formatting\nClean tabular format etc.\nUse excel like a boss, if you’re going to: More excel data https://cghlewis.com/blog/excel_entry/\nFlat files: Flat files: https://evidence.dev/blog/what-is-a-flat-file?utm_campaign=Data_Elixir&utm_source=Data_Elixir_526",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#data-types",
    "href": "data.html#data-types",
    "title": "3  Data",
    "section": "\n3.3 Data types",
    "text": "3.3 Data types\nStep 2: For each data element, consider the data type\n\nBinary (aka dichotomous scale): e.g. Yes or No, 0 or 1\nUnordered Categorical (nominal scale): e.g. Utah, Colorado, Nevada, Idaho\nOrdered Categorical (ordinal scale): e.g. Room air, nasal cannula, HFNC, intubated, ECMO, dead\nContinuous (interval & ratio scales - differ by whether 0 is special): e.g. Temperature (Celsius or Kelvin, respectively)\n\n\n\n\n\n\n\n\n\n\n\n\ndichotomous\nnominal\nordinal\ninterval\n\n\na.ka.\nbinary\ncategorical\nordered categorical\ncontinuous\n\n\nn\nX\nX\nX\nX\n\n\n%\nX\nX\nX\nX\n\n\nmin\n\n\nX\nX\n\n\nmax\n\n\nX\nX\n\n\nrange\n\n\nX\nX\n\n\nmode\nX\nX\nX\nX\n\n\nmean\n\n\n\nX\n\n\nmedian\n\n\nX\nX\n\n\nIQR\n\n\nX\nX\n\n\nStd. dev.\n\n\n\nX\n\n\nStd. err.\n\n\n\nX\n\n\n\nFrom: Stoddard GJ. Biostatistics and Epidemiology Using Stata: A Course Manual. Salt Lake City, UT: University of Utah School of Medicine.\nTODO: not sure this stuff should live here vs elsewhere:\nStep 3: Visualize the distribution of each data-point (detect outliers, data entry errors, etc.)\nDarren’s hypothetical code lives in a spreadsheet “darren_proj.xlsx”:\nHere is some code that loads the excel spreadsheet into R (we’ll revisit)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nIt’s already (mostly) clean.\nLet’s summarize it:\n\nsummary(darren_data_sheet)\n\n   patient_id    splenectomy        prox_v_dist           qanadli     \n Min.   : 1.00   Length:20          Length:20          Min.   : 2.00  \n 1st Qu.: 5.75   Class :character   Class :character   1st Qu.: 3.75  \n Median :10.50   Mode  :character   Mode  :character   Median :10.00  \n Mean   :10.50                                         Mean   :10.30  \n 3rd Qu.:15.25                                         3rd Qu.:15.00  \n Max.   :20.00                                         Max.   :25.00  \n   got_cteph?       hosp          \n Min.   :0.00   Length:20         \n 1st Qu.:0.00   Class :character  \n Median :0.00   Mode  :character  \n Mean   :0.25                     \n 3rd Qu.:0.25                     \n Max.   :1.00                     \n\n\nHmmm.. what’s wrong with this?\nR need to be told that the binary variables are binary (and not characters)\n\nlibrary(dplyr)\n\n# Convert 'y'/'n' in the splenectomy column to TRUE/FALSE\ndarren_data_sheet &lt;- darren_data_sheet %&gt;%\n  mutate(splenectomy = ifelse(splenectomy == \"y\", TRUE, FALSE))\n\n# Assuming darren_data_sheet is your dataframe\ndarren_data_sheet &lt;- darren_data_sheet %&gt;%\n  mutate(`got_cteph?` = ifelse(`got_cteph?` == 1, TRUE, FALSE))\n\nLet’s visualize each element:\n\nlibrary(ggplot2)\n\n# First, the binary ones\n\n# Plot for splenectomy\nggplot(darren_data_sheet, aes(x = factor(splenectomy))) +\n  geom_bar() +\n  labs(title = \"Distribution of Splenectomy\", x = \"Splenectomy\", y = \"Count\")\n\n\n\n\n\n\n# Plot for prox_v_dist\nggplot(darren_data_sheet, aes(x = factor(prox_v_dist))) +\n  geom_bar() +\n  labs(title = \"Distribution of Proximal vs. Distal\", x = \"Proximal vs Distal\", y = \"Count\")\n\n\n\n\n\n\n# Plot for got_cteph?\nggplot(darren_data_sheet, aes(x = factor(`got_cteph?`))) +\n  geom_bar() +\n  labs(title = \"Distribution of CTEPH Diagnosis\", x = \"Got CTEPH?\", y = \"Count\")\n\n\n\n\n\n\n\nThe categorical one:\n\n# Bar chart for hosp\nggplot(darren_data_sheet, aes(x = factor(hosp))) +\n  geom_bar(fill = \"coral\", color = \"black\") +\n  labs(title = \"Distribution of Hospital\", x = \"Hospital\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Adjust text angle for better readability if needed\n\n\n\n\n\n\n\nand finally, the continuous one:\n\n# Histogram for qanadli\nggplot(darren_data_sheet, aes(x = qanadli)) +\n  geom_histogram(bins = 30, fill = \"blue\", color = \"black\") +\n  labs(title = \"Histogram of Qanadli Scores\", x = \"Qanadli Score\", y = \"Frequency\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "study_design.html",
    "href": "study_design.html",
    "title": "4  Basics of Study Design",
    "section": "",
    "text": "4.1 Variable Definitions\nIn this section, we’ll cover the basics of study design.\nWe circle back to this after covering the overall logic of scientific argument (the disjunctive syllogism, Chapter 5) and the basics of regression (?sec-intro_regression). The main remaining topics to cover are related to how w ecan set up an analysis to differentiate ‘signal from noise’ - a helpful analogy for understanding the related issues of variation, end-points, and power.\nStructure of scientific argument: disjunctive syllogism\nFor this section, we’ll use somewhat broad stand-ins to represent the types of variables relevant to many types of analyses. I’ll give specific examples from respiratory failure research that might be sensible.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of Study Design</span>"
    ]
  },
  {
    "objectID": "study_design.html#outcome",
    "href": "study_design.html#outcome",
    "title": "4  Basics of Study Design",
    "section": "\n4.2 Outcome",
    "text": "4.2 Outcome\nThe thing you’re interesting in measuring.\nEffect measures:\nRisk ratio\nOdds ratio : https://onlinelibrary.wiley.com/doi/10.1111/test.12391\n\n4.2.1 Considerations in choosing an outcome\nEndpoint considerations: https://hbiostat.org/endpoint/",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of Study Design</span>"
    ]
  },
  {
    "objectID": "study_design.html#confounding",
    "href": "study_design.html#confounding",
    "title": "4  Basics of Study Design",
    "section": "\n4.2 Confounding",
    "text": "4.2 Confounding\nWhats a confounder?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of Study Design</span>"
    ]
  },
  {
    "objectID": "study_design.html#effect-modification-and-interactions",
    "href": "study_design.html#effect-modification-and-interactions",
    "title": "4  Basics of Study Design",
    "section": "\n4.3 Effect Modification and Interactions",
    "text": "4.3 Effect Modification and Interactions",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of Study Design</span>"
    ]
  },
  {
    "objectID": "study_design.html#risk-factor",
    "href": "study_design.html#risk-factor",
    "title": "4  Basics of Study Design",
    "section": "\n4.4 Risk factor",
    "text": "4.4 Risk factor",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of Study Design</span>"
    ]
  },
  {
    "objectID": "study_design.html#counterfactual-reasoning",
    "href": "study_design.html#counterfactual-reasoning",
    "title": "4  Basics of Study Design",
    "section": "\n4.5 Counterfactual reasoning",
    "text": "4.5 Counterfactual reasoning\nAdvanced topics (maybe a little bit)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of Study Design</span>"
    ]
  },
  {
    "objectID": "intro_stats.html",
    "href": "intro_stats.html",
    "title": "5  Statistical Foundations",
    "section": "",
    "text": "5.1 The Disjunctive Syllogism\nThe goal of this section is to build an intuition for what statistical tests accomplish, and what the assumptions are.\nBy the end of this page, you should have an intuitive sense of what a p-value is (and what it is not) and the role that it plays in the argument of a scientific report.\nTo forecast the punchline: We seek to make arguments that observed relationships are not explained by confounding, bias, or chance - and therefore must be causation. P-values summarize how surprising it would be to see the observed data, assuming there is nothing going on and the assumptions of the test hold.\nWe’ll start out abstract, and become more concrete.\nThe root of the problem is that we don’t directly observe cause and effect. Instead, we must make an argument for it.\nThe structure of the usual scientific argument mirrors Sherlock Holmes quote:\nThis “argument-by-excluding-alternatives” is termed the disjunctive syllogism. NOTE: this isn’t the only possible argument - for example, likelihoodism (and by extension, Bayesianism), makes the argument that when comparing a hypothesis to the alternative hypotheses the data would be much less likely. etc.\nIn the language of epidemiology, we’re interested in the relationship between an “exposure” (meant broadly - could refer to a treatment, an occupational exposure, a characteristics, etc.) and an “outcome” (also meant broadly, could be the occurence of an event, a side effect, a health state, etc.).\nIf an exposure and an outcome are associated, there are 4 possible explanations:\nThus, the way we’ll seek evidence for causation is indirect. First, we’ll show there is an association. Then, we’ll make arguments against the possibility of confounding, bias, and chance explaining the association. If the reader accepts there’s an association but that reasons 1-3 are not plausible explanations, they’re left accepting causation as an explanation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#how-to-formulate-your-actual-question-as-a-test",
    "href": "intro_stats.html#how-to-formulate-your-actual-question-as-a-test",
    "title": "5  Statistical Foundations",
    "section": "\n5.3 How to formulate your actual question as a test:",
    "text": "5.3 How to formulate your actual question as a test:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#the-logic-of-a-scientific-argument",
    "href": "intro_stats.html#the-logic-of-a-scientific-argument",
    "title": "5  Statistical Foundations",
    "section": "\n5.4 The logic of a scientific argument:",
    "text": "5.4 The logic of a scientific argument:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#p-values",
    "href": "intro_stats.html#p-values",
    "title": "5  Statistical Foundations",
    "section": "\n5.5 P-values",
    "text": "5.5 P-values\nFrequentist P-values - how suprising would this be if there was nothing going on?\nS-value example… and then twist it so that I don’t say how many times I tried (and I’m incentivized to find one)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#whats-power-got-to-do-with-it",
    "href": "intro_stats.html#whats-power-got-to-do-with-it",
    "title": "5  Statistical Foundations",
    "section": "\n5.7 What’s power got to do with it?",
    "text": "5.7 What’s power got to do with it?\nEverything is underpowered ***\nPower - https://onlinelibrary.wiley.com/doi/full/10.1111/test.12403?campaign=wolearlyview",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_regression.html",
    "href": "intro_regression.html",
    "title": "6  Intro to Regressions",
    "section": "",
    "text": "6.1 What use are regression models?\nStatistical modeling is a bit counter-intuitive, so it makes sense to talk a bit about what we’re trying to accomplish.\nLets first start with why - what is it that regressions accomplish? Then, we’ll discuss the “model” part of regression model - which implies that (much like with statistical tests) there are decisions about how one represents the data that influence the results we’ll get. We’ll end with some practical advice, though there’s obviously a huge corpus of content we can barely scratch the surface of.\nYou can do a few things with a regression model, and it helps to be clear about what you’re aiming for.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Regressions</span>"
    ]
  },
  {
    "objectID": "project_management.html",
    "href": "project_management.html",
    "title": "7  Getting your project done",
    "section": "",
    "text": "7.1 Key Figures\nIn this section, we’ll discuss how to actually make the research happen: what should your workflow be, what are some hints for how to do things quickly and robustly.\nWorkflow….\nStart by making your key figures.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Getting your project done</span>"
    ]
  },
  {
    "objectID": "resources.html#project-management",
    "href": "resources.html#project-management",
    "title": "Appendix A — Other helpful resources",
    "section": "\nA.2 Project Management",
    "text": "A.2 Project Management\n\nA.2.1 Figures\n\nhttps://nrennie.rbind.io/blog/chart-makeover/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Other helpful resources</span>"
    ]
  },
  {
    "objectID": "resources.html#take-your-knowledge-to-the-next-level",
    "href": "resources.html#take-your-knowledge-to-the-next-level",
    "title": "Appendix A — Other helpful resources",
    "section": "\nA.3 Take your knowledge to the next level",
    "text": "A.3 Take your knowledge to the next level\nAdvanced topic list:\nBoostrap Survival Analysis / competing events? Multiple testing /FWER EM algorithm??? Random effects models / GEEs General linear model Propensity Score methods\n\nA.3.1 Bayesian Analysis:\nBayes factors - https://statsedge.org/shiny/LearnBF/\n\nA.3.2 Machine Learning\ntodo",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Other helpful resources</span>"
    ]
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Introduction to Clinical Research",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\nBy the end of this module you will be able to:\n\nSelect a feasible, worthwhile research topic that balances rigor, resources, and personal motivation.\nDraft a reproducible study plan. This entails formulating a testable question, choosing an appropriate design, outlining data acquisition, and creating (and possibly preregistering) an analytic protocol.\nPerform and interpret (frequentist) statistical analyses, including interpreting p-values, classifying data-types, and and avoiding common pitfalls.\nBuild and explain basic regression models (linear, logistic, survival), including understanding when adjustment is required/desired, interpreting the regression coeficients, and communicating results clearly.\nPrepare a manuscript suitable for peer review, applying reporting guidelines (e.g., STROBE) and constructing a coherent argument from introduction through discussion.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "intro_stats.html#what-is-a-statistical-test",
    "href": "intro_stats.html#what-is-a-statistical-test",
    "title": "5  Statistical Foundations",
    "section": "\n5.6 What is a statistical test?",
    "text": "5.6 What is a statistical test?\n[ ] create some examples - choosing a statistical test.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_regression.html#objective-4-understand-the-logic-of-regression-analysis",
    "href": "intro_regression.html#objective-4-understand-the-logic-of-regression-analysis",
    "title": "6  Regressions",
    "section": "",
    "text": "Chance\nConfounding (some other factor influences the exposure and the outcome)\nBias \nOr, causation (a real effect)\n\n\n\n\n\n\nInferential Statistics: Hypothesis testing with confounding control\nDescriptive Statistics: Summarize the strength of association\nPrediction of an outcome (e.g. statistical machine learning)\n\n\n\nIndependent observations (special “mixed models” can relax this)\nThe form of the output variable is correct* \nThe form of the predictor variables are correct\nThe relationship between the predictors are properly specified.**\nAdditional constraints (e.g. constant variance)\n\n\n\nNo model is perfect, but some models are useful\n\n\nMorris moment(TM)\n\n\n\n\n\n\n\n\n\n\nLevel of measurement of outcome variable\nTwo Independent Groups without Confounding Adjustment\nTwo Independent Groups without Confounding Adjustment\n\n\nDichotomous\nChi2 Test\nlogistic regression\n\n\nUnordered categorical\nChi2 Test\nmultinomial logistic regression\n\n\nOrdered categorical\nWilcoxon-Mann-Whitney\nordinal logistic regression\n\n\nContinuous (normally distributed)\nT-test\nlinear regression\n\n\nCensored: time to event\nLog-rank test\nCox regression\n\n\n\n\n\n\n\nFor linear regression: additive change in outcome\nFor logistic regression: multiplicative change in odds of the outcome\nFor Cox regression: multiplicative change in the hazard of the outcome. \n\n\n\n\n\n\n\n\n\nCausal relationship of Splenectomy and CTEPH\n\n\n\n\n\nCausal diagram of Splenectomy, Prox_v_dist, and CTEPH",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regressions</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#objective-3-the-logic-of-frequentist-inferential-statistics",
    "href": "intro_stats.html#objective-3-the-logic-of-frequentist-inferential-statistics",
    "title": "5  Statistical Foundations",
    "section": "",
    "text": "David Hume: causation is never directly observed\n\n\n\nChance\nConfounding (some other factor influences the exposure and the outcome)\nBias\nOr, causation (meaning, a real effect)\n\n\n\nWhen you have eliminated the impossible, whatever remains, however improbable, must be the truth. \n\n\n\n\n\n\n\nSequence\nFlips\nP-value\n\n\nHH\n2 flips\n0.25\n\n\nHHH\n3 flips\n0.125\n\n\nHHHH\n4 flips\n0.0625\n\n\n\n4.32 flips\n0.05\n\n\nHHHHH\n5 flips\n0.03125\n\n\n\n\n\n\nIf 10 people = (1-0.03125)^10 = 0.727.  23.3% of at least 1 HHHHH -&gt; we’re back to weak evidence of a real effect. \nThis is obviously true when multiple tests are reported, but less obviously also true if you try several analyses and choose the “best one” after seeing the result. Hence, prespecification.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevel of measurement of outcome variable\nTwo Independent Groups\nThree or more Independent Groups\nTwo Correlated* Samples\nThree or more Correlated* Samples\n\n\nDichotomous\n\nchi-square or Fisher’s exact test\n\nchi-square or Fisher-Freeman-Halton test\nMcNemar test\nCochran Q test\n\n\nUnordered Categorical\n\nchi-square or Fisher-Freeman-Halton test\n\nchi-square or Fisher-Freeman-Halton test\nStuart-Maxwell test\nMultiplicity adjusted Stuart-Maxwell tests#\n\n\n\nOrdered categorical\nWilcoxon-Mann-Whitney (WMW) test\n\nOld School***: Kruskal-Wallis analysis of variance (ANOVA)\nNew School***: multiplicity adjusted WMW test\n\nWilcoxon sign rank test\n\nOld School# Friedman two-way ANOVA by ranks\nNew School# Mulitiplicity adjusted Wilcoxon sign rank tests\n\n\n\nContinuous\nindependent groups t-test\n\nOld school***: oneway ANOVA\nNew school***: multiplicity adjusted independent groups t tests\n\npaired t-test\nmixed effects linear regression\n\n\nCensored: time to event\nlog-rank test\nMultiplicity adjusted log-rank test\nShared-frailty Cox regression\nShared-frailty Cox regression",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#how-to-formulate-your-scientific-question-as-a-statistically-testable-hypothesis",
    "href": "intro_stats.html#how-to-formulate-your-scientific-question-as-a-statistically-testable-hypothesis",
    "title": "5  Statistical Foundations",
    "section": "\n5.3 How to formulate your scientific question as a statistically testable hypothesis?",
    "text": "5.3 How to formulate your scientific question as a statistically testable hypothesis?\nWithout a doubt, the most common error trainees make when formulating their scientific hypothesis as a (statistically-)testable hypothesis is not realizing that you must define a null hypothesis where there is no effect, then find evidence against it - so-called Null Hypotheses Significance Testing. It goes something like this:\n\nScientifically, I suspect that high PaCO2 levels indicate a patient is at higher risk of 30-day readmission\nThe null hypothesis is that PaCO2 levels are not associated with risk of 30-day readmission\nI then evaluate the numbers of readmissions across the range of PaCO2 levels and ask the question: is there a substantial enough excess in readmissions among patients with higher PaCO2 levels that it’s unlikely to be explainable just by random variation?\nif it’s more suspicious than getting heads 4.32 times in a row, we say that chance alone can’t explain it.\n\n(Note, I’ll need to make separate arguments that confounders or biased assessment of either PaCO2 or readmission could explain it, if I want to make a convincing argument is the PaCO2 per se - for more on that, Chapter 6)\n\n5.3.1 Severity:\nA last topic that I’ve included here - but would often usually be introduced later… but I think warrants early exposure - is statistical severity. If we zoom out - we see that we’re generating exclusionary evidence against “null” hypothesis - but we’re not necessarily summarizing the strength of evidence in favor of a hypothesis.\nHow should we summarize how much evidence for a hypothesis? The idea is that we have evidence in favor of a theory in proportion to the strength of challenges it has survived. For example, if theres a theory that makes a prediction that would be extremely unlikely to occur unless the theory is true -e.g. einstein predicting the gravity-induced curvature of light - but then the evidence does NOT disprove it… that’s pretty strong evidence - because that test was severe.\nSimilarly, if you prespecify an analysis - stick to the statistical analysis plan, and find an effect - that’s a much stronger challenge than if you have a lot of ways to slice the data or analyze the effect. Prespecification is more severe.\nApplied to your design - you want to balance power (see Chapter 4) and severity. ***",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "index.html#must-you-learn-to-do-your-own-stats",
    "href": "index.html#must-you-learn-to-do-your-own-stats",
    "title": "Introduction to Clinical Research",
    "section": "Must you learn to do your own stats?",
    "text": "Must you learn to do your own stats?\nStrictly speaking, no. Not every successful clinician-researcher masters statistics or study design. Research increasingly involves collaborative teams, where methodologists and/or statisticians can handle technical details. In fact, some advocate for involving a statistician in every research project, a model that is more feasible in well-resourced settings.\nHowever, relying exclusively on others for statistical expertise has important drawbacks:\n\nSpeed: Conducting at least basic analyses yourself substantially accelerates research.\nEffectiveness: Understanding key statistical concepts enables more productive interactions with the rest of the research team, as you’re more likely to understand key issues and understand the proposed solutions.\nIndependence: Dependence on external expertise restricts the range of research questions you can feasibly explore.\nEducational Value: A stated reason (see note) that many training programs retain research requirements is that familiarity with study design and statistics directly enhances clinical practice by improving your interpretation of research literature. Outsourcing related tasks will not help you critically evaluate evidence or apply findings effectively to patient care.\n\nNote: There are less complimentary reasons that explain why such requirements actually persist.\nFor these reasons and others, I strongly recommend acquiring statistical and study design skills, even though it’s not strictly required in all cases. I still recommend it even if you have access to statisticians and methodologists because it will help you interact with them, and it’ll help you appraise the research.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Introduction to Clinical Research",
    "section": "About the author",
    "text": "About the author\nI am an Assistant Professor of Research in the Department of Critical Care at Intermountain Medical Center and core faculty in the University of Utah Pulmonary and Critical Care Medicine Fellowship Program and closely involved with the University of Utah Internal Medicine Residency. I earned a Master of Science in Clinical Investigation in 2024. My academic research primarily addresses the epidemiology of hypercapnic respiratory failure, clinical reasoning, and research informatics. For more information about my current projects, visit reblocke.github.io",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#todo-list",
    "href": "index.html#todo-list",
    "title": "Introduction to Clinical Research",
    "section": "TODO List:",
    "text": "TODO List:\n\n# Trouble shoot regression and chi2 equivalence... may be easier with linear regression.\n# Create an example of using chatgpt to troubleshoot an error\n# Find statisticians in all projects citation - and set up references/bibliography\n\nIf you find errors or have suggestions (content, resources, corrections, anything), email me and {first}dot{last}at imail.org",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "project_selection.html#sec-mentor",
    "href": "project_selection.html#sec-mentor",
    "title": "1  How to Select a Project",
    "section": "\n1.2 What Kind of Mentor(s) Do You Need?",
    "text": "1.2 What Kind of Mentor(s) Do You Need?\nThe perfect mentor doesn’t exist, and even good mentors are challenging to identify.\nThe concept of “mentoring up” has a lot of value. Essentially, the idea is to be thoughtful about what you want from a mentor, and being proactive about facilitating that.\nI think there is a spectrum from “entirely under your mentors wing” (your mentor has a project in mind and you do it) to “free-range mentorship” (you have an idea, but enroll a mentors help). In the case where there is an available mentor that is working on something that is working on exactly what you’re interested in, there’s no trade-off. The classic arrangement is entirely under your mentors wing - and this has the highest chance of getting a worthwhile result for the least amount of trouble/work.\nHowever, trainee-driven research has it’s benefits and can work [ ]EBTapper Paper. First, the rate-limiting resource is enthusiasm, so choosing an idea you’re excited about (ie. addressing a gap you feel passionate about) may ultimately be a good strategy. Furthermore, the process of identifying a gap, and finding a feasible way to address that gap really is the most interesting bit of research - so it’s a shame to cede that roll to the mentor. There’s something fulfilling about seeing an idea through from start to finish. If you do opt for a proposal that is more free-range, ensure your mentor is still going to be adequately supportive and can help to get the resources you may need.\nConversely, you should know that a trainee-initiated research project is going to be riskier and more work, so make the choice informed by how motivated you are to drive your research forward. I’d advice you to not go all the way in either direction.\nFood for thought - but consider taking a bit more autonomy in project creation than mentors might suggest.\nGreat summarization of early career approach: https://www.aasurg.org/blog/moving-forward-going-faster-scaling-impact-strategies-to-develop-early-career-surgeon-scientists/\nFigure idea: How do you find a good study question? (unique dataset? unique analysis? - why hasn’t it been done before?) - there is always a balance between what you find interesting, what potential mentors are working on, and what will be feasible in a fellow timeline. [ ] venn diagram.\nMatthew Effect https://x.com/jenlovechem/status/1755590367477436695?s=46&t=5eJ6uoTQrbbYTlHIOnRYRg Yikes - Mobility, male, and institutional funding matter - https://x.com/elife/status/1755029870784954520?s=46&t=5eJ6uoTQrbbYTlHIOnRYRg",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "project_selection.html#todo-list",
    "href": "project_selection.html#todo-list",
    "title": "1  How to Select a Project",
    "section": "TODO List:",
    "text": "TODO List:\n\n# Find the EB Tapper citation on fellow-driven research \n# Set up citations and glossary\n#",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "study_design.html#variable-definitions",
    "href": "study_design.html#variable-definitions",
    "title": "4  Basics of Study Design",
    "section": "",
    "text": "4.1.1 Exposures\nThe ‘cause’ that we investigate is generally termed the exposure. This could represent a specific exposure (in the common language sense), like air pollution. But it is also used to refer to treatment group assignment of 0 or 1 in a trial, or things that aren’t modified per se, like race/ethnicity, if we’re interested on the influence of that variable on our outcome of interest.\n\n4.1.2 Outcome\nThe thing you’re interesting in measuring is referred to as the outcome. Choosing an outcome is not as simple as it would seem. Consider a trial of a medication for ARDS.\nA common argument is made for using “hard outcomes”, like whether a patient died or not 90-days after they were enrolled in the trial. A benefit to this type of outcome is that when an effect is found, there’s little doubt as to whether it’s important or how to quantify the magnitude. We can then choose one of a handful of measure to summarize the effect: a risk ratio (risk of death among exposed over risk of death among non-exposed), the risk difference (risk of death among exposed - risk of death in the unexposed), or the odds ratio ( [exposed alive / exposed dead] / [nonexposed alive / non-exposed dead] Odds ratio : https://onlinelibrary.wiley.com/doi/10.1111/test.12391 ).\nHowever, the downside is that the outcome doesn’t contain much information, so a binary outcome is not very efficient. All we get is whether a patient died or not at 90-days. For the exposure to be shown to have an effect, there must be enough patients where they would have died if they hadn’t received the treatment, but would survive with it. For everyone else (they were destined to live or destined to die regardless of the exposure), we don’t learn anything. Consequently, it takes a lot of patients in the trial to convincingly separate signal from noise.\nA variety of more informative end-point could be used: - you could define an ordinal outcome like: alive and independent at 90-days, alive but discharged to facility, alive but still in acute care, or dead - which increases the power as you now can learn something from patients who would potentially switch categories. Sometimes this effect is quantified using a win ratio or a proportional odds ratio - you could look at time to death, knowing that death won’t be observed in many patients (called right censoring). This means that you’d capture even the patients where the treatment kept them alive for longer, but not enough for their to be a counterfactual difference at 90-days (which generally increases the power, but might not mirror what we care about so much). This is a more efficient option when more people have the outcome. - you could model the duration of respiratory support on each day in the following period, such as using a longitudinal ordinal model. That way, you have many opportunities to learn whether the exposure is having an effect (each day, it can be influencing the amount of ventilator support). However, the outcome of the analysis may be harder to quantify and also may be less patient centered.\nEndpoint considerations: https://hbiostat.org/endpoint/\n\n4.1.3 Considerations in choosing an outcome\n‘Signal to noise’: [ ] maybe move this to the study design one? I think so - cover power at the same time.\nif the alternative hypothesis is that the coin is subtly imbalanced, it’ll be a much harder to detect signal. This is the logic of power analysis - if you’re looking for a subtle signal (e.g. small difference, noisy data, rare events), you’ll need a bigger study.\nHowever, there is no free lunch: a statistical tests makes assumptions about the data, and if those assumptions hold in reality, then the implications from the analysis follow.\nExample interpretation:\nIf the p-value from a Chi2 test is P=0.03 - we say it’s as unlikely this would occur from just chance as it would be to flip a fair coin heads 5 times in a row.\nIF observations are independent; both variables are categorical; there are enough observations of each, then it is unlikely chance alone explains the difference. \n[ ] cover signal to noise ratio: history? https://www.sensible-med.com/p/doing-statistics-can-be-difficult\nWhat’s power got to do with it?\nEverything is underpowered ***\nPower - https://onlinelibrary.wiley.com/doi/full/10.1111/test.12403?campaign=wolearlyview",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of Study Design</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#the-disjunctive-syllogism",
    "href": "intro_stats.html#the-disjunctive-syllogism",
    "title": "5  Statistical Foundations",
    "section": "",
    "text": "David Hume: Causation is never directly observed\n\n\n\nWhen you have eliminated the impossible, whatever remains, however improbable, must be the truth. \n\n\n\n\n\nConfounding (some other factor, the confounder, influences the likelihood of the exposure and the likelihood of the outcome through other mechanisms)\nBias (some non-random distortion of the measurements in a study)\nChance\nOr, causation (meaning, a real effect)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#whats-a-p-value-good-for",
    "href": "intro_stats.html#whats-a-p-value-good-for",
    "title": "5  Statistical Foundations",
    "section": "\n5.2 What’s a P-value good for?",
    "text": "5.2 What’s a P-value good for?\nTo provide evidence for cause and effect, we need compelling arguments against confounding, bias, and chance. Arguments against confounding are best done by by study design (e.g. randomization) but can be addressed using statistical methods (see Chapter 6). Bias is best addressed by careful choice of instruments/assessments, but can also be supported by elements of study design (e.g. blinding).\nWhether or not chance is a plausible explanation or not is where P-values come in. Repeated for emphasis: P-values ONLY address the plausibilit of chance explaining an apparent association. Having a low p-value tells you nothing about whether confounding or bias could explain an association - you need other arguments for that.\n\n5.2.1 Intuitively, what is a p-value?\nIf there was no association, how surprising would it be to see the observed data?\n\nif there is a large p-value, it would not be very surprising to see the observed data by just the play of chance.\nif the p-value is small, then it would be quite surprising to see the observed data if there’s really nothing going on.\n\nNote: the P-value is not comparing one hypothesis to another (ie. an alternative hypothesis). Nor is it saying how likely it would be to find an effect if there is something going on - it’s just saying: “if there’s actually nothing going on here (ie. no causal effect, no confounding, no bias), how unlikely would this finding be?\nThis is a conditional probability: “IF nothing is going on” (or, assume nothing is going on), then, how unlikely would this be. The notation is P ( observed_data | nothing going on).\nBox: note that p-values are NOT a measure of how strong evidence is. A finding with p-value 0.03 isn’t necessarily less robust even than p-value 0.001 - even if there is no bias or confounding. Consider, a huge study can have a very small p-value (unlikely chance.. but could be a small amount of bias, confounding, or a trivial effect), or a smaller study could have a larger p-value. This is termed “Lindley’s Paradox”\n\n5.2.2 How surprising is too surprising?\nArguing by coincidence begs the question: how surprising does something have to be before you say “there must be something going on here!”.\nLuckily, you can convert P-values to coin flips to get an intuitive sense (look inside the box for the actual conversion, but feel free to skip of logarithms make you queasy).  \nBox: “Shannon Transform: S-value”\nIn a world where the null value is true (ie. either heads or tails is equally likely to occur; but it’s going to be one of those two), you can characterize how suprising it would be to see a particular sequence. this is calculated by S-value = -log_2_ P-value.\nHere’s the setup… say I start flipping a coin, how many consecutive “Heads” need to occur before you’ll suspect it’s not a fair coin? \n\n\nSequence\nFlips (S-Value)\nP-value\n\n\nHH\n2 flips\n0.25\n\n\nHHH\n3 flips\n0.125\n\n\nHHHH\n4 flips\n0.0625\n\n\n\n4.32 flips\n0.05\n\n\nHHHHH\n5 flips\n0.03125\n\n\nMore at: https://stat.lesslikely.com/s-values/\nIMPORTANT POINT: the p-value is NOT the probability that the coin is biased. It is the probability of seeing that result (or more extreme) ASSUMING the coin is biased. \nSo, to answer the question from the lede… the specific answer to “How surprising is too surprising for chance to be a compelling explanation? Since we, as a community, have agreed that P = 0.05 is the threshold… a bit more surprising than 4 consecutive heads with a fair coin is too suspicious.\nBox: Multiplicity understanding multiplicity: If we all flipped a coin 5 times, what is the chance that one of us would get 5 heads in a row? \n\nIf 10 people = (1-0.03125)^10 = 0.727.  23.3% of at least 1 HHHHH -&gt; we’re back to weak evidence of a real effect. \nThis is obviously true when multiple tests are reported, but less obviously also true if you try several analyses and choose the “best one” after seeing the result. Hence, prespecification.\n\nBox: What, then, do confidence intervals mean?\nNote - it’s NOT that there’s a 95% chance that the true value is within this range. It’s if you execute this method 100 times, you’d the value to be outside the range 5% of the time. [ ].gif\nHow do you choose the right test?\nWe have to go one step further to understand where the P-value comes from because of one sneaky clause that I snuck into the definition of the p-value. Quote: “P-values summarize how surprising it would be to see the observed data, assuming there is nothing going on and the assumptions of the test hold.”\nLike regression models (covered soon Chapter 6), there are many statistical tests, each of which requires assumptions to be made. The main characteristics of the data that allow one to choose an appropriate tests are how many groups, the size of the groups, the type of variable, and whether measurements are independent of each other (ie. knowing one observation tells you nothing about another) or correlated (e.g. observation from the same patient at two different times)?\n\n\n\n\n\n\n\n\n\n\nLevel of measurement of outcome variable\nTwo Independent Groups\nThree or more Independent Groups\nTwo Correlated* Samples\nThree or more Correlated* Samples\n\n\n\nDichotomous\n(e.g. yes/no)\n\n\nchi-square or Fisher’s exact test\n\nchi-square or Fisher-Freeman-Halton test\nMcNemar test\nCochran Q test\n\n\n\nUnordered Categorical\n(e.g. red/blue/grey)\n\n\nchi-square or Fisher-Freeman-Halton test\n\nchi-square or Fisher-Freeman-Halton test\nStuart-Maxwell test\nMultiplicity adjusted Stuart-Maxwell tests#\n\n\n\n\nOrdered categorical\n(e.g. bad, neutral, good)\n\nWilcoxon-Mann-Whitney (WMW) test\n\nOld School***: Kruskal-Wallis analysis of variance (ANOVA)\nNew School***: multiplicity adjusted WMW test\n\nWilcoxon sign rank test\n\nOld School# Friedman two-way ANOVA by ranks\nNew School# Mulitiplicity adjusted Wilcoxon sign rank tests\n\n\n\n\nContinuous\n(e.g. BMI)\n\nindependent groups t-test\n\nOld school***: oneway ANOVA\nNew school***: multiplicity adjusted independent groups t tests\n\npaired t-test\nmixed effects linear regression\n\n\n\nTime to event\n(e.g. survival)\n\nlog-rank test\nMultiplicity adjusted log-rank test\nShared-frailty Cox regression\nShared-frailty Cox regression\n\n\n\nFrom: From: Stoddard GJ. Biostatistics and Epidemiology Using Stata: A Course Manual. Salt Lake City, UT: University of Utah School of Medicine.\nSide-box: How can you collaborate effectively with a statistician? They will know these assumptions and can tell you when your analyses makes dubious assumptions (if you communicate the constraints of the problem correctly). THus, the reason to know about these things, even if you’re enrolling the help of a statistician, is to realize when you’re making a decision that bakes assumptions into your analysis - and you want to help the statistician understand the clinical situation so they can help match)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_regression.html#what-use-are-regression-models",
    "href": "intro_regression.html#what-use-are-regression-models",
    "title": "6  Intro to Regressions",
    "section": "",
    "text": "You can use regressions to make predictions (e.g. using a linear regression to predict the FEV1 by age, sex, and height)\nYou can summarize the association between two variables after “adjusting” for the effect of other variables in the model (e.g. the effect of changing a treatment indicator from 0 to 1, correcting for other observed covariates)\n\n\n6.1.1 When do you (not) want a corrected estimate?\nTo answer this question, we need to differentiate a couple of related terms that describe the way that an Exposure (E) -&gt; Outcome (O) relationship can be distored by a third variable. To explain this, we’ll use Directed Acyclic Graphs, which can be created with tools like Dagitty.\nThe idea is that you draw the causal connections between variables to define how you think they are related. It’s helpful for our purposes because it can define the different ways that three variables may interrelate.\n\n6.1.1.1 Confounder (C)\nAs mentioned earlier, a confounder is something that influences both the likelihood of exposure and the likelihood of the outcome via other mechanisms than through the exposure.\nFor example, consider the risk of death in patients with elevated PaCO2 levels. Say that older patients are more likely to get hypercapnia. In an unadjusted analysis, PaCO2 levels might be associated with death because age is associated with likelihood of hypercapnia, and likelihood of death via a variety of mechanisms. In this case, what we’d really like to do is evaluate whether PaCO2 levels remain associated with the risk of death after controlling for age (a confounder).\n[ ] DAG.\n\n6.1.1.2 Mediator (M)\nAnother way\n(cover this here? or elsewhere)\n\n6.1.1.3 Effect Modifier\nEffect modifiers (and the related concept of interactions) refer to situations where a third variable influences the effect of the exposure on the outcome. As a hypothetical example, a bacterial pathogen is presumably an effect modifier for response to antibiotics on community acquired pneumonia: if bacterial CAP, antibiotics help, and if viral it probably doesn’t.\nThis might seem like a very distinct concept from mediators and confounders - and it is. But the reason I bring it up here is that it’ll appear similar in regression analyses if not specified.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Regressions</span>"
    ]
  },
  {
    "objectID": "intro_regression.html#understand-the-logic-of-regression-analysis",
    "href": "intro_regression.html#understand-the-logic-of-regression-analysis",
    "title": "6  Intro to Regressions",
    "section": "\n6.2 Understand the logic of regression analysis",
    "text": "6.2 Understand the logic of regression analysis\nRecall, if there is an association between an ‘exposure’ and an ‘outcome’, there are 4 possible explanations\n\nChance\nConfounding (some other factor influences the exposure and the outcome)\nBias \nOr, causation (a real effect)\n\nRandomization = takes care of some (but not all) of the other reasons for an association = can convince a skeptic. However, observational research must make arguments (based on assumptions) and they must be explicit. Randomization addresses point 2 (essentially, converts it to point 1, in that only chance confounding can occur)\nFor non-randomized data, you must make an argument against point 2. This is the most common use of regression. \n[the methods section of your paper is the argument against point 3; pull in RECORD/STROBE recs]\nThere are at least 3 uses of regression models: \n\nInferential Statistics: Hypothesis testing with confounding control\nDescriptive Statistics: Summarize the strength of association\nPrediction of an outcome (e.g. statistical machine learning)\n\nRegression comes with additional assumptions: \n\nIndependent observations (special “mixed models” can relax this)\nThe form of the output variable is correct* \nThe form of the predictor variables are correct\nThe relationship between the predictors are properly specified.**\nAdditional constraints (e.g. constant variance)\n\nThus the logic is: if the assumptions of the models hold in reality, then the described relationships are valid\n\nNo model is perfect, but some models are useful\n\n\nMorris moment(TM)\n\nOutput variable (aka the dependent variable, predicted variable) form determines the type of regression : \n\n\n\n\n\n\n\n\nLevel of measurement of outcome variable\nTwo Independent Groups without Confounding Adjustment\nTwo Independent Groups without Confounding Adjustment\n\n\nDichotomous\nChi2 Test\nlogistic regression\n\n\nUnordered categorical\nChi2 Test\nmultinomial logistic regression\n\n\nOrdered categorical\nWilcoxon-Mann-Whitney\nordinal logistic regression\n\n\nContinuous (normally distributed)\nT-test\nlinear regression\n\n\nCensored: time to event\nLog-rank test\nCox regression\n\n\n\nFrom: From: Stoddard GJ. Biostatistics and Epidemiology Using Stata: A Course Manual. Salt Lake City, UT: University of Utah School of Medicine.\nInterpretation:\nRegression coefficient = What change in the outcome do you expected if you change the predictor by 1 unit, holding all other variables constant\n\nFor linear regression: additive change in outcome\nFor logistic regression: multiplicative change in odds of the outcome\nFor Cox regression: multiplicative change in the hazard of the outcome. \n\nExample:\nConsider, if we want to test whether ‘splenectomy’ and ‘got_cteph?’ are associated, we could use a chi2 test:\n\n#chi2_test_result &lt;- chisq.test(darren_data_sheet$splenectomy, darren_data_sheet$`got_cteph?`)\n#print(chi2_test_result)\n\nAlternatively you could specify a logistic regression\n(“GLM” standards for ‘general linear model’. Logistic regression is a type of glm where the family is binomial)\n\n#logistic_model &lt;- glm(`got_cteph?` ~ splenectomy, data = darren_data_sheet, family = binomial())\n\n# Output the summary of the model to see coefficients and statistics\n#summary(logistic_model)\n\n\n\nCausal relationship of Splenectomy and CTEPH\n\n(https://www.dagitty.net/dags.html Daggity is a tool to specify such diagrams)\n\n#logistic_model_updated &lt;- glm(`got_cteph?` ~ splenectomy + prox_v_dist, data = darren_data_sheet, family = binomial())\n#summary(logistic_model_updated)\n\n\n\nCausal diagram of Splenectomy, Prox_v_dist, and CTEPH\n\nConsider: do you want the adjusted or the unadjusted estimate? \nHint: it depends….\nDistributions:\n\nRegressions -\n[ ] create the linear regression interpretation and example.\nlogic of different choices… ie. “under the following assumptions this is the estimate” - thus, if you make different assumptions, you make a different answers. Therefore, knowing the assumptinos are very important for knowing whether the result is believable.\nModels - you choose a way to distill the relationships that are contingent on certain assumptions - and if those assumptions hold, your conclusions follow.\nDAGs What does it mean to control? https://idlhy0218.github.io/page%20building/blog.html#control\nConceptually - you are modeling something… all models are wrong, some models are useful. You just need to know the assumptions you are implying by your choice, so that you can make an argument about whether the assumptions are warranted or not .\nFunctional forms *** dichotomization vs flexible models\nWhat type of questions can regression be used? - controlling for the effect of one thing on another. - prediction\n— how to do in python: python resources for how to do that section of it: https://ajthurston.com/predprobs?utm_source=substack&utm_medium=email",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Regressions</span>"
    ]
  },
  {
    "objectID": "stat_software.html#sec-options",
    "href": "stat_software.html#sec-options",
    "title": "2  Statistical Software",
    "section": "",
    "text": "R\nPython\nStata\n\n\nCost\nFree\nFree\nRequires License\n\n\nIDE\nRStudio/Posit\nMany, Visual Code is good\nBuilt in editor\n\n\nStrengths\nBest libraries for epidemiology, trial statistics.\nBest libraries for text processing, machine learning, AI integration\nSimple syntax; powerful quasi-experimental/meta-analysis packages. Used by U of U MSCI.\n\n\nWeakness\nClunky syntax; many ‘dialects’\nOverkill for many, complex development environment\nClunkiest machine learning, explainable programing, cost.\n\n\nExplainable programmingWriting analytic code in a clear, structured, and understandable way so that readers (including reviewers, collaborators, and future researchers) can easily follow and replicate the steps of your analysis. In quantitative disciplines such as mathematics and computer science, providing such code is already standard practice. Medicine is beginning to adopt this approach, recognizing its importance for transparency and reproducibility.\nQuarto\nJupyter, Quarto\nNot native (though can use Jupyter)\n\n\n\n\n\n\n\nterm\n\n\ndefinition\n\n\n\n\nExplainable programming\n\n\nWriting analytic code in a clear, structured, and understandable way so that readers (including reviewers, collaborators, and future researchers) can easily follow and replicate the steps of your analysis. In quantitative disciplines such as mathematics and computer science, providing such code is already standard practice. Medicine is beginning to adopt this approach, recognizing its importance for transparency and reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#sec-install",
    "href": "stat_software.html#sec-install",
    "title": "2  Statistical Software",
    "section": "\n2.2 How to install",
    "text": "2.2 How to install\nChoose the tab for the language(s) you plan to use:\n\n\nR\nPython\nStata\n\n\n\n\n\n1:\nInstall R Language\nhttps://cran.r-project.org/\nThis installs the base programming language\n\n\n2:\nInstall RStudio\nhttps://posit.co/downloads/\nRStudio is an IDEIntegrated Development Environment, a program that allows for writing, running, and debugging code within a single program. (integrated development environment) that allows you to write, execute, and debug code from within a single program.\n\n\n3:\nInstall Quarto (formerly Markdown)\nhttps://quarto.org/docs/get-started/\nFacilitates sharing and explaining your code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1:\n\n\n\n\nInstall Python Language and dependencies\n\n\n\n\nhttps://github.com/conda-forge/miniforge?tab=readme-ov-file\n\n\n\n\nThis (mini-forge) installs the base Python programming language, the things it depends on, and many useful packages\n\n\n\n\n\n\n2:\n\n\n\n\nCreate an environment\n\n\n\n\nExecute the following commands in a terminal:\n\n\nmamba create -n stats python=3.12 numpy pandas scipy statsmodels scikit-learn lifelines pingouinmatplotlib seaborn plotnine pyreadstatjupyterlab ipykernel jupytext –channel conda-forge\n\n\nconda activate stats\n\n\n\n\nThis sets up an environment (controls the versions and packages that are used).\n\n\n\n\n\n\n3:\n\n\n\n\nInstall Visual Code\n\n\n\n\nhttps://code.visualstudio.com/download\n\n\n\n\nVisual Code is an IDE (integrated development environment) that allows you to write, execute, and debug code from within a single program.\n\n\n\n\n\n*Note: there are many IDEIntegrated Development Environment, a program that allows for writing, running, and debugging code within a single program.. Visual Code is a classic one with a lot of functionality, though there are AI enabled ones (e.g. Cursor) that may be more helpful depending on how much programming you plan to do and whether you want to bother with the added complexity (discussed more in Section 2.7).\nYou can also use Quarto for explainable programming in Python - but Jupyter is a more common workflow so we focus on that.\n\n\n\n\n1:\nGet a product key\nif U of U Trainee, contact me\nThis verifies you or your institutions’ purchase\n\n\n2:\nInstall STATA\nhttps://www.stata.com/install-guide/\nIncludes language and IDEIntegrated Development Environment, a program that allows for writing, running, and debugging code within a single program. (integrated development environment)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#sec-packages",
    "href": "stat_software.html#sec-packages",
    "title": "2  Statistical Software",
    "section": "\n2.3 Packages\n",
    "text": "2.3 Packages\n\nFor common statistical analyses in any of these languages, specialized packages already exist that handle these tasks efficiently. Whenever you find yourself manually calculating or coding a statistical procedure, consider that someone likely has already written reliable, tested code that will perform the analysis faster and more accurately. You’ll want to use these packages whenever possible.\nFirst, a few terms: functionA reusable piece of code that performs a specific task. Examples include calculating the mean of a dataset or running logistic regression., argumentsInputs provided to a function so it can perform its task. For instance, a function calculating a mean needs a dataset, while logistic regression requires data, the outcome variable, and predictor variables., packageA curated collection of functions designed to accomplish related tasks. Programming languages come pre-installed with basic packages, but you’ll often download additional packages to access specialized functions. Each language provides straightforward ways to locate and install new packages.\n\n\n\nterm\n\n\ndefinition\n\n\n\n\n\narguments\n\n\nInputs provided to a function so it can perform its task. For instance, a function calculating a mean needs a dataset, while logistic regression requires data, the outcome variable, and predictor variables.\n\n\n\n\nfunction\n\n\nA reusable piece of code that performs a specific task. Examples include calculating the mean of a dataset or running logistic regression.\n\n\n\n\npackage\n\n\nA curated collection of functions designed to accomplish related tasks. Programming languages come pre-installed with basic packages, but you’ll often download additional packages to access specialized functions. Each language provides straightforward ways to locate and install new packages.\n\n\n\n\n\n\nR\nPython\nStata\n\n\n\n\n\nWhere to find packages?\nhttps://cran.r-project.org/web/views/\n\n\nCommand to install packages\n\ninstall.packages(`package_name`)\n\n\n\nHow to access documentation file?\n\n?package_name or ?command_name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere to find packages?\n\n\n\n\n\n\nhttps://anaconda.org/conda-forge\n\n\n\n\nhttps://pypi.org/\n\n\n\n\nhttps://github.com/topics/python\n\n\n\n\n\n\n\n\nCommand to install packages\n\n\n\n\nmamba install package_name (rarely, packages that have not been compiled on conda-forge will require pip install package_name to be used)\n\n\n\n\n\n\nHow to access documentation file?\n\n\n\n\nThe project page on https://pypi.org/ or https://github.com/\n\n\n\n\n\n\n\n\n\nWhere to find packages?\nfindit package_name\n\n\nCommand to install packages\nssc install package_name\n\n\nHow to access documentation file?\nhelp package_name",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#sec-reproducible",
    "href": "stat_software.html#sec-reproducible",
    "title": "2  Statistical Software",
    "section": "\n2.4 Reproducible Research",
    "text": "2.4 Reproducible Research\nAs mentioned before, Explainable programmingWriting analytic code in a clear, structured, and understandable way so that readers (including reviewers, collaborators, and future researchers) can easily follow and replicate the steps of your analysis. In quantitative disciplines such as mathematics and computer science, providing such code is already standard practice. Medicine is beginning to adopt this approach, recognizing its importance for transparency and reproducibility. is an increasingly important idea because programming errors frequently lead to erroneous research results. Understandable analytic code is important for co-authors to understand and verify what you’ve done, as well as facilitating replication and improvements.\nImportantly (and, as will be discussed in Chapter 3), it’s not just the statistical design choices that can have an influence on the observed outcomes. Researchers must decide what data should be included, how the data should be cleaned, whether missing values will be imputed (and if so, how). All this occurs before talk of statistical tests, regressions, or presentation occur.\nBy some estimates, variation in how these pre-processing tasks are done accounts for more variability in findings than the design choices most readers focus on. Accordingly, the data processing code that is shared should include the entire process, from data cleaning to figure generation.\n\n2.4.1 How to share code\n\n\nR\nPython\nStata\n\n\n\nIn R, the easiest way to put together understandable and sharable code is to use Quarto documents (this website is actually is generated in Quarto).\nQuarto creates notebooks that include segments that can contain text and pictures with other sections that contain executable R code. The segments containing R code can be executed in subsequent order - variables are shared between the boxes - thus allowing a long analytic pipeline to be broken up into small understandable chunks.\n\n\n\n\n\n\nTip\n\n\n\nTo create a new Quarto notebook, go to File-&gt;New-&gt;Quarto Document (*.qmd). Leave all the options in their default. Try it to make the below notebook.\n\n\nFor example, a hypothetical analysis might go something like:\n\n2.4.1.1 Example Simulation of Coin Flips:\nOur aim is to simulate the number of heads seen if a coin is flipped 50 times.\nFirst, we create a function that simulates a fair coin flip: 50% probability heads (coded as 1), 50% probability of tails (coded as 0)\n\n## one fair-coin flip: 1 = Heads, 0 = Tails\nflip_coin &lt;- function(n = 1) {\n  sample(c(0, 1), size = n, replace = TRUE)\n}\n\nThen, we write a program that repesents a single simulation: the coin is flipped 50 times\n\n## run one experiment of 50 flips and return #Heads\nsimulate_50 &lt;- function() {\n  sum(flip_coin(50))\n}\n\n## quick demo\nset.seed(42)   # reproducible example\nsimulate_50()\n\n[1] 27\n\n\nThen, we perform 1000 simulations and store the results in an array, which we then present summary statistics on.\n\nset.seed(42)                       # reproducible simulations\nn_sims  &lt;- 1000                    # how many experiments?\nresults &lt;- replicate(n_sims, simulate_50())\n\nsummary(results)                   # five-number summary\nhist(results,\n     breaks = 20,\n     main   = \"Distribution of heads in 1 000 × 50-flip experiments\",\n     xlab   = \"Number of heads\") \n\n\n\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  14.00   22.00   25.00   24.74   27.00   35.00 \n\n\n\n2.4.1.2 Nuts and Bolts\nObviously, this example is a bit contrived, but is meant to show how the text and code can be interleaved to make the resulting output easy to understand. This quarto file can then be shared with collaborators (or posted on a code sharing site, such as github - provided none of the protected health information is hard coded into the text.\n\n\n\n\n\n\nTip\n\n\n\nAs an exercise, see if you can create your own Quarto markdown file, and extend the analysis to generate the interquartile range for the mean number of Heads in 50 flips.\n\n\n\n\n\nIn Python, the easiest way to put together understandable and sharable code is to use Jupyter notebooks.\nJupyter (like Quarto) creates notebooks that include segments that can contain text and pictures with other sections that contain executable Python code. The segments containing Python code can be executed in subsequent order - variables are shared between the boxes - thus allowing a long analytic pipeline to be broken up into small understandable chunks.\n\n\n\n\n\n\nTip\n\n\n\nTo create a new Jupyter notebook, go to File -&gt; New Jupyter Notebook (*.ipynb). Try it to make the below notebook.\n\n\nFor example, a hypothetical analysis might go something like:\n\n2.4.1.3 Example Simulation of 50 Coin Flips\nOur aim is to simulate the number of heads seen if a coin is flipped 50 times.\nFirst, we create a function that simulates a fair coin flip: 50% probability heads (coded as 1), 50% probability of tails (coded as 0)\n\nimport random\n\ndef flip_coin(n: int = 1) -&gt; list[int]:\n    \"\"\"Return a list of 0 / 1 draws; 1 = heads, 0 = tails.\"\"\"\n    return random.choices([0, 1], k=n)\n\nThen, we write a program that repesents a single simulation: the coin is flipped 50 times\n\ndef simulate_50() -&gt; int:\n    \"\"\"Flip a coin 50 × and return the number of heads.\"\"\"\n    return sum(flip_coin(50))\n\nrandom.seed(42)          # reproducible example\nsimulate_50()\n\n25\n\n\nThen, we perform 1000 simulations and store the results in an array, which we then present summary statistics on.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrandom.seed(42)\n\nn_sims  = 1000\nresults = [simulate_50() for _ in range(n_sims)]\n\n# summary statistics\nprint(f\"min  : {min(results)}\")\nprint(f\"max  : {max(results)}\")\nprint(f\"mean : {np.mean(results):.2f}\")\nprint(f\"sd   : {np.std(results, ddof=1):.2f}\")\n\n# visualise\nplt.hist(results, bins=20, edgecolor=\"black\")\nplt.title(\"Heads in 1 000 × 50-flip experiments\")\nplt.xlabel(\"Number of heads\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nmin  : 13\nmax  : 36\nmean : 24.93\nsd   : 3.48\n\n\n\n\n\n\n\n\n\n2.4.1.4 Nuts and Bolts\nObviously, this example is a bit contrived, but is meant to show how the text and code can be interleaved to make the resulting output easy to understand. This Jupyter notebook can then be shared with collaborators (or posted on a code sharing site, such as github - provided none of the protected health information is hard coded into the text.\n\n\n\n\n\n\nTip\n\n\n\nAs an exercise, see if you can create your own Jupyter notebook file, and extend the analysis to generate the interquartile range for the mean number of Heads in 50 flips.\n\n\n\n\n\nStata does not have a native notebook-style way to share code, but instead you have *.do files that contain a script of the relevant commands, and you can add comments using either # , *, or //* *//*\n\n\n\n\n\n\nTip\n\n\n\nTo create a new STATA do file, go to File -&gt; New -&gt; Do File (*.do). Try it to make the below script.\n\n\nStata Code: *.do files dont have an easy way to execute within the web page, so you’ll have to run the below script on your own machine to see the output.\n*-----------------------------------------------------*\n* 1.  Set the number of simulations                  *\n*-----------------------------------------------------*\nlocal nsims   = 1000      // how many experiments?\nlocal nflips  = 50        // flips per experiment\n\nset obs `nsims'           // create `nsims' empty observations\n\n* Optional: reproducibility\nset seed 42\n\n*-----------------------------------------------------*\n* 2.  Simulate 50 fair-coin flips for each experiment *\n*     A \"head\" is coded 1 when runiform() &lt; .5        *\n*-----------------------------------------------------*\nforvalues j = 1/`nflips' {\n    generate byte flip`j' = runiform() &lt; .5\n}\n\n*-----------------------------------------------------*\n* 3.  Count heads in each experiment                  *\n*-----------------------------------------------------*\negen heads = rowtotal(flip1-flip`nflips')\n\n*-----------------------------------------------------*\n* 4.  Inspect the sampling distribution               *\n*-----------------------------------------------------*\nsummarize heads\n\nhistogram heads , ///\n    width(1) start(0.5)          /// each bin spans one integer count\n    title(\"Heads in `nsims' × `nflips'-flip experiments\") ///\n    xlabel(0(5)50)               /// tick every 5 heads; adjust as desired\n    ylabel(, angle(horizontal))\n\n\n2.4.1.5 Nuts and Bolts\nObviously, this example is a bit contrived, but is meant to show how the text in comments and code can be interleaved to make the resulting output easy to understand. This do file can then be shared with collaborators (or posted on a code sharing site, such as github - provided none of the protected health information is hard coded into the text.\n\n\n\n\n\n\nTip\n\n\n\nAs an exercise, see if you can create your own Stata do file, and extend the analysis to generate the interquartile range for the mean number of Heads in 50 flips.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#sec-llm",
    "href": "stat_software.html#sec-llm",
    "title": "2  Statistical Software",
    "section": "\n2.7 Large Language Models (LLMs)",
    "text": "2.7 Large Language Models (LLMs)\nRandomized trials (!) show the professional coders are more productive when using LLMs to assist with code. Novices see larger relative gains because LLMs supply boiler-plate, interface syntax, and “first drafts” of code.\nHowever, LLMs can hallucinate. They can also be tricky to use. Here’s some guidance that holds for any of the frontier company models (OpenAI: chatGPT, Anthropic Claude, Google Gemini)\n\n2.7.1 Minimal viable set up:\n\n\nStep\nWhy\nTool\n\n\n\nUse your local IDEIntegrated Development Environment, a program that allows for writing, running, and debugging code within a single program. with a plain chat tab or desktop app.\nAllows you to (quality)control inputs and outputs - which is harder to do with an integrated IDE (e.g. Cursor) or agentic models.\nBuilt-in browser tab or official desktop app\n\n\nDo NOT upload data to LLMs\nThe companies do not have Business Access Agreements with institutions, and this generally violates the consent/IRB authorization of most studies.\nKeep data local, only share schemas (descriptions of the variables) or mock data.\n\n\nSanity test all LLM-generated code\nHallucinations exist, and often task specifications are subtly wrong\nYou can writing coding tests (so called “Unit Tests”, but should also visualize and consider all code (this is true when not using LLMs)\n\n\nGive the LLM examples of what you want\nLLMs are VERY good at understanding what existing code does - and can often modify much better than create de-novo\nUse things like: here’s the code that generates this figure - modify it so that title is larger.\n\n\nGive the LLM specific instructions, often with steps\nThe more “context” you give the LLM for what you want, the more likely the associations it follows will be relevant\nDetailed instructions in the prompt. More on prompt engineering is available here: https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNever put PHI into a commercial LLM interface. Not even ‘publicly available’ data like MIMIC (which has terms of use that forbid this). Most companies market their LLMs as being able to act as data-analysts on your behalf (meaning, you give it the data and it analyzes it for you). Don’t do this. https://pmc.ncbi.nlm.nih.gov/articles/PMC10937180/\nInstead, use the LLM to help you write statistical code that you then run on your machine.\n\n\n\n\n\n\n\n\n\nMistake\nConsequence\nQuick fix\n\n\n\nSkipping LLM entirely\nSlower learning curve\nUse it for boiler-plate, explaining things, first drafts\n\n\nBlind trust\nSilent bugs ≈ misleading science\nUnit tests, peer-review, benchmark against known outputs\n\n\nVague prompt\nGeneric, unusable code\nInclude language, package, data schema, desired output\n\n\nManual debugging\nTime sink\nFeed the exact error message back to the model\n\n\nPasting PHI\nCompliance breach\nUse synthetic or sampled data; keep true identifiers offline",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#an-example-meta-analyzing-the-effect-of-steroids-in-cap",
    "href": "stat_software.html#an-example-meta-analyzing-the-effect-of-steroids-in-cap",
    "title": "2  Statistical Software",
    "section": "\n2.5 An Example: meta-analyzing the effect of *** steroids in CAP",
    "text": "2.5 An Example: meta-analyzing the effect of *** steroids in CAP\nThis example is chosen to demonstrate how to make a reproducible notebook, download a package, and execute the relevant commands.\nExample - simple meta-analysis in all 3 languages? (all default in STATA?)\ninstall the needed packages (meta, excel spreadsheet import)\ndisplay the data\nmeta-analyze\ngenerate a figure.\nRelevant packages: https://cran.r-project.org/web/views/MetaAnalysis.html\nThe ‘meta’ package looks good. Try using `install.packages(‘meta’)` to install it, then you can you can access the documentation using `?meta`\nLet’s try an example… I extracted data on all of the trials comparing high O2 to low O2 targets and uploaded to github.\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nhead(data_sheet)\nauthors &lt;- select(data_sheet, author)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nauthor\nyear\ndoi\nnum_randomized\nnum_patients\nnum_high_o2\nhigh_o2_died\nhigh_o2_alive\nnum_low_o2\nlow_o2_died\nlow_o2_alive\ntarget\noutcome\npopulation\n\n\n\nOxygen-ICU\nGirardis\n2016\n10.1001/jama.2016.11993\n460\n432\n216\n74.0\n142.0\n216\n52.000\n164.000\neither\nin-hosp\nAll\n\n\nCLOSE\nPanwar\n2016\n10.1164/rccm.201505-1019OC\n104\n103\n51\n19.0\n32.0\n52\n21.000\n31.000\nspo2\n90d\nAll\n\n\nHYPER2S\nAsfar\n2017\n10.1016/S2213-2600(17)30046-2\n442\n434\n217\n104.0\n113.0\n217\n90.000\n127.000\nsao2 vs fio2\n90d\nSeptic Shock\n\n\nLang2018\nLang\n2018\n10.1111/aas.13093\n65\n65\n38\n9.0\n29.0\n27\n8.000\n19.000\nfio2\n6m\nTBI\n\n\nCOMACARE\nJakkula\n2018\n10.1007/s00134-018-5453-9\n123\n120\n59\n20.0\n39.0\n61\n18.000\n43.000\npao2\n30d\nOHCA\n\n\nICU-ROX\nMackle\n2020\n10.1056/NEJMoa1903297\n1000\n965\n484\n157.3\n326.7\n481\n166.907\n314.093\nspo2\n90d\nunknown\n\n\n\n\n\n\nNow, let’s meta-analyze it:\n\nlibrary(meta)\n\nLoading required package: metadat\n\n\nLoading 'meta' package (version 8.1-0).\nType 'help(meta)' for a brief overview.\n\n#metabin takes events, total (rather than events, nonevents)\nm_ex1 &lt;- meta::metabin(low_o2_died, num_low_o2, high_o2_died, num_high_o2, data = data_sheet, studlab = paste(name, author, year), sm = \"OR\")\nmeta::forest(m_ex1, comb.random = FALSE, lab.c = \"High Oxygen\", lab.e = \"Low Oxygen\", label.left = \"Favors Low O2\", label.right = \"Favors High O2\")\n\nWarning: Use argument 'label.e' instead of 'lab.e' (deprecated).\n\n\nWarning: Use argument 'label.c' instead of 'lab.c' (deprecated).\n\n\n\n\n\n\n\n\nAnd if you want to get really cutting edge, you can do a trial sequential analysis (TSA) on it:\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.2     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: Unknown or uninitialised column: `order`.\nUnknown or uninitialised column: `order`.\n\n\nWarning in RTSA(type = \"analysis\", data = rtsa_df, outcome = \"RR\", mc = 0.9, :\nNB. The required information size adjusted by Diversity (D^2). This might cause\nan under-powered analysis. Consider changing the argument `random_adj` from\n`D2` (default) to `tau2`.\n\n\n$study\n[1] \"character\"\n\n$author\n[1] \"character\"\n\n$year\n[1] \"numeric\"\n\n$doi\n[1] \"character\"\n\n$num_randomized\n[1] \"numeric\"\n\n$num_patients\n[1] \"numeric\"\n\n$nI\n[1] \"numeric\"\n\n$eI\n[1] \"numeric\"\n\n$high_o2_alive\n[1] \"numeric\"\n\n$nC\n[1] \"numeric\"\n\n$eC\n[1] \"numeric\"\n\n$low_o2_alive\n[1] \"numeric\"\n\n$target\n[1] \"character\"\n\n$outcome\n[1] \"character\"\n\n$population\n[1] \"character\"\n\n$study\ninteger(0)\n\n$author\ninteger(0)\n\n$year\ninteger(0)\n\n$doi\ninteger(0)\n\n$num_randomized\ninteger(0)\n\n$num_patients\ninteger(0)\n\n$nI\ninteger(0)\n\n$eI\ninteger(0)\n\n$high_o2_alive\ninteger(0)\n\n$nC\ninteger(0)\n\n$eC\ninteger(0)\n\n$low_o2_alive\ninteger(0)\n\n$target\ninteger(0)\n\n$outcome\ninteger(0)\n\n$population\ninteger(0)\n\n\n\nplot(an_rtsa)\n\nWarning in geom_segment(aes(x = 0, xend = max(sma_timing, na.rm = T), y = y_val1, : All aesthetics have length 1, but the data has 13 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, xend = 0, y = y_val1, yend = y_val2)): All aesthetics have length 1, but the data has 13 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\nMeaning, we’ve passed futility (at 90% power) for a 10% relative risk reduction a few trials ago. Cool.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#sec-utah_resources",
    "href": "stat_software.html#sec-utah_resources",
    "title": "2  Statistical Software",
    "section": "\n2.8 Local (and other) resources",
    "text": "2.8 Local (and other) resources\n\nOne Data Science Hub Workshops: https://utah-data-science-hub.github.io/education_archived.html  \nRequest CTSI help: https://ctsi.utah.edu/cores-and-services/triad",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#todo-list",
    "href": "stat_software.html#todo-list",
    "title": "2  Statistical Software",
    "section": "TODO List:",
    "text": "TODO List:\n\n# find citation on how frequently errors occur in scientific programming. \n# find citation for data cleaning influence on stability of findings",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#an-example-meta-analyzing-the-effect-of-corticosteroids-in-cap",
    "href": "stat_software.html#an-example-meta-analyzing-the-effect-of-corticosteroids-in-cap",
    "title": "2  Statistical Software",
    "section": "\n2.6 An Example: Meta-analyzing the effect of corticosteroids in CAP",
    "text": "2.6 An Example: Meta-analyzing the effect of corticosteroids in CAP\nLet’s use meta-analysis as a quick example of how to put this all into action.\nA meta-analysis typically accompanies a systematic review to comprehensively capture relevant studies on a question. Systematic reviews are complex, so we’ll skip the details here. Instead, we’ll perform a meta-analysis using a prepared spreadsheet of all known steroids-for-CAP studies (let me know if any studies are missing).\nDownload the data here: Steroid CAP Trials Spreadsheet\n\n# ---- display the table ------------------------------------------------------\nkable(steroids_pna, caption = \"Steroids PNa meta-analysis data\")\n\n\nSteroids PNa meta-analysis data\n\n\n\n\n\n\n\n\n\n\nstudy\nyear\nchest_ma\nint_death\nint_alive\npla_death\npla_alive\n\n\n\nWagner\n1956\n0\n1\n51\n1\n60\n\n\nMcHardy\n1972\n0\n3\n37\n9\n77\n\n\nMarik\n1993\n0\n1\n13\n3\n13\n\n\nConfalonieri\n2005\n0\n0\n23\n7\n16\n\n\nEl Ghamrawy\n2006\n0\n3\n14\n6\n11\n\n\nMikami\n2007\n0\n1\n15\n0\n15\n\n\nSnijders\n2010\n0\n6\n98\n6\n103\n\n\nMelvis\n2011\n0\n17\n134\n19\n134\n\n\nFerrandez-Serrano\n2011\n0\n1\n27\n1\n27\n\n\nSabry\n2011\n0\n2\n38\n6\n34\n\n\nNafae\n2013\n0\n4\n56\n6\n14\n\n\nBlum\n2015\n0\n16\n386\n13\n387\n\n\nTorres\n2015\n0\n8\n53\n11\n48\n\n\nLloyd\n2019\n0\n69\n338\n63\n362\n\n\nWittermans\n2021\n0\n3\n206\n7\n196\n\n\nESCAPe Study\n2023\n0\n47\n239\n50\n227\n\n\nDequin\n2023\n0\n25\n375\n47\n348\n\n\nREMAP-CAP\n2025\n1\n78\n443\n12\n110\n\n\n\n\n\nNow, we’ll cover how you’d meta-analyze these studies in each language as an exercise:\n\n\nR\nPython\nStata\n\n\n\nIn R, here’s the steps needed to perform the meta-analysis:\nFirst, set your working directory to wherever you downloaded the file. Then, we’ll need to make sure we have the right package to read in the spreadsheet\nWe’ll need the ‘readxl’ package to read in the *.xls file https://cran.r-project.org/web/packages/readxl/index.html\nThen, we write code to import the spreadsheet with study data\n\nlibrary(readxl)                           # install.packages(\"readxl\") if needed\n\n## --- OPTION A – read from a local copy ------------------------------------\n# setwd(\"~/your_dir\")                       # uncomment and edit this line\n# dat &lt;- read_xls(\"Steroids PNa MA.xls\", sheet = \"Sheet1\") |&gt;\n#        janitor::clean_names()\n\n## --- OPTION B – read directly from the GitHub raw file             ----\nurl  &lt;- \"https://raw.githubusercontent.com/reblocke/statistics_sandbox/\" |&gt;\n        paste0(\"1b87fb4e65da11a7f9541ea9c7f93b7e3947a13a/\",\n                \"Steroids%20PNa/Steroids%20PNa%20MA.xls\")\n\ntmp  &lt;- tempfile(fileext = \".xls\")          # download to a temp file\ndownload.file(url, tmp, mode = \"wb\")\n\ndat  &lt;- read_xls(tmp, sheet = \"Sheet1\") |&gt;\n        janitor::clean_names()                       # lower-case, snake_case\n\n# Inspect the key columns\ndat |&gt;\n  dplyr::select(study, year,\n                int_death, int_alive,\n                pla_death, pla_alive) |&gt;\n  print()\n\n# A tibble: 18 × 6\n   study              year int_death int_alive pla_death pla_alive\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Wagner             1956         1        51         1        60\n 2 McHardy            1972         3        37         9        77\n 3 Marik              1993         1        13         3        13\n 4 Confalonieri       2005         0        23         7        16\n 5 El Ghamrawy        2006         3        14         6        11\n 6 Mikami             2007         1        15         0        15\n 7 Snijders           2010         6        98         6       103\n 8 Melvis             2011        17       134        19       134\n 9 Ferrandez-Serrano  2011         1        27         1        27\n10 Sabry              2011         2        38         6        34\n11 Nafae              2013         4        56         6        14\n12 Blum               2015        16       386        13       387\n13 Torres             2015         8        53        11        48\n14 Lloyd              2019        69       338        63       362\n15 Wittermans         2021         3       206         7       196\n16 ESCAPe Study       2023        47       239        50       227\n17 Dequin             2023        25       375        47       348\n18 REMAP-CAP          2025        78       443        12       110\n\n\nNext, we’ll need to get a package to do the meta-analysis. Here’s a list of relevant packages: https://cran.r-project.org/web/views/MetaAnalysis.html\nThe ‘meta’ package looks good. Try using `install.packages(‘meta’)` to install it, then you can you can access the documentation using `?meta`\nNow, we’ll perform a random-effects meta-analysis (Using DL variance stimator)\n\nlibrary(meta)                             # install.packages(\"meta\")\n\nLoading required package: metadat\n\n\nLoading 'meta' package (version 8.1-0).\nType 'help(meta)' for a brief overview.\n\nm &lt;- metabin(\n  event.e     = int_death,\n  n.e         = int_death + int_alive,\n  event.c     = pla_death,\n  n.c         = pla_death + pla_alive,\n  studlab     = paste(study, year),\n  data        = dat,\n  sm          = \"OR\",      # odds ratio\n  method.tau  = \"DL\",      # &lt;-- DerSimonian–Laird estimator\n  method.random.ci        = FALSE\n)\n\nLastly, we’ll need to plot the result as a Forest Plot on a log scale (always log scale with ratio variables)\n\nforest(\n  m,\n  comb.fixed  = FALSE,\n  comb.random = TRUE,\n  print.tau2  = FALSE,\n  backtransf  = TRUE,\n  xlab        = \"Odds Ratio (death)\",\n  leftlabs    = c(\"Study (year)\", \"Steroid\", \"Placebo\"),\n  xlog        = TRUE,\n  at          = c(1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32),\n  label.e     = \"Steroid\",\n  label.c     = \"Placebo\",\n  col.diamond = \"navy\",\n  overall.lty = 2,\n  ## keep the default right-hand columns\n  # rightcols = FALSE,          # &lt;-- delete this line\n  main        = \"Steroids OR for Death, Random-Effects Meta-analysis\"\n)\n\n\n\n\n\n\n\n\n\nIn Python, here’s the steps needed to perform the meta-analysis:\nFirst, set your working directory to wherever you downloaded the file. Then, we’ll need to make sure we have the right package to read in the spreadsheet\nThen, we write code to import the spreadsheet with study data using pandas\n\nimport pandas as pd\n\n# --- OPTION A: read directly from disk -------------------------------\n# df = pd.read_excel(\"Steroids PNa MA.xls\", sheet_name=\"Sheet1\")\n\n# --- OPTION B:  For this demo we fetch the raw file from GitHub (same content as R block)\nurl = (\"https://raw.githubusercontent.com/reblocke/statistics_sandbox/\"\n       \"1b87fb4e65da11a7f9541ea9c7f93b7e3947a13a/\"\n       \"Steroids%20PNa/Steroids%20PNa%20MA.xls\")\ndf = pd.read_excel(url)           # GitHub serves raw file\n\n# clean column names to snake_case like janitor::clean_names()\ndf.columns = (df.columns\n                .str.strip()\n                .str.lower()\n                .str.replace(\" \", \"_\"))\n\n# quick sanity check\ndf.head(18)\n\n                study  year  chest_ma  ...  int_alive  pla_death  pla_alive\n0              Wagner  1956         0  ...         51          1         60\n1             McHardy  1972         0  ...         37          9         77\n2               Marik  1993         0  ...         13          3         13\n3        Confalonieri  2005         0  ...         23          7         16\n4         El Ghamrawy  2006         0  ...         14          6         11\n5              Mikami  2007         0  ...         15          0         15\n6            Snijders  2010         0  ...         98          6        103\n7              Melvis  2011         0  ...        134         19        134\n8   Ferrandez-Serrano  2011         0  ...         27          1         27\n9               Sabry  2011         0  ...         38          6         34\n10              Nafae  2013         0  ...         56          6         14\n11               Blum  2015         0  ...        386         13        387\n12             Torres  2015         0  ...         53         11         48\n13              Lloyd  2019         0  ...        338         63        362\n14         Wittermans  2021         0  ...        206          7        196\n15       ESCAPe Study  2023         0  ...        239         50        227\n16             Dequin  2023         0  ...        375         47        348\n17          REMAP-CAP  2025         1  ...        443         12        110\n\n[18 rows x 7 columns]\n\n\nNext, we’ll need to get a package to do the meta-analysis. We’ll use the statsmodels (statsmodels.stats.meta_analysis) package https://www.statsmodels.org/dev/examples/notebooks/generated/metaanalysis1.html\nNow, we’ll perform a random-effects meta-analysis (Using DL variance estimator - which takes some manual preparation work).\n\nimport numpy as np\nfrom statsmodels.stats.meta_analysis import (\n    effectsize_2proportions,    # log-odds-ratio + variance   \n    combine_effects             # pooling with DL            \n)\n\n# --- 2. per-study log(OR) & variance with 0.5 correction --------------------\na = df[\"int_death\"].to_numpy(dtype=float)\nb = df[\"int_alive\"].to_numpy(dtype=float)\nc = df[\"pla_death\"].to_numpy(dtype=float)\nd = df[\"pla_alive\"].to_numpy(dtype=float)\n\n# continuity correction if any cell is zero in that study\ncc = ((a == 0) | (b == 0) | (c == 0) | (d == 0)).astype(float) * 0.5\na += cc; b += cc; c += cc; d += cc\n\nlog_or = np.log((a * d) / (b * c))\nvar_or = 1 / a + 1 / b + 1 / c + 1 / d          # variance of log(OR)\n\n# --- 3. DerSimonian-Laird random-effects pooling ---------------------------\nres = combine_effects(log_or, var_or, method_re=\"dl\")   # DL estimator\n\nprint(\"Current statsmodels version:\", importlib.metadata.version(\"statsmodels\"))\n\nsf = res.summary_frame()          # still log-OR\nsf[\"eff\"]    = np.exp(sf[\"eff\"])\nsf[\"ci_low\"] = np.exp(sf[\"ci_low\"])\nsf[\"ci_upp\"] = np.exp(sf[\"ci_upp\"])\nprint(sf.round(3))\n\nCurrent statsmodels version: 0.14.4\n                     eff  sd_eff  ci_low  ci_upp   w_fe   w_re\n0                  1.176   1.427   0.072  19.285  0.005  0.010\n1                  0.694   0.696   0.177   2.714  0.020  0.036\n2                  0.333   1.219   0.031   3.638  0.006  0.013\n3                  0.047   1.495   0.002   0.878  0.004  0.009\n4                  0.393   0.814   0.080   1.936  0.014  0.028\n5                  3.000   1.672   0.113  79.499  0.003  0.007\n6                  1.051   0.594   0.328   3.369  0.027  0.046\n7                  0.895   0.356   0.446   1.796  0.075  0.092\n8                  1.000   1.440   0.059  16.822  0.005  0.010\n9                  0.298   0.850   0.056   1.578  0.013  0.026\n10                 0.167   0.711   0.041   0.672  0.019  0.035\n11                 1.234   0.380   0.586   2.600  0.066  0.085\n12                 0.659   0.506   0.245   1.774  0.037  0.059\n13                 1.173   0.190   0.808   1.702  0.264  0.150\n14                 0.408   0.697   0.104   1.599  0.020  0.036\n15                 0.893   0.223   0.576   1.383  0.191  0.137\n16                 0.494   0.258   0.297   0.819  0.142  0.123\n17                 1.614   0.328   0.849   3.069  0.089  0.100\nfixed effect       0.861   0.098   0.711   1.042  1.000    NaN\nrandom effect      0.785   0.145   0.590   1.044    NaN  1.000\nfixed effect wls   0.861   0.121   0.678   1.092  1.000    NaN\nrandom effect wls  0.785   0.146   0.590   1.045    NaN  1.000\n\n\nLastly, we’ll need to plot the result as a Forest Plot on a log scale (always log scale with ratio variables)\n\nimport matplotlib.pyplot as plt\n\n# draw the default forest plot (no extra kwargs)\nfig = res.plot_forest(use_exp=True)\n\n# post-process the axis with Matplotlib\nax = fig.axes[0]          # the single Axes returned by plot_forest\nax.set_xscale(\"log\")\nax.set_xlim(0.03125, 32)\nax.set_xlabel(\"Odds Ratio (death)\")\nax.set_xticks([1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32])\nax.set_xticklabels(\n    [\"1/32\", \"1/16\", \"1/8\", \"1/4\", \"1/2\",\n     \"1\", \"2\", \"4\", \"8\", \"16\", \"32\"]\n)\n\nplt.title(\"Steroids OR for Death, Random-Effects Meta-analysis (DL)\")\nplt.tight_layout()\nplt.show()\n\n(0.03125, 32)\n\n\n\n\n\n\n\n\n\n\nAs before, Stata does not have a native notebook-style way to execute code within this webpage, so you’ll have to run the below script on your own machine to see the output.\nHere’s the steps:\n\n*-----------------------------------------------------*\n* 1. Set your_dir to wherever you downloaded the doc  *\n*-----------------------------------------------------*\ncd your_dir\n\n*-----------------------------------------------------*\n* 2. Import the Spreadsheet with Study Data           *\n*-----------------------------------------------------*\nclear\nimport excel \"Steroids PNa MA.xls\", sheet(\"Sheet1\") firstrow case(lower)\nlist study year int_death int_alive pla_death pla_alive\nlabel variable study \"Study\"\n\n*-----------------------------------------------------*\n* 3. Meta-analyze the studies using REML              *\n*-----------------------------------------------------*\nmeta esize int_death int_alive pla_death pla_alive, random(reml) esize(lnor) studylabel(study year) \n\n*-----------------------------------------------------*\n* 4. Create a forest plot with the result             *\n*-----------------------------------------------------*\nmeta forestplot, eform  nullrefline(lcolor(gs3)) esrefline(lcolor(gs3) lpattern(dash_dot)) title(\"Steroids OR for Death, Random-Effects Meta-analysis\")  xsize(11) ysize(7) xscale(log range(0.01 64)) xlabel(0.03125 \"1/32\" 0.0625 \"1/16\" 0.125 \"1/8\" 0.25 \"1/4\" 0.5 \"1/2\" 1 \"1\" 2 \"2\" 4 \"4\" 8 \"8\" 16 \"16\" 32 \"32\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "data.html#getting-data",
    "href": "data.html#getting-data",
    "title": "3  Data",
    "section": "",
    "text": "Each row is an observation (usually a patient)\nEach column contains only 1 type of data (more below)\nNo free text (if you need to, categorize responses)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-get_data",
    "href": "data.html#sec-get_data",
    "title": "3  Data",
    "section": "",
    "text": "Database\nFeatures\nLink\n\n\n\nMIMIC III\nEHR, notes, high-frequency physiology; ICU\nhttps://physionet.org/content/mimiciii/1.4/\n\n\nMIMIC IV\nEHR, notes, high-frequency physiology, electrocardiograms, radiologic images, EEG, echocardiograms; Emergency department, hospital, ICU\nhttps://physionet.org/content/mimiciv/2.2/\n\n\neICU\nEHR; ICU\nhttps://physionet.org/content/eicu-crd/2.0/\n\n\nAmsterdamUMCdb\nEHR; ICU\nhttps://amsterdammedicaldatascience.nl/amsterdamumcdb/\n\n\nHiRID\nEHR, high-frequency physiology; ICU; COVID-19 focused\nhttps://physionet.org/content/hirid/1.1.1/\n\n\nSICdb\nEHR; high-frequency physiology; ICU\nhttps://physionet.org/content/sicdb/1.06/\n\n\nZhejiang\nEHR; ICU\nhttps://physionet.org/content/zhejiang-ehr-critical-care/1.0/\n\n\nPediatric Intensive Care\nEHR; ICU\nhttps://physionet.org/content/picdb/1.1.0/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nServices prioritized if they involve a grant or a grant application (4h, or requires seed function). Also can be prioritized as short queue (4-5 hours or less).\n  In the future, there will be a merit reward to prioritize your project.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-formatting",
    "href": "data.html#sec-formatting",
    "title": "3  Data",
    "section": "\n3.2 Formatting",
    "text": "3.2 Formatting\nStep 0: Save yourself a headache and collect your data in a processable format https://open.substack.com/pub/statsepi/p/simple-tips-for-recording-data-in \nStep 1: Data Wrangling\n\nEach row is an observation (usually a patient)\nEach column contains only 1 type of data (more below)\nNo free text (if you need to, categorize responses)\n\nClean tabular format etc.\nUse excel like a boss, if you’re going to: More excel data https://cghlewis.com/blog/excel_entry/\nFlat files: Flat files: https://evidence.dev/blog/what-is-a-flat-file?utm_campaign=Data_Elixir&utm_source=Data_Elixir_526",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data_types",
    "href": "data.html#sec-data_types",
    "title": "3  Data",
    "section": "\n3.3 Primer on Data types",
    "text": "3.3 Primer on Data types\nStep 2: For each data element, consider the data type\n\nBinary (aka dichotomous scale): e.g. Yes or No, 0 or 1\nUnordered Categorical (nominal scale): e.g. Utah, Colorado, Nevada, Idaho\nOrdered Categorical (ordinal scale): e.g. Room air, nasal cannula, HFNC, intubated, ECMO, dead\nContinuous (interval & ratio scales - differ by whether 0 is special): e.g. Temperature (Celsius or Kelvin, respectively)\n\n\n\n\n\n\n\n\n\n\n\n\ndichotomous\nnominal\nordinal\ninterval\n\n\na.ka.\nbinary\ncategorical\nordered categorical\ncontinuous\n\n\nn\nX\nX\nX\nX\n\n\n%\nX\nX\nX\nX\n\n\nmin\n\n\nX\nX\n\n\nmax\n\n\nX\nX\n\n\nrange\n\n\nX\nX\n\n\nmode\nX\nX\nX\nX\n\n\nmean\n\n\n\nX\n\n\nmedian\n\n\nX\nX\n\n\nIQR\n\n\nX\nX\n\n\nStd. dev.\n\n\n\nX\n\n\nStd. err.\n\n\n\nX\n\n\n\nFrom: Stoddard GJ. Biostatistics and Epidemiology Using Stata: A Course Manual. Salt Lake City, UT: University of Utah School of Medicine.\nTODO: not sure this stuff should live here vs elsewhere:\nStep 3: Visualize the distribution of each data-point (detect outliers, data entry errors, etc.)\nDarren’s hypothetical code lives in a spreadsheet “darren_proj.xlsx”:\nHere is some code that loads the excel spreadsheet into R (we’ll revisit)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nIt’s already (mostly) clean.\nLet’s summarize it:\n\nsummary(darren_data_sheet)\n\n   patient_id    splenectomy        prox_v_dist           qanadli     \n Min.   : 1.00   Length:20          Length:20          Min.   : 2.00  \n 1st Qu.: 5.75   Class :character   Class :character   1st Qu.: 3.75  \n Median :10.50   Mode  :character   Mode  :character   Median :10.00  \n Mean   :10.50                                         Mean   :10.30  \n 3rd Qu.:15.25                                         3rd Qu.:15.00  \n Max.   :20.00                                         Max.   :25.00  \n   got_cteph?       hosp          \n Min.   :0.00   Length:20         \n 1st Qu.:0.00   Class :character  \n Median :0.00   Mode  :character  \n Mean   :0.25                     \n 3rd Qu.:0.25                     \n Max.   :1.00                     \n\n\nHmmm.. what’s wrong with this?\nR need to be told that the binary variables are binary (and not characters)\n\nlibrary(dplyr)\n\n# Convert 'y'/'n' in the splenectomy column to TRUE/FALSE\ndarren_data_sheet &lt;- darren_data_sheet %&gt;%\n  mutate(splenectomy = ifelse(splenectomy == \"y\", TRUE, FALSE))\n\n# Assuming darren_data_sheet is your dataframe\ndarren_data_sheet &lt;- darren_data_sheet %&gt;%\n  mutate(`got_cteph?` = ifelse(`got_cteph?` == 1, TRUE, FALSE))\n\nLet’s visualize each element:\n\nlibrary(ggplot2)\n\n# First, the binary ones\n\n# Plot for splenectomy\nggplot(darren_data_sheet, aes(x = factor(splenectomy))) +\n  geom_bar() +\n  labs(title = \"Distribution of Splenectomy\", x = \"Splenectomy\", y = \"Count\")\n\n\n\n\n\n\n# Plot for prox_v_dist\nggplot(darren_data_sheet, aes(x = factor(prox_v_dist))) +\n  geom_bar() +\n  labs(title = \"Distribution of Proximal vs. Distal\", x = \"Proximal vs Distal\", y = \"Count\")\n\n\n\n\n\n\n# Plot for got_cteph?\nggplot(darren_data_sheet, aes(x = factor(`got_cteph?`))) +\n  geom_bar() +\n  labs(title = \"Distribution of CTEPH Diagnosis\", x = \"Got CTEPH?\", y = \"Count\")\n\n\n\n\n\n\n\nThe categorical one:\n\n# Bar chart for hosp\nggplot(darren_data_sheet, aes(x = factor(hosp))) +\n  geom_bar(fill = \"coral\", color = \"black\") +\n  labs(title = \"Distribution of Hospital\", x = \"Hospital\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Adjust text angle for better readability if needed\n\n\n\n\n\n\n\nand finally, the continuous one:\n\n# Histogram for qanadli\nggplot(darren_data_sheet, aes(x = qanadli)) +\n  geom_histogram(bins = 30, fill = \"blue\", color = \"black\") +\n  labs(title = \"Histogram of Qanadli Scores\", x = \"Qanadli Score\", y = \"Frequency\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "webexercises.html#example-question-types",
    "href": "webexercises.html#example-question-types",
    "title": "Appendix B — Webexercises",
    "section": "",
    "text": "B.1.1 Fill-In-The-Blanks (fitb())\nCreate fill-in-the-blank questions using fitb(), providing the answer as the first argument.\n\n2 + 2 is \n\n\nYou can also create these questions dynamically, using variables from your R session.\n\nThe square root of 9 is: \n\n\nThe blanks are case-sensitive; if you don’t care about case, use the argument ignore_case = TRUE.\n\nWhat is the letter after D? \n\n\nIf you want to ignore differences in whitespace use, use the argument ignore_ws = TRUE (which is the default) and include spaces in your answer anywhere they could be acceptable.\n\nHow do you load the tidyverse package? \n\n\nYou can set more than one possible correct answer by setting the answers as a vector.\n\nType a vowel: \n\n\nYou can use regular expressions to test answers against more complex rules.\n\nType any 3 letters: \n\n\nB.1.2 Multiple Choice (mcq())\n\n“Never gonna give you up, never gonna: \nlet you go\nturn you down\nrun away\nlet you down”\n“I \nbless the rains\nguess it rains\nsense the rain down in Africa” -Toto\n\nB.1.3 True or False (torf())\n\nTrue or False? You can permute values in a vector using sample(). \nTRUE\nFALSE\n\n\nB.1.4 Longer MCQs (longmcq())\nWhen your answers are very long, sometimes a drop-down select box gets formatted oddly. You can use longmcq() to deal with this. Since the answers are long, It’s probably best to set up the options inside an R chunk with echo=FALSE.\nWhat is a p-value?\n\nthe probability that the null hypothesis is truethe probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is truethe probability of making an error in your conclusion\n\nWhat is true about a 95% confidence interval of the mean?\n\nthere is a 95% probability that the true mean lies within this rangeif you repeated the process many times, 95% of intervals calculated in this way contain the true mean95% of the data fall within this range",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  }
]