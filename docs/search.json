[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Clinical Research",
    "section": "",
    "text": "Overview\nThis workbook provides a practical, step-by-step guide for medical students, residents, and fellows embarking on a clinical research project.\nThe focus is specifically on clinical epidemiology and health services research, reflecting both my expertise and the suitability of these areas for clinician-researchers (discussed further in What Type of Project Should You Do?).\nWhile numerous excellent resources exist, this guide uniquely emphasizes:\nIdeally, this guide serves as a complement to active mentorship, where a dedicated mentor can address specific questions and contextual challenges. However, as detailed in What Kind of Mentor(s) Do You Need?, mentorship can vary significantly in quality and depth. Some mentors might lack content-specific expertise (methodologists are unfortunately rare and highly sought-after) or sufficient involvement, making self-directed resources particularly valuable for trainees eager to independently advance their research skills.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "instructions.html",
    "href": "instructions.html",
    "title": "1  How to Use this Book",
    "section": "",
    "text": "1.1 Setup",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "instructions.html#sec-setup",
    "href": "instructions.html#sec-setup",
    "title": "1  How to Use this Book",
    "section": "",
    "text": "1.1.1 Install booktem\n# install.packages(\"devtools\")\ndevtools::install_github(\"debruine/booktem\")\n\n1.1.2 Quarto Options\nThe file _quarto.yml contains various options that you can set to change the format and look of your book.\n\n1.1.2.1 Language Options\nThere is some default text for things like the “authors” list and “table of contents” that might need translations. Set the lang key to the 2-letter language code for your language.\nYou can make a custom translation by translating the values in the include/_language.yml file.\nlang: en\n# language: include/_language.yml\n\n1.1.2.2 Project Options\nThe project key defines the inputs and outputs for the book (quarto reference).\n\n\n\n\n\n\nproject key\n\n\n\n\n\nproject:\n  type: book\n  output-dir: docs\n  resources: resources \n\n\n\nThe output-dir key defines the directory where the rendered web files will be saved. This is set to docs in order to be compatible with GitHub Pages, but you can change this if you are working with a different repository that expects the web files to be in a different directory.\nThe resources key specifies a directory that is copied verbatim to the output directory. This is where you should put, for example, data files that you want to make accessible online (sometimes they don’t automatically copy over when linked).\n\n1.1.2.3 Book Options\nThe book key defines options that affect the look and function of the book (quarto reference).\n\n\n\n\n\n\nbook key\n\n\n\n\n\nbook:\n  title: Book\n  subtitle: ~\n  author: ~\n  doi: ~\n  license: CC-BY 4.0\n  description: ~\n  cover-image: images/logos/logo.png\n  image: images/logos/logo.png\n  favicon: images/logos/logo.png\n  cookie-consent: false\n  google-analytics: ~\n  page-navigation: true\n  search: true\n  # comments:\n  #   hypothesis:\n  #     theme: clean\n  #     openSidebar: false\n  downloads: ~\n  sharing: ~\n  sidebar:\n    title: ~\n    logo: ~\n    search: true\n    contents: ~\n    style: floating\n    background: ~\n    foreground: ~\n    border: true\n    alignment: left\n    collapse-level: 3\n    pinned: true\n    header: \"\"\n    footer: \"\"\n  margin-header: ~\n  page-footer:\n    left: ~\n    right: ~\n  chapters:\n  - index.qmd\n  - instructions.qmd\n  appendices:\n  - references.qmd\n\n\n\n\n1.1.2.4 html Options\nThe format key defines options for specific formats, such as html or pdf. We’ll only be using html here (quarto reference).\n\n\n\n\n\n\nformat:html key\n\n\n\n\n\nformat:\n  html:\n    theme:\n      light:\n      - flatly\n      - include/light.scss\n      dark:\n      - darkly\n      - include/dark.scss\n    css:\n    - https://use.fontawesome.com/releases/v5.13.0/css/all.css\n    - include/booktem.css\n    - include/glossary.css\n    - include/style.css\n    df-print: kable\n    code-link: true\n    code-fold: false\n    code-line-numbers: true\n    code-overflow: wrap\n    code-copy: hover\n    highlight-style: a11y\n    mainfont: ~\n    monofont: ~\n    include-after-body: [include/script.js]\n\n\n\n\n1.1.3 Crossrefs\nSection links must start with sec- and look like this: Section 1.1.5.\n## Section Title {#sec-section-title}\n\nInternal links look like this: @sec-section-title\nFigure links must start with fig- and look like this: Figure 1.1.\n\n\n\n\n\n\n\nFigure 1.1: A histogram of a Poisson distribution with lambda = 3\n\n\n\n\nTable links must start with tbl- and look like this: Table 1.1.\n\n\n\nTable 1.1: The authors of this book\n\n\n\n\n\nfirst_name\nlast_name\n\n\n\nLisa\nDeBruine\n\n\nDaniël\nLakens\n\n\n\n\n\n\n\n\n\nSee the quarto documentation for more information.\n\n1.1.4 References\nZotero export - keep updated\n\n1.1.5 Snippets\nSnippets in RStudio provide shortcuts to syntax. For example, in an RMarkdown document, type “r” and shift-tab to expand a code chunk.\nYou can add your own snippets. Under the Tools menu, choose Edit Code Snippets... and paste the following text into the end of the appropriate sections.\n\n1.1.5.1 Markdown\nsnippet gls\n    r glossary(\"${1:term}\")\n    \nsnippet gls2\n    r glossary(\"${1:term}\", \"${2:display}\")\n    \nsnippet h1\n    # ${1:title} {#sec-${2:ref}}\n    \nsnippet h2\n    ## ${1:title} {#sec-${2:ref}}\n    \nsnippet h3\n    ### ${1:title} {#sec-${2:ref}}\n    \nsnippet h4\n    #### ${1:title} {#sec-${2:ref}}\n    \nsnippet h5\n    ##### ${1:title} {#sec-${2:ref}}\n\n1.1.6 Customize\n\n1.1.6.1 Page Footer\nThe default footer includes license YEAR, author, and github and twitter icons, but you can customize this in the _quarto.yml file under page-footer:. See the quarto documentation for more options. See the available icons at https://icons.getbootstrap.com/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "instructions.html#sec-layout",
    "href": "instructions.html#sec-layout",
    "title": "1  How to Use this Book",
    "section": "\n1.2 Layout",
    "text": "1.2 Layout\n\n1.2.1 Conventions\nThis book will use the following conventions:\n\nCode: list(number = 1, letter = \"A\")\n\nFile paths: data/sales.csv\n\nMenu/interface options: Tools &gt; Global Options… &gt; Pane Layout\n\nR Packages: tidyverse\n\nGlossary items: alphaThe threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\n\nCitations: Wickham et al. (2022)\n\nInternal links: Section 1.2.1\n\nExternal links: Mastering Shiny\n\nMac-specific: Cmd-Shift-F10\n\nWindows-specific: Ctl-Shift-F10\n\n\nA list of mac and windows keyboard shortcuts.\n\n1.2.2 Figures\nIt is best practice to set a custom ggplot theme, then each subsequent plot will use that theme. You can put this code in R/my_setup.R after loading ggplot2.\nStart with a built-in theme and then add any tweaks with the theme() function.\n\nlibrary(ggplot2)\n\nmy_theme &lt;- theme_minimal(base_size = 16) + \n            theme(panel.background = element_rect(fill = \"red\", \n                                                  color = \"black\", \n                                                  size = 5),\n                  panel.grid = element_blank())\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\ntheme_set(my_theme)\n\n\nggplot(midwest, aes(popdensity, percollege)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Population Density\", y = \"Percent College Educated\")\n\n\n\n\n\n\nFigure 1.2: Demographic information of midwest counties from 2000 US census\n\n\n\n\n\n1.2.3 Tables\n\nhead(beaver1)\n\n\n\nBeavers\n\nday\ntime\ntemp\nactiv\n\n\n\n346\n840\n36.33\n0\n\n\n346\n850\n36.34\n0\n\n\n346\n900\n36.35\n0\n\n\n346\n910\n36.42\n0\n\n\n346\n920\n36.55\n0\n\n\n346\n930\n36.69\n0\n\n\n\n\n\n\n\n1.2.4 Callout boxes\nSee the quarto reference for more options.]{.aside}\n\n\n\n\n\n\nNote\n\n\n\n.callout-note: Informational asides.\n\n\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\ncolapse = “true”: Expanded!\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n.callout-tip: Tips\n\n\n\n\n\n\n\n\nWarning\n\n\n\n.callout-warning: Notes to warn you about something.\n\n\n\n\n\n\n\n\nCaution\n\n\n\n.callout-caution: Notes about things that could cause serious errors.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n.callout-important: Notes about things that are important.\n\n\n\n1.2.5 Code and Output\n\n# code chunks\npaste(\"Code\", \"Output\", 1, sep = \" \")\n\n[1] \"Code Output 1\"\n\n\n\n\n\nFilename or header\n\n# code chunks with filename\na &lt;- 1\n\n\n\n\n```{r, fig.width = 2, fig.height = 2}\n# code chunks with visible headers\nhist(rnorm(100000))\n```\n\n\n## Markdown Example\n\n* Inline code: `r nrow(iris)`\n* *Italics*\n* **Bold**\n* [Linked text](https://psyteachr.github.io)\n\n1.2.6 Fonts",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "instructions.html#sec-extras",
    "href": "instructions.html#sec-extras",
    "title": "1  How to Use this Book",
    "section": "\n1.3 Extras",
    "text": "1.3 Extras\n\n1.3.1 Glossary\nBooks are set up with lightweight glossary functions from the glossary package.\n\n# code in R/my_setup.R to initialise the glossary on each page\nlibrary(glossary)\nglossary_path(\"include/glossary.yml\")\nglossary_popup(\"click\") # \"click\", \"hover\" or \"none\"\n\nEdit the file glossary.yml with your glossary terms like this:\nalpha: |\n  The threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\np-value: |\n  The probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\nLook up a term from the glossary file with glossary(\"alpha\"): alphaThe threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\nDisplay a different value for the term with glossary(\"alpha\", \"$\\\\alpha$\"): \\(\\alpha\\)The threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\nUse an inline definition instead of the glossary file with glossary(\"beta\", def = \"The second letter of the Greek alphabet\"): betaThe second letter of the Greek alphabet\nJust show the definition with glossary(\"p-value\", show = \"def\"): The probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\nShow the table of terms defined on this page with glossary_table():\n\n\n\n\nterm\ndefinition\n\n\n\nalpha\nThe threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\n\n\nbeta\nThe second letter of the Greek alphabet\n\n\np-value\nThe probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\n\n\n\n\n\n\n1.3.2 FontAwesome\nThe fontAwesome quarto extension allows you to use the free icons with syntax like:\n{{&lt; fa dragon &gt;}}\n{{&lt; fa brands github size=5x title=\"(github logo)\" &gt;}}\nTo install it, just run this code in the Terminal pane of RStudio (not the Console pane).\nquarto install extension quarto-ext/fontawesome\n\n\n\n\nWickham, H., Bryan, J., & Barrett, M. (2022). Usethis: Automate package and project setup. https://CRAN.R-project.org/package=usethis",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "webexercises.html",
    "href": "webexercises.html",
    "title": "Appendix B — Webexercises",
    "section": "",
    "text": "B.1 Example Question types\nThis template shows how instructors can easily create interactive web documents that students can use in self-guided learning. The webexercises package provides a number of functions that you use in inline R code or through code chunk options to create HTML widgets (text boxes, pull down menus, buttons that reveal hidden content). Examples are given below. Render this file to HTML to see how it works.\nNOTE: To use the widgets in the compiled HTML file, you need to have a JavaScript-enabled browser.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "webexercises.html#example-questions",
    "href": "webexercises.html#example-questions",
    "title": "Appendix B — Webexercises",
    "section": "",
    "text": "B.1.1 Fill-In-The-Blanks (fitb())\nCreate fill-in-the-blank questions using fitb(), providing the answer as the first argument.\n\n2 + 2 is \n\n\nYou can also create these questions dynamically, using variables from your R session.\n\nThe square root of 64 is: \n\n\nThe blanks are case-sensitive; if you don’t care about case, use the argument ignore_case = TRUE.\n\nWhat is the letter after D? \n\n\nIf you want to ignore differences in whitespace use, use the argument ignore_ws = TRUE (which is the default) and include spaces in your answer anywhere they could be acceptable.\n\nHow do you load the tidyverse package? \n\n\nYou can set more than one possible correct answer by setting the answers as a vector.\n\nType a vowel: \n\n\nYou can use regular expressions to test answers against more complex rules.\n\nType any 3 letters: \n\n\nB.1.2 Multiple Choice (mcq())\n\n“Never gonna give you up, never gonna: \nlet you go\nturn you down\nrun away\nlet you down”\n“I \nbless the rains\nguess it rains\nsense the rain down in Africa” -Toto\n\nB.1.3 True or False (torf())\n\nTrue or False? You can permute values in a vector using sample(). \nTRUE\nFALSE\n\n\nB.1.4 Longer MCQs (longmcq())\nWhen your answers are very long, sometimes a drop-down select box gets formatted oddly. You can use longmcq() to deal with this. Since the answers are long, It’s probably best to set up the options inside an R chunk with echo=FALSE.\nWhat is a p-value?\n\nthe probability that the null hypothesis is truethe probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is truethe probability of making an error in your conclusion\n\nWhat is true about a 95% confidence interval of the mean?\n\n95% of the data fall within this rangeif you repeated the process many times, 95% of intervals calculated in this way contain the true meanthere is a 95% probability that the true mean lies within this range",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "webexercises.html#checked-sections",
    "href": "webexercises.html#checked-sections",
    "title": "Appendix B — Webexercises",
    "section": "\nB.2 Checked sections",
    "text": "B.2 Checked sections\nCreate sections with the class webex-check to add a button that hides feedback until it is pressed. Add the class webex-box to draw a box around the section (or use your own styles).\n\nI am going to learn a lot: \nTRUE\nFALSE\n\nWhat is a p-value?\n\nthe probability that the null hypothesis is truethe probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is truethe probability of making an error in your conclusion",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "webexercises.html#hidden-solutions-and-hints",
    "href": "webexercises.html#hidden-solutions-and-hints",
    "title": "Appendix B — Webexercises",
    "section": "\nB.3 Hidden solutions and hints",
    "text": "B.3 Hidden solutions and hints\nYou can fence off a solution area that will be hidden behind a button using hide() before the solution and unhide() after, each as inline R code. Pass the text you want to appear on the button to the hide() function.\nIf the solution is a code chunk, instead of using hide() and unhide(), simply set the webex.hide chunk option to TRUE, or set it to the string you wish to display on the button.\nRecreate the scatterplot below, using the built-in cars dataset.\n\n\n\n\n\n\n\n\n\n\nI need a hint\n\nSee the documentation for plot() (?plot)\n\n\n\n\n\nClick here to see the solution\n\nplot(cars$speed, cars$dist)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Wickham, H., Bryan, J., & Barrett, M. (2022). Usethis: Automate\npackage and project setup. https://CRAN.R-project.org/package=usethis",
    "crumbs": [
      "Appendices",
      "References"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Appendix A — Other helpful resources",
    "section": "",
    "text": "A.1 Statiatical Programming:\nHere are some other helpful resources that still need to be organized and triaged.\nResources…. Here are a few references that might be helpful for learning R: University of Utah Resource https://uofudelphi-r-23-08-21.netlify.app/ More in depth resource/book https://r4ds.hadley.nz/ and slightly more advanced: https://rap4mads.eu/03-functional-programming.html style guide for how to name things https://style.tidyverse.org/syntax.html",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Other helpful resources</span>"
    ]
  },
  {
    "objectID": "project_selection.html",
    "href": "project_selection.html",
    "title": "1  How to Select a Project",
    "section": "",
    "text": "1.1 What Type of Project Should You Do?\nThere are a variety of inter-related considerations that influence the choice of a research/academic project: choosing a topic, choosing a mentor, and choosing a particular study question. Each of these will greatly influence the the experience, requirements, and skills obtained.\nThere are types of research projects that a trainee could do, but some are better than others. My opinionated guidance is:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "project_selection.html#sec-setup",
    "href": "project_selection.html#sec-setup",
    "title": "1  How to Select a Project",
    "section": "",
    "text": "1.1.1 Install booktem\n# install.packages(\"devtools\")\ndevtools::install_github(\"debruine/booktem\")\n\n1.1.2 Quarto Options\nThe file _quarto.yml contains various options that you can set to change the format and look of your book.\n\n1.1.2.1 Language Options\nThere is some default text for things like the “authors” list and “table of contents” that might need translations. Set the lang key to the 2-letter language code for your language.\nYou can make a custom translation by translating the values in the include/_language.yml file.\nlang: en\n# language: include/_language.yml\n\n1.1.2.2 Project Options\nThe project key defines the inputs and outputs for the book (quarto reference).\n\n\n\n\n\n\nproject key\n\n\n\n\n\nproject:\n  type: book\n  output-dir: docs\n  resources: resources \n\n\n\nThe output-dir key defines the directory where the rendered web files will be saved. This is set to docs in order to be compatible with GitHub Pages, but you can change this if you are working with a different repository that expects the web files to be in a different directory.\nThe resources key specifies a directory that is copied verbatim to the output directory. This is where you should put, for example, data files that you want to make accessible online (sometimes they don’t automatically copy over when linked).\n\n1.1.2.3 Book Options\nThe book key defines options that affect the look and function of the book (quarto reference).\n\n\n\n\n\n\nbook key\n\n\n\n\n\nbook:\n  title: Book\n  subtitle: ~\n  author: ~\n  doi: ~\n  license: CC-BY 4.0\n  description: ~\n  cover-image: images/logos/logo.png\n  image: images/logos/logo.png\n  favicon: images/logos/logo.png\n  cookie-consent: false\n  google-analytics: ~\n  page-navigation: true\n  search: true\n  # comments:\n  #   hypothesis:\n  #     theme: clean\n  #     openSidebar: false\n  downloads: ~\n  sharing: ~\n  sidebar:\n    title: ~\n    logo: ~\n    search: true\n    contents: ~\n    style: floating\n    background: ~\n    foreground: ~\n    border: true\n    alignment: left\n    collapse-level: 3\n    pinned: true\n    header: \"\"\n    footer: \"\"\n  margin-header: ~\n  page-footer:\n    left: ~\n    right: ~\n  chapters:\n  - index.qmd\n  - instructions.qmd\n  appendices:\n  - references.qmd\n\n\n\n\n1.1.2.4 html Options\nThe format key defines options for specific formats, such as html or pdf. We’ll only be using html here (quarto reference).\n\n\n\n\n\n\nformat:html key\n\n\n\n\n\nformat:\n  html:\n    theme:\n      light:\n      - flatly\n      - include/light.scss\n      dark:\n      - darkly\n      - include/dark.scss\n    css:\n    - https://use.fontawesome.com/releases/v5.13.0/css/all.css\n    - include/booktem.css\n    - include/glossary.css\n    - include/style.css\n    df-print: kable\n    code-link: true\n    code-fold: false\n    code-line-numbers: true\n    code-overflow: wrap\n    code-copy: hover\n    highlight-style: a11y\n    mainfont: ~\n    monofont: ~\n    include-after-body: [include/script.js]\n\n\n\n\n1.1.3 Crossrefs\nSection links must start with sec- and look like this: Section 1.1.5.\n## Section Title {#sec-section-title}\n\nInternal links look like this: @sec-section-title\nFigure links must start with fig- and look like this: Figure 1.1.\n\n\n\n\n\n\n\nFigure 1.1: A histogram of a Poisson distribution with lambda = 3\n\n\n\n\nTable links must start with tbl- and look like this: Table 1.1.\n\n\n\nTable 1.1: The authors of this book\n\n\n\n\n\nfirst_name\nlast_name\n\n\n\nLisa\nDeBruine\n\n\nDaniël\nLakens\n\n\n\n\n\n\n\n\n\nSee the quarto documentation for more information.\n\n1.1.4 References\nZotero export - keep updated\n\n1.1.5 Snippets\nSnippets in RStudio provide shortcuts to syntax. For example, in an RMarkdown document, type “r” and shift-tab to expand a code chunk.\nYou can add your own snippets. Under the Tools menu, choose Edit Code Snippets... and paste the following text into the end of the appropriate sections.\n\n1.1.5.1 Markdown\nsnippet gls\n    r glossary(\"${1:term}\")\n    \nsnippet gls2\n    r glossary(\"${1:term}\", \"${2:display}\")\n    \nsnippet h1\n    # ${1:title} {#sec-${2:ref}}\n    \nsnippet h2\n    ## ${1:title} {#sec-${2:ref}}\n    \nsnippet h3\n    ### ${1:title} {#sec-${2:ref}}\n    \nsnippet h4\n    #### ${1:title} {#sec-${2:ref}}\n    \nsnippet h5\n    ##### ${1:title} {#sec-${2:ref}}\n\n1.1.6 Customize\n\n1.1.6.1 Page Footer\nThe default footer includes license YEAR, author, and github and twitter icons, but you can customize this in the _quarto.yml file under page-footer:. See the quarto documentation for more options. See the available icons at https://icons.getbootstrap.com/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "project_selection.html#sec-layout",
    "href": "project_selection.html#sec-layout",
    "title": "1  How to Select a Project",
    "section": "\n1.4 Layout",
    "text": "1.4 Layout\n\n1.4.1 Conventions\nThis book will use the following conventions:\n\nCode: list(number = 1, letter = \"A\")\n\nFile paths: data/sales.csv\n\nMenu/interface options: Tools &gt; Global Options… &gt; Pane Layout\n\nR Packages: tidyverse\n\nGlossary items: alpha\n\nCitations: Wickham et al. (2022)\n\nInternal links: Section 1.4.1\n\nExternal links: Mastering Shiny\n\nMac-specific: Cmd-Shift-F10\n\nWindows-specific: Ctl-Shift-F10\n\n\nA list of mac and windows keyboard shortcuts.\n\n1.4.2 Figures\nIt is best practice to set a custom ggplot theme, then each subsequent plot will use that theme. You can put this code in R/my_setup.R after loading ggplot2.\nStart with a built-in theme and then add any tweaks with the theme() function.\n\nlibrary(ggplot2)\n\nmy_theme &lt;- theme_minimal(base_size = 16) + \n            theme(panel.background = element_rect(fill = \"red\", \n                                                  color = \"black\", \n                                                  size = 5),\n                  panel.grid = element_blank())\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\ntheme_set(my_theme)\n\n\nggplot(midwest, aes(popdensity, percollege)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Population Density\", y = \"Percent College Educated\")\n\n\n\n\n\n\nFigure 1.2: Demographic information of midwest counties from 2000 US census\n\n\n\n\n\n1.4.3 Tables\n\nhead(beaver1)\n\n\n\nBeavers\n\nday\ntime\ntemp\nactiv\n\n\n\n346\n840\n36.33\n0\n\n\n346\n850\n36.34\n0\n\n\n346\n900\n36.35\n0\n\n\n346\n910\n36.42\n0\n\n\n346\n920\n36.55\n0\n\n\n346\n930\n36.69\n0\n\n\n\n\n\n\n\n1.4.4 Callout boxes\nSee the quarto reference for more options.]{.aside}\n\n\n\n\n\n\nNote\n\n\n\n.callout-note: Informational asides.\n\n\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\ncolapse = “true”: Expanded!\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n.callout-tip: Tips\n\n\n\n\n\n\n\n\nWarning\n\n\n\n.callout-warning: Notes to warn you about something.\n\n\n\n\n\n\n\n\nCaution\n\n\n\n.callout-caution: Notes about things that could cause serious errors.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n.callout-important: Notes about things that are important.\n\n\n\n1.4.5 Code and Output\n\n# code chunks\npaste(\"Code\", \"Output\", 1, sep = \" \")\n\n[1] \"Code Output 1\"\n\n\n\n\n\nFilename or header\n\n# code chunks with filename\na &lt;- 1\n\n\n\n\n```{r, fig.width = 2, fig.height = 2}\n# code chunks with visible headers\nhist(rnorm(100000))\n```\n\n\n## Markdown Example\n\n* Inline code: `r nrow(iris)`\n* *Italics*\n* **Bold**\n* [Linked text](https://psyteachr.github.io)\n\n\n\n\nWickham, H., Bryan, J., & Barrett, M. (2022). Usethis: Automate package and project setup. https://CRAN.R-project.org/package=usethis",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "project_selection.html#sec-extras",
    "href": "project_selection.html#sec-extras",
    "title": "1  How to Select a Project",
    "section": "\n1.5 Extras",
    "text": "1.5 Extras\n\n1.5.1 Glossary\nBooks are set up with lightweight glossary functions from the glossary package.\n\n# code in R/my_setup.R to initialise the glossary on each page\nlibrary(glossary)\nglossary_path(\"include/glossary.yml\")\nglossary_popup(\"click\") # \"click\", \"hover\" or \"none\"\n\nEdit the file glossary.yml with your glossary terms like this:\nalpha: |\n  The threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\np-value: |\n  The probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\nLook up a term from the glossary file with glossary(\"alpha\"): alpha\nDisplay a different value for the term with glossary(\"alpha\", \"$\\\\alpha$\"): \\(\\alpha\\)\nUse an inline definition instead of the glossary file with glossary(\"beta\", def = \"The second letter of the Greek alphabet\"): betaThe second letter of the Greek alphabet\nJust show the definition with glossary(\"p-value\", show = \"def\"):\nShow the table of terms defined on this page with glossary_table():\n\n\n\n\nterm\ndefinition\n\n\n\nalpha\n\n\n\nbeta\nThe second letter of the Greek alphabet\n\n\np-value\n\n\n\n\n\n\n\n1.5.2 FontAwesome\nThe fontAwesome quarto extension allows you to use the free icons with syntax like:\n\n\nTo install it, just run this code in the Terminal pane of RStudio (not the Console pane).\nquarto install extension quarto-ext/fontawesome\n\n\n\n\nWickham, H., Bryan, J., & Barrett, M. (2022). Usethis: Automate package and project setup. https://CRAN.R-project.org/package=usethis",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "project_selection.html#sec-type",
    "href": "project_selection.html#sec-type",
    "title": "1  How to Select a Project",
    "section": "",
    "text": "Clinical Epidemiology or Health Services research (broadly defined as observational research on patterns of disease, medical care, and outcomes). Generally advisable because projects can be sufficiently small, short-timelne (needed for fellows/residents), and clinically relevant… and thus can be more useful in terms of understanding the application of research to clinical care, even if you end up doing a non-research job. Thus, this is the primary focus of this module. However, it is not the only option to consider.\n\n\n\n\n\n\n\nOther types of research you might consider:\n\n\n\n\n\n\nSurvey research - this is the most tractable flavor of ‘prospective’ research for a trainee, as the regulatory requirements and data-accrual rate is amenable to trainee timelines. Often times, this can involved mixed-methods (where quantification is paired with interviews to understand qualitatively what factors influence or lead to observed outcomes)\nMedical Education scholarship - undestandably, many trainees are interested in ways to improve the trainee experience. And there is lots of room for improvement. However, educational scholarship really needs rigorous assessment to generate generalizable knowledge. Thus, good educational research ends up overlapping substantially with the concepts discussed here.trainees who want to be medical educators should learn research skills because curricular evaluation is an increasingly important component of education (and still should be more important than it is)\nBasic science - I do not cover this much here because I’m not qualified to. My impression is that it’s harder to have a successful basic science project as a trainee, but some motivated folks do manage. If you’re basic science committed or curious, fellowship is the time to try. (nothing against basic science per-se.. some of this may pertain, but I have no first hand experience on how to do that well.\nSystematic Review and Meta-analysis - these are a suprisingly large amount of work, and require a team to complete. If pursuing, I would recommend to do as part of a structured experience (e.g. a course) or with a group who has experience with each of the roles (ie. a librarian, duplicate data assessors/extractors, etc.). The main risk of this sort of project is that it’s easy to get scooped, there are many groups competing to meta-analyze new data when it comes, and it’s hard to think of good research gaps that are answerable with available studies but haven’t been aswered (this is, at it’s core, the crux of doing a SRMA)\nOther reviews - ideally, if doing this you’d want to know you have a mentor with an “in” for getting it published - whether that’s an invitation or gravitas in the relevant field.\nSecondary (retrospective) analyses of existing data-sets (e.g. from prospective cohorts or previously completed trials)\nRandomized trials are the gold standard workhorse of medical evidence, but are generally not feasible for most trainees to substantively contribute to, owing to the time required, regulatory burden, and resources required. Challenges associated with prospective research: likely relevant to this: https://www.atsjournals.org/doi/full/10.34197/ats-scholar.2022-0130PS",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Introduction to Clinical Research",
    "section": "About",
    "text": "About\nI am an Assistant Professor of Research in the Department of Critical Care at Intermountain Medical Center. I am core faculty in the University of Utah Pulmonary and Critical Care Medicine Fellowship Program and Internal Medicine Residency. Academically, I focus on hypercapnic respiratory failure epidemiology, clinical reasoning, and research informatics. More information and current projects at reblocke.github.io\n\n# TODO:  \n# Shorten a bit for 1h session (I think not)\n# Change structure to intentionally start people off installing, then progress\n# Trouble shoot regression and chi2 equivalence... may be easier with linear regression.\n# Create an example of using chatgpt to troubleshoot an error",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "project_selection.html#sec-question",
    "href": "project_selection.html#sec-question",
    "title": "1  How to Select a Project",
    "section": "\n1.3 What makes an interesting (good) research question?",
    "text": "1.3 What makes an interesting (good) research question?\nSay you’ve decided you’re going to do clinical research, and you’re going to work with a mentor to identify a suitable project that gets you excited and your mentor(s) can support.\nIt’s worth considering a few things before delving into an investigation on a particular sutdy question:\nFirst, you must have a way to get the needed data in order to answer the question. Collecting\nConsider your research edge: do you have access to data that know one else can get? if so, you can do research that no one else can.\nDo you have an idea that no-one else has had? (believe it or not, there is a large role for clearer thinking on topics.. so the fact that no-one has done it yet doesn’t necessarily mean that no one could do it)\nDo you have a new way of looking at the same data as others? e.g. either new methodologies or hypotheses that can be re-evaluated in light of subsequent evidence. Consider\n\n1.3.1 What type of Question?\nMost research is inferential: attempting to support inferences about causes and effects. Even most research that claims to be about associations is actually about cause and effect . In some ways this makes sense - we want to understand cause and effect so that we can intervene. However, when working with observational data (ie. not running an experiment - such as a trial), it’s very hard to meet all the assumptions required to identify a causal effect (see (sec?)-***)\nDescriptive epidemiology: [ ] find the paper\nHowever, an alternative objective is to just describe something, i.e. descriptive epidemiology. How often does something happen? What is the ultimate outcome for patients who face a particular situation? What is the base rate of occurence of a diagnosis? The assumptions required to support such a question are often much more believable than establishing cause and effect.\nA researcher will often face a dilemma - do we attempt to use the available data to support a potentially dubious argument about cause and effect? Or robustly describe something? As a matter of personal preference - it seems to me usually better to go with the descriptive (ie. more strongly supported) Question\n\n1.3.2 Exploratory vs Confirmatory Research\nYou can’t propose and confirm a theory based on the same data.\nOne way of framing this issue is categorizing research as either exploratory or confirmatory. In exploratory research, you can look through the data and see what relationships are there. You can look at as many potential relationships as you want and see which ones are interesting. However, when you find one - you have to keep in mind that you’ve had many opportunities to find a relationship, and thus metrics based on controllign the false positive rate (such as p-values) aren’t valid.\nIf, however, you already have a theory and you plan to test it in your data - this is confirmatory research. For reasons we’ll explain in the (intro_stat?) section, you should pre-specify your theory, analysis, and criteria for success before looking at the data. Pre-registration is the recommended way to commit to an analysis plan.\nThis is a high bar that most research doesn’t meet. That’s OK, but you just have to be transparent about it.\n[ ] Exploratory research: https://www.tandfonline.com/doi/full/10.1080/02640414.2025.2486871\n\n1.3.3 ‘Bullshit science’\nThere is a lot of low quality medical science that gets done by trainees. By this, I mean research that cannot answer the question it seeks to investigate, either due to issues with study design, data, or analysis. This is not judgement on the trainees (I’ve been there), but a statement about the end-product - which either makes a compelling argument or it doesn’t.\nMethodologists have long recognized ([ ] Bland Altman) the reason for the excess of low-quality medical research: incentives. There’s much less reward for critiquing research than there is in doing it, and you’re not likely to make friends in close-knit research communities by poking wholes in your colleagues work. For trainees particularly, research is correctly perceived as the only way to have a compelling application to the best training programs (whether one ultimately wants to do research or not). This leads to a commodification of research, where the number of posters, presentations, and manuscripts are tallied in an application, rather than judged on whether there was a novel and rigorous contribution.\nThis is, of course, a waste. Trainees doing bullshit research to check boxes both dilutes scientific norms and we’d be better off if we decoupled research from applications. Thankfully, there’s plenty of real research that should be done. So, while it may take a bit more effort and activation, you can always decline to do bullshit research. If you ever think to yourself “I don’t really care what this research finds”, you should consider why you’re going through the process at all. Life is too short to waste your time doing bullshit science.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "stat_software.html",
    "href": "stat_software.html",
    "title": "2  Statistical Software",
    "section": "",
    "text": "2.1 Statistical software options:\nIn the past, researchers had to manually code all the statistical formulae for their analyses, which was tedious and error-prone. Today, statistical software simplifies this process dramatically. Researchers can shift their focus from the technical complexities of computation to understanding statistical logic and applying analyses correctly.\nThis section offers guidance on selecting appropriate statistical programming language, walks through the set-up process, and introduces the basics of conducting statistical analyses using modern tools.\nR, Python, and Stata are the 3 most commonly used languages.\nThere are a few other language options (SPSS, SAS, Julia, etc.), but they are omitted for brevity (generally, not the best modern options)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#statistical-software-options",
    "href": "stat_software.html#statistical-software-options",
    "title": "2  Statistical Software",
    "section": "",
    "text": "R\nStata\nPython\n\n\nCost\nFree\nRequires License\nFree\n\n\nIDE\nRStudio (Posit)\nBuilt in editor\nMany (Visual Code best)\n\n\nStrengths\nBest epi / trials libraries for helpful functions\nSimple functionality; powerful quasi-experimental/Meta-analysis. U of U MSCI uses.\nBest NLP, machine learning libraries\n\n\nWeakness\nClunky syntax; many ‘dialects’\nSimple syntax\nModerately Complex Syntax\n\n\nExplainable Programming*\nQuarto\nNot native (can use Jupyter)\nJupyter, Quarto\n\n\n\n\n\n2.1.1 How to install\nInstructions for how to do the multiple tabs: https://x.com/rlmcelreath/status/1793641224941007261?s=46&t=5eJ6uoTQrbbYTlHIOnRYRg\nIn quarto just use\n\n\nInstallation Instructions\nFor python\n\n\n\n## For R\n\n## For Stata\n\n\nStep 1:\nInstall R Language\nhttps://cran.r-project.org/\n\n\n\nStep 2:\nInstall RStudio\nhttps://posit.co/downloads/\nRStudio is an IDE (development environment)\n\n\nStep 3:\nInstall Quarto (formerly Markdown)\nhttps://quarto.org/docs/get-started/\nFacilitates sharing and explaining your code. Will soon be standard in medical science.\n\n\nStep 4:\nDownload this document\nhttps://github.com/reblocke/fellow_stats\n“fellow_stats.qmd”\n\n\n\nGet a Key -\nDownload the DMG\nInstall\n\n\na\nb\nc\n\n\n\nhttps://quarto.org/docs/interactive/layout.html\nThis page is made using Quarto.\nPackages\nOther people have mostly done all the analyses you’ll want to do:\n\nCurated lists of relevant packages: https://cran.r-project.org/web/views/\n\n`install.packages( )` will install the packages\n`?package`  or `?command` will bring up the documentation\n\nExample: Say you want to do a meta-analysis.\nRelevant packages: https://cran.r-project.org/web/views/MetaAnalysis.html\nThe ‘meta’ package looks good. Try using `install.packages(‘meta’)` to install it, then you can you can access the documentation using `?meta`\nLet’s try an example… I extracted data on all of the trials comparing high O2 to low O2 targets and uploaded to github.\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nhead(data_sheet)\nauthors &lt;- select(data_sheet, author)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nauthor\nyear\ndoi\nnum_randomized\nnum_patients\nnum_high_o2\nhigh_o2_died\nhigh_o2_alive\nnum_low_o2\nlow_o2_died\nlow_o2_alive\ntarget\noutcome\npopulation\n\n\n\nOxygen-ICU\nGirardis\n2016\n10.1001/jama.2016.11993\n460\n432\n216\n74.0\n142.0\n216\n52.000\n164.000\neither\nin-hosp\nAll\n\n\nCLOSE\nPanwar\n2016\n10.1164/rccm.201505-1019OC\n104\n103\n51\n19.0\n32.0\n52\n21.000\n31.000\nspo2\n90d\nAll\n\n\nHYPER2S\nAsfar\n2017\n10.1016/S2213-2600(17)30046-2\n442\n434\n217\n104.0\n113.0\n217\n90.000\n127.000\nsao2 vs fio2\n90d\nSeptic Shock\n\n\nLang2018\nLang\n2018\n10.1111/aas.13093\n65\n65\n38\n9.0\n29.0\n27\n8.000\n19.000\nfio2\n6m\nTBI\n\n\nCOMACARE\nJakkula\n2018\n10.1007/s00134-018-5453-9\n123\n120\n59\n20.0\n39.0\n61\n18.000\n43.000\npao2\n30d\nOHCA\n\n\nICU-ROX\nMackle\n2020\n10.1056/NEJMoa1903297\n1000\n965\n484\n157.3\n326.7\n481\n166.907\n314.093\nspo2\n90d\nunknown\n\n\n\n\n\n\nNow, let’s meta-analyze it:\n\nlibrary(meta)\n\nLoading required package: metadat\n\n\nLoading 'meta' package (version 8.1-0).\nType 'help(meta)' for a brief overview.\n\n#metabin takes events, total (rather than events, nonevents)\nm_ex1 &lt;- meta::metabin(low_o2_died, num_low_o2, high_o2_died, num_high_o2, data = data_sheet, studlab = paste(name, author, year), sm = \"OR\")\nmeta::forest(m_ex1, comb.random = FALSE, lab.c = \"High Oxygen\", lab.e = \"Low Oxygen\", label.left = \"Favors Low O2\", label.right = \"Favors High O2\")\n\nWarning: Use argument 'label.e' instead of 'lab.e' (deprecated).\n\n\nWarning: Use argument 'label.c' instead of 'lab.c' (deprecated).\n\n\n\n\n\n\n\n\nAnd if you want to get really cutting edge, you can do a trial sequential analysis (TSA) on it:\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.2     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: Unknown or uninitialised column: `order`.\nUnknown or uninitialised column: `order`.\n\n\nWarning in RTSA(type = \"analysis\", data = rtsa_df, outcome = \"RR\", mc = 0.9, :\nNB. The required information size adjusted by Diversity (D^2). This might cause\nan under-powered analysis. Consider changing the argument `random_adj` from\n`D2` (default) to `tau2`.\n\n\n$study\n[1] \"character\"\n\n$author\n[1] \"character\"\n\n$year\n[1] \"numeric\"\n\n$doi\n[1] \"character\"\n\n$num_randomized\n[1] \"numeric\"\n\n$num_patients\n[1] \"numeric\"\n\n$nI\n[1] \"numeric\"\n\n$eI\n[1] \"numeric\"\n\n$high_o2_alive\n[1] \"numeric\"\n\n$nC\n[1] \"numeric\"\n\n$eC\n[1] \"numeric\"\n\n$low_o2_alive\n[1] \"numeric\"\n\n$target\n[1] \"character\"\n\n$outcome\n[1] \"character\"\n\n$population\n[1] \"character\"\n\n$study\ninteger(0)\n\n$author\ninteger(0)\n\n$year\ninteger(0)\n\n$doi\ninteger(0)\n\n$num_randomized\ninteger(0)\n\n$num_patients\ninteger(0)\n\n$nI\ninteger(0)\n\n$eI\ninteger(0)\n\n$high_o2_alive\ninteger(0)\n\n$nC\ninteger(0)\n\n$eC\ninteger(0)\n\n$low_o2_alive\ninteger(0)\n\n$target\ninteger(0)\n\n$outcome\ninteger(0)\n\n$population\ninteger(0)\n\n\n\nplot(an_rtsa)\n\nWarning in geom_segment(aes(x = 0, xend = max(sma_timing, na.rm = T), y = y_val1, : All aesthetics have length 1, but the data has 13 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, xend = 0, y = y_val1, yend = y_val2)): All aesthetics have length 1, but the data has 13 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\nMeaning, we’ve passed futility (at 90% power) for a 10% relative risk reduction a few trials ago. Cool.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#reproducible-research",
    "href": "stat_software.html#reproducible-research",
    "title": "2  Statistical Software",
    "section": "\n2.5 Reproducible Research",
    "text": "2.5 Reproducible Research\nFrom a perspective of scientific rigor, sharing should also include the individual patient data to allow researchers to directly replicate or modify the reported analyses. However, despite research participant support for data sharing, privacy concerns and research ethics concerns generally do not permit the sharing of data, even if it has been pseudonomized. This is an active space where “ideal” and “actual” are far apart… but the current takeaway is that individual patient data should not ben shared unless that was explicitly part of the IRB authorization.\nFurther reading (R focus): https://raps-with-r.dev/intro.html building reproducible analytical pipelines: https://rap4mads.eu/?utm_source=substack&utm_medium=email Citation: reproducible computing: https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745\nprinciples for reproducible and efficient computing; https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#vibe-coding-and-related-concepts.",
    "href": "stat_software.html#vibe-coding-and-related-concepts.",
    "title": "2  Statistical Software",
    "section": "\n2.3 ‘Vibe Coding’ and related concepts.",
    "text": "2.3 ‘Vibe Coding’ and related concepts.\n\nGPT usage.\n\nLarge Language Models: Options:\n\nOpenAI Chat GPT (requires subscription for best performance; custom GPTs)\nGithub CoPilot (programming specific)\nMicrosoft CoPilot - access to GPT4 = free through University of Utah\n\nCopilot:\n\nVisit bing.com/chat.\nSelect “sign in with a work or school account” under the Sign in icon in the upper right corner of the page.\nEnter your unid@umail.utah.edu and uNID password.\nComplete Duo two-factor authentication.\nThe conversation is protected when a green shield appears in the upper right corner next to your username. It is critical to verify that the green shield is present for all conversations.\n\nPrompt Engineering:\n\nhave the GPT take the persona that you want\nspell out the chain of thougt that you want the GPT to take (either multiple steps in 1 prompt or several prompts building on one another works)\nGive examples or specifications of what you want done. [this is particularly useful because the documents you give it can form a context and examples]. \n\nHow I used GPT4 creating this workbook:\n\n\nThe prompt I used to create the above example.\n\nU of U Resources - \n\nOne Data Science Hub Workshops: https://utah-data-science-hub.github.io/education_archived.html  \nRequest CTSI help: https://ctsi.utah.edu/cores-and-services/triad \nIntuitive Biostatistics by Harvey Motulsky - https://a.co/d/4NCk2bS",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "3  Data",
    "section": "",
    "text": "3.1 Getting Data\nYes - this really is that important to deserve its own chapter.\nwhat are some relevant (to PCCM) data-sets.\nDatabases compiled by NLM https://www.datasetcatalog.nlm.nih.gov/index.html\nThe NHLBI Pooled Cohorts Study harmonized spirometry data from nine U.S. population-based cohorts: 9 Prospective US Cohorts from NHLBI  includig spirometry:  (from Am J Epidemiol. 2018;187(11):2265–2278 )\nhttps://academic-oup-com.ezproxy.lib.utah.edu/aje/article/187/11/2265/5047150\nARIC, Atherosclerosis Risk in Communities\nCARDIA, Coronary Artery Risk Development in Young Adults\nCHS, Cardiovascular Health Study;\nFHS-O, Framingham Heart Study—Offspring Cohort\nHABC, Health, Aging and Body Composition\nHCHS/SOL, Hispanic Community Health Study/Study of Latinos\nJHS, Jackson Heart Study\nMESA, Multi-Ethnic Study of Atherosclerosis;\nSHS, Strong Heart Study.\nSpiromics\nCOPDGene\nTriNetX\nPinc AI Healthcare\nSleep: Sleepdata.org\nNCHS: National Center for Health Statistics Datasets:: https://www.cdc.gov/nchs/nhis/nhis_questionnaires.htm\nhttps://nhis.ipums.org/nhis/aboutIPUMSNHIS.shtml   &lt;— documentation for the NCHS datasets, and an integration of several years with weightings.\nReference for sampling designs - https://stats.oarc.ucla.edu/other/mult-pkg/faq/faq-choosing-the-correct-analysis-for-various-survey-designs/  ; https://stats.oarc.ucla.edu/stata/seminars/survey-data-analysis-in-stata-17/\nNHIS—list of variables pertinent to respiratory health: https://nhis.ipums.org/nhis/userNotes_HP2020.shtml#group14\n— can be linked with the National Death Index\nNational Inpatient Sample Data elements: https://hcup-us.ahrq.gov/db/nation/nis/nisdde.jsp\nNHANES\nMIMIC - III/IV\nEICU - https://www.nature.com/articles/sdata2018178\nSicDB - https://link.springer.com/article/10.1007/s00134-023-07046-3 Salzburg “SICdb (1.0.4) contains 27,386 admissions from 4 different intensive care units (ICUs) at 1 single tertiary care institution of the Department of Anesthesiology and Intensive Care Medicine at the Salzburger Landesklinik (SALK) and Paracelsus Medical University (PMU) between 2013 and 2021.” 1-per-minute. https://www.sicdb.com/\n—-&gt; comment on anonymization https://link.springer.com/article/10.1007/s00134-023-07153-1\nUPDB ***\nNHLBI BioData Catalyst - https://academic-oup-com.ezproxy.lib.utah.edu/jamia/article/30/7/1293/7165700?utm_source=etoc&utm_campaign=jamia&utm_medium=email&nbd=41184264570&nbd_source=campaigner - includes TOPmed, COVID data-sets. Idea = a place for researchers to store these resources\nEDW.\nResearch Networks: (get Limited dataset)\n-PCORNET (can access broad network) - need to submit an IRB to them. Dr. Hess is local contact\n-ACT (smaller version of PCORNET)\n-Clinithink\n-TriNetX\n-Epic Cosmos\nData Science Services (since ~2016) - handles query with research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#formatting",
    "href": "data.html#formatting",
    "title": "3  Data",
    "section": "\n3.2 Formatting",
    "text": "3.2 Formatting\nClean tabular format etc.\nUse excel like a boss, if you’re going to: More excel data https://cghlewis.com/blog/excel_entry/\nFlat files: Flat files: https://evidence.dev/blog/what-is-a-flat-file?utm_campaign=Data_Elixir&utm_source=Data_Elixir_526",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#data-types",
    "href": "data.html#data-types",
    "title": "3  Data",
    "section": "\n3.3 Data types",
    "text": "3.3 Data types\nStep 2: For each data element, consider the data type\n\nBinary (aka dichotomous scale): e.g. Yes or No, 0 or 1\nUnordered Categorical (nominal scale): e.g. Utah, Colorado, Nevada, Idaho\nOrdered Categorical (ordinal scale): e.g. Room air, nasal cannula, HFNC, intubated, ECMO, dead\nContinuous (interval & ratio scales - differ by whether 0 is special): e.g. Temperature (Celsius or Kelvin, respectively)\n\n\n\n\n\n\n\n\n\n\n\n\ndichotomous\nnominal\nordinal\ninterval\n\n\na.ka.\nbinary\ncategorical\nordered categorical\ncontinuous\n\n\nn\nX\nX\nX\nX\n\n\n%\nX\nX\nX\nX\n\n\nmin\n\n\nX\nX\n\n\nmax\n\n\nX\nX\n\n\nrange\n\n\nX\nX\n\n\nmode\nX\nX\nX\nX\n\n\nmean\n\n\n\nX\n\n\nmedian\n\n\nX\nX\n\n\nIQR\n\n\nX\nX\n\n\nStd. dev.\n\n\n\nX\n\n\nStd. err.\n\n\n\nX\n\n\n\nFrom: Stoddard GJ. Biostatistics and Epidemiology Using Stata: A Course Manual. Salt Lake City, UT: University of Utah School of Medicine.\nTODO: not sure this stuff should live here vs elsewhere:\nStep 3: Visualize the distribution of each data-point (detect outliers, data entry errors, etc.)\nDarren’s hypothetical code lives in a spreadsheet “darren_proj.xlsx”:\nHere is some code that loads the excel spreadsheet into R (we’ll revisit)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nIt’s already (mostly) clean.\nLet’s summarize it:\n\nsummary(darren_data_sheet)\n\n   patient_id    splenectomy        prox_v_dist           qanadli     \n Min.   : 1.00   Length:20          Length:20          Min.   : 2.00  \n 1st Qu.: 5.75   Class :character   Class :character   1st Qu.: 3.75  \n Median :10.50   Mode  :character   Mode  :character   Median :10.00  \n Mean   :10.50                                         Mean   :10.30  \n 3rd Qu.:15.25                                         3rd Qu.:15.00  \n Max.   :20.00                                         Max.   :25.00  \n   got_cteph?       hosp          \n Min.   :0.00   Length:20         \n 1st Qu.:0.00   Class :character  \n Median :0.00   Mode  :character  \n Mean   :0.25                     \n 3rd Qu.:0.25                     \n Max.   :1.00                     \n\n\nHmmm.. what’s wrong with this?\nR need to be told that the binary variables are binary (and not characters)\n\nlibrary(dplyr)\n\n# Convert 'y'/'n' in the splenectomy column to TRUE/FALSE\ndarren_data_sheet &lt;- darren_data_sheet %&gt;%\n  mutate(splenectomy = ifelse(splenectomy == \"y\", TRUE, FALSE))\n\n# Assuming darren_data_sheet is your dataframe\ndarren_data_sheet &lt;- darren_data_sheet %&gt;%\n  mutate(`got_cteph?` = ifelse(`got_cteph?` == 1, TRUE, FALSE))\n\nLet’s visualize each element:\n\nlibrary(ggplot2)\n\n# First, the binary ones\n\n# Plot for splenectomy\nggplot(darren_data_sheet, aes(x = factor(splenectomy))) +\n  geom_bar() +\n  labs(title = \"Distribution of Splenectomy\", x = \"Splenectomy\", y = \"Count\")\n\n\n\n\n\n\n# Plot for prox_v_dist\nggplot(darren_data_sheet, aes(x = factor(prox_v_dist))) +\n  geom_bar() +\n  labs(title = \"Distribution of Proximal vs. Distal\", x = \"Proximal vs Distal\", y = \"Count\")\n\n\n\n\n\n\n# Plot for got_cteph?\nggplot(darren_data_sheet, aes(x = factor(`got_cteph?`))) +\n  geom_bar() +\n  labs(title = \"Distribution of CTEPH Diagnosis\", x = \"Got CTEPH?\", y = \"Count\")\n\n\n\n\n\n\n\nThe categorical one:\n\n# Bar chart for hosp\nggplot(darren_data_sheet, aes(x = factor(hosp))) +\n  geom_bar(fill = \"coral\", color = \"black\") +\n  labs(title = \"Distribution of Hospital\", x = \"Hospital\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Adjust text angle for better readability if needed\n\n\n\n\n\n\n\nand finally, the continuous one:\n\n# Histogram for qanadli\nggplot(darren_data_sheet, aes(x = qanadli)) +\n  geom_histogram(bins = 30, fill = \"blue\", color = \"black\") +\n  labs(title = \"Histogram of Qanadli Scores\", x = \"Qanadli Score\", y = \"Frequency\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "study_design.html",
    "href": "study_design.html",
    "title": "4  Basics of Study Design",
    "section": "",
    "text": "4.1 Variable Definitions\nIn this section, we’ll cover the basics of study design.\nWe circle back to this after covering the overall logic of scientific argument (the disjunctive syllogism, Chapter 5) and the basics of regression (?sec-intro_regression). The main remaining topics to cover are related to how w ecan set up an analysis to differentiate ‘signal from noise’ - a helpful analogy for understanding the related issues of variation, end-points, and power.\nStructure of scientific argument: disjunctive syllogism\nFor this section, we’ll use somewhat broad stand-ins to represent the types of variables relevant to many types of analyses. I’ll give specific examples from respiratory failure research that might be sensible.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of Study Design</span>"
    ]
  },
  {
    "objectID": "study_design.html#outcome",
    "href": "study_design.html#outcome",
    "title": "4  Basics of Study Design",
    "section": "\n4.2 Outcome",
    "text": "4.2 Outcome\nThe thing you’re interesting in measuring.\nEffect measures:\nRisk ratio\nOdds ratio : https://onlinelibrary.wiley.com/doi/10.1111/test.12391\n\n4.2.1 Considerations in choosing an outcome\nEndpoint considerations: https://hbiostat.org/endpoint/",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of Study Design</span>"
    ]
  },
  {
    "objectID": "study_design.html#confounding",
    "href": "study_design.html#confounding",
    "title": "4  Basics of Study Design",
    "section": "\n4.2 Confounding",
    "text": "4.2 Confounding\nWhats a confounder?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of Study Design</span>"
    ]
  },
  {
    "objectID": "study_design.html#effect-modification-and-interactions",
    "href": "study_design.html#effect-modification-and-interactions",
    "title": "4  Basics of Study Design",
    "section": "\n4.3 Effect Modification and Interactions",
    "text": "4.3 Effect Modification and Interactions",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of Study Design</span>"
    ]
  },
  {
    "objectID": "study_design.html#risk-factor",
    "href": "study_design.html#risk-factor",
    "title": "4  Basics of Study Design",
    "section": "\n4.4 Risk factor",
    "text": "4.4 Risk factor",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of Study Design</span>"
    ]
  },
  {
    "objectID": "study_design.html#counterfactual-reasoning",
    "href": "study_design.html#counterfactual-reasoning",
    "title": "4  Basics of Study Design",
    "section": "\n4.5 Counterfactual reasoning",
    "text": "4.5 Counterfactual reasoning\nAdvanced topics (maybe a little bit)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of Study Design</span>"
    ]
  },
  {
    "objectID": "intro_stats.html",
    "href": "intro_stats.html",
    "title": "5  Statistical Foundations",
    "section": "",
    "text": "5.1 The Disjunctive Syllogism\nThe goal of this section is to build an intuition for what statistical tests accomplish, and what the assumptions are.\nBy the end of this page, you should have an intuitive sense of what a p-value is (and what it is not) and the role that it plays in the argument of a scientific report.\nTo forecast the punchline: We seek to make arguments that observed relationships are not explained by confounding, bias, or chance - and therefore must be causation. P-values summarize how surprising it would be to see the observed data, assuming there is nothing going on and the assumptions of the test hold.\nWe’ll start out abstract, and become more concrete.\nThe root of the problem is that we don’t directly observe cause and effect. Instead, we must make an argument for it.\nThe structure of the usual scientific argument mirrors Sherlock Holmes quote:\nThis “argument-by-excluding-alternatives” is termed the disjunctive syllogism. NOTE: this isn’t the only possible argument - for example, likelihoodism (and by extension, Bayesianism), makes the argument that when comparing a hypothesis to the alternative hypotheses the data would be much less likely. etc.\nIn the language of epidemiology, we’re interested in the relationship between an “exposure” (meant broadly - could refer to a treatment, an occupational exposure, a characteristics, etc.) and an “outcome” (also meant broadly, could be the occurence of an event, a side effect, a health state, etc.).\nIf an exposure and an outcome are associated, there are 4 possible explanations:\nThus, the way we’ll seek evidence for causation is indirect. First, we’ll show there is an association. Then, we’ll make arguments against the possibility of confounding, bias, and chance explaining the association. If the reader accepts there’s an association but that reasons 1-3 are not plausible explanations, they’re left accepting causation as an explanation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#how-to-formulate-your-actual-question-as-a-test",
    "href": "intro_stats.html#how-to-formulate-your-actual-question-as-a-test",
    "title": "5  Statistical Foundations",
    "section": "\n5.3 How to formulate your actual question as a test:",
    "text": "5.3 How to formulate your actual question as a test:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#the-logic-of-a-scientific-argument",
    "href": "intro_stats.html#the-logic-of-a-scientific-argument",
    "title": "5  Statistical Foundations",
    "section": "\n5.4 The logic of a scientific argument:",
    "text": "5.4 The logic of a scientific argument:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#p-values",
    "href": "intro_stats.html#p-values",
    "title": "5  Statistical Foundations",
    "section": "\n5.5 P-values",
    "text": "5.5 P-values\nFrequentist P-values - how suprising would this be if there was nothing going on?\nS-value example… and then twist it so that I don’t say how many times I tried (and I’m incentivized to find one)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#whats-power-got-to-do-with-it",
    "href": "intro_stats.html#whats-power-got-to-do-with-it",
    "title": "5  Statistical Foundations",
    "section": "\n5.7 What’s power got to do with it?",
    "text": "5.7 What’s power got to do with it?\nEverything is underpowered ***\nPower - https://onlinelibrary.wiley.com/doi/full/10.1111/test.12403?campaign=wolearlyview",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_regression.html",
    "href": "intro_regression.html",
    "title": "6  Intro to Regressions",
    "section": "",
    "text": "6.1 What can regression models do for you?\nStatistical modeling can be counter intuitive, so let’s pause and clarify what we want to achieve before jumping into details.\nFirst, we’ll discuss why we use regression: what’s the purpose, and what does it help us understand? Next, we’ll explore the idea of a “model” in regression, emphasizing that—just as with statistical tests—your decisions about representing data influence your results. Finally, we’ll wrap up with some practical advice, though we’ll only scratch the surface of this rich topic.\nRegression models have multiple uses; it’s helpful to clarify your goal upfront. One intuitive way to think about their purpose is by grouping them into three main types: descriptive, predictive, or causal (as described in https://onlinelibrary.wiley.com/doi/full/10.1002/sim.10244)\nConsider a regression model that predicts weight based on age, sex, and height. Depending on your objective, you might ask different questions:\nResearchers typically specify clearly when they’re making predictions, but they often blur the lines between descriptive and causal statements. For example, you’ll frequently see euphemisms like “linked to” or “associated with,” followed by suggestions that changing a predictor could influence the outcome. This confusion between description and causation is a widespread issue(academic.oup.com/aje/article/191/12/2084/6655746). Don’t add to it. If your goal is to understand cause-and-effect relationships, communicate that.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Regressions</span>"
    ]
  },
  {
    "objectID": "project_management.html",
    "href": "project_management.html",
    "title": "7  Getting your project done",
    "section": "",
    "text": "7.1 Key Figures\nIn this section, we’ll discuss how to actually make the research happen: what should your workflow be, what are some hints for how to do things quickly and robustly.\nNicoas Zaorsky Guide: https://twitter.com/nicholaszaorsky/status/1479549305623035904?s=11 and Supplemental info:\nhttps://drive.google.com/file/d/1xSB0-UsAKWbZTvxa3stGMSmEP2QGQ7nO/view\nCreating the outline:\nRough Drafts:\nRough drafts can be very rough and should be very fast - just try to get the ideas out on to paper.  Ideally, letting the draft flow as best as possible, taking a break, then revisiting with editing.\nShort sentences. Punch intro sentences to paragraphs, end paragraphs that collect the important points then transition to the next. One idea per paragraph.\nEditing:\nWriting https://twitter.com/samirshahmd/status/1526917833912946693?s=21&t=_0rCq0fFXkdkj1-069zuNw\nElements of academic style https://twitter.com/juliencayla/status/1574628448680566784?s=12&t=eyTORVO29cWHwMChGFjQeQ\nAcademic Writing guides in the Academic Writing folder.\nGood writing habits – a little bit every day vs blocking off days at a time. Try to overcome inertia.\n–writing routines can also help initiate the writing process. Same chair and desk, and coffee. Set mini deadlines.  Getting it down below the word limit: https://twitter.com/scientistswrite/status/1636757776633307137?s=20\nWriting Style:\nSentence structure / writing mechanics:\n-try to keep subjects and verbs together in sentences.\n-every unit of discourse should serve a function or make a single point\n-the end of a sentence is the ‘stress position’. The information presented at the end will have more impact.\n-the beginning of a sentence is the ‘topic position’ - structure sentences to match what you want them to be about: consider: pollen is dispersed by bees vs bees disperse pollen.  Topic position = what the sentence is about\n-linkage (forward and back) - when new information is introduced, it is better in the stress position; old information that is backward linking is better in the topic position.\n-linkage between sentences is what forms the basis for the logical argument - if you can follow from link to link from the start to conclusion, the argument is made. Gaps often occur when specialized knowledge leads an author to ‘assume’ the reader will know\nAlways remove “fundamentally”, “certainly”, “basically”, “briefly”, “very” - and similar words which can be removed without changing the meaning at all.\nWriting a first draft: https://tapper.substack.com/p/paper-writing-secrets?utm_campaign=post\nSome advocate for Figures/legends -&gt; methods -&gt; results -&gt; discussion -&gt; introduction -&gt; abstract/title\nBMC how to write… series (esp discussion. https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-018-0490-1)\nWorkflow….\nFirst - agree on roles and authorship order -\nAuthorship order, corresponding author matters - https://journals.lww.com/epidem/fulltext/2004/01000/interpreting_authorship_order_and_corresponding.22.aspx\nSecond - IRB authorization - For IRB:\nIf you need PHI, get IRB and justify the need. Must minimize risk to patients (securely receive, store, and analyze - computing infrastructure)\nStart by making your key figures.\nVisualization vocabulary https://www.ft.com/content/c7bb24c9-964d-479f-ba24-03a2b2df6e85\nBetter diagrams advice: “https://vexlio.com/blog/five-simple-things-that-will-immediately-improve-your-diagrams/?utm_source=substack&utm_medium=email” - Fill shapes with 2–3 neutral-grey fills and remove outlines; assign darker greys to higher-priority elements; ensure text-to-background contrast ≥ 4.5 : 1.\n- Limit the palette to at most 2 accent colors in addition to greys; use color only for semantic categories or highlighting, never both.\n- Insert ample whitespace: group elements by proximity rather than bounding boxes, leave small gaps at connector endpoints, and offset labels/titles from their subjects.\n- Standardize shape, gap, and text sizes to a small fixed set and repeat consistently across the diagram.\n- Label every arrow or connector unless its meaning is unmistakable from context.\n- Increase all text sizes by roughly 20 percent beyond initial choice.\nOverall figure checklist https://x.com/scientistswrite/status/1698718678722728212?s=46&t=5eJ6uoTQrbbYTlHIOnRYRg\nbetter options than bar charts (such as dot plot) for continuous data. https://pubmed.ncbi.nlm.nih.gov/25901488/ https://pubmed.ncbi.nlm.nih.gov/31657957/\nSome guidance on which type of figure to choose: https://www.datawrapper.de/blog/chart-types-guide?utm_campaign=Data_Elixir&utm_source=Data_Elixir_539\nRules for figures: https://robjhyndman.com/hyndsight/graphics/\nhttps://twitter.com/asjadnaqvi/status/1513863752201998340?s=12&t=Zbxqxo5GBwdUGsn30Nc-Lw\nhttps://medium.com/the-stata-guide\nR - https://twitter.com/f2harrell/status/1515284543204474881?s=21&t=1ZhOw-Mre83Ylh60daEZiw\nDensity strips: https://twitter.com/bmj_ebm/status/1482023471928516609?s=43&t=5eJ6uoTQrbbYTlHIOnRYRg\nAnd tables:\nTables- https://www.cambridge.org/core/journals/journal-of-benefit-cost-analysis/article/abs/ten-guidelines-for-better-tables/74C6FD9FEB12038A52A95B9FBCA05A12\nStatistical Reporting",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Getting your project done</span>"
    ]
  },
  {
    "objectID": "resources.html#project-management",
    "href": "resources.html#project-management",
    "title": "Appendix A — Other helpful resources",
    "section": "\nA.2 Project Management",
    "text": "A.2 Project Management\n\nA.2.1 Figures\n\nhttps://nrennie.rbind.io/blog/chart-makeover/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Other helpful resources</span>"
    ]
  },
  {
    "objectID": "resources.html#take-your-knowledge-to-the-next-level",
    "href": "resources.html#take-your-knowledge-to-the-next-level",
    "title": "Appendix A — Other helpful resources",
    "section": "\nA.3 Take your knowledge to the next level",
    "text": "A.3 Take your knowledge to the next level\nAdvanced topic list:\nBoostrap Survival Analysis / competing events? Multiple testing /FWER EM algorithm??? Random effects models / GEEs General linear model Propensity Score methods\n\nA.3.1 Bayesian Analysis:\nBayes factors - https://statsedge.org/shiny/LearnBF/\n\nA.3.2 Machine Learning\ntodo\n\nA.3.3 Getting grants:\nA couple of very good tips for grants and papers at: https://sites.google.com/a/nyu.edu/ankit-parekh/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Other helpful resources</span>"
    ]
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Introduction to Clinical Research",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\nBy the end of this module you will be able to:\n\nSelect a feasible, worthwhile research topic that balances rigor, resources, and personal motivation.\nDraft a reproducible study plan. This entails formulating a testable question, choosing an appropriate design, outlining data acquisition, and creating (and possibly preregistering) an analytic protocol.\nPerform and interpret (frequentist) statistical analyses, including interpreting p-values, classifying data-types, and and avoiding common pitfalls.\nBuild and explain basic regression models (linear, logistic, survival), including understanding when adjustment is required/desired, interpreting the regression coeficients, and communicating results clearly.\nPrepare a manuscript suitable for peer review, applying reporting guidelines (e.g., STROBE) and constructing a coherent argument from introduction through discussion.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "intro_stats.html#what-is-a-statistical-test",
    "href": "intro_stats.html#what-is-a-statistical-test",
    "title": "5  Statistical Foundations",
    "section": "\n5.6 What is a statistical test?",
    "text": "5.6 What is a statistical test?\n[ ] create some examples - choosing a statistical test.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_regression.html#objective-4-understand-the-logic-of-regression-analysis",
    "href": "intro_regression.html#objective-4-understand-the-logic-of-regression-analysis",
    "title": "6  Regressions",
    "section": "",
    "text": "Chance\nConfounding (some other factor influences the exposure and the outcome)\nBias \nOr, causation (a real effect)\n\n\n\n\n\n\nInferential Statistics: Hypothesis testing with confounding control\nDescriptive Statistics: Summarize the strength of association\nPrediction of an outcome (e.g. statistical machine learning)\n\n\n\nIndependent observations (special “mixed models” can relax this)\nThe form of the output variable is correct* \nThe form of the predictor variables are correct\nThe relationship between the predictors are properly specified.**\nAdditional constraints (e.g. constant variance)\n\n\n\nNo model is perfect, but some models are useful\n\n\nMorris moment(TM)\n\n\n\n\n\n\n\n\n\n\nLevel of measurement of outcome variable\nTwo Independent Groups without Confounding Adjustment\nTwo Independent Groups without Confounding Adjustment\n\n\nDichotomous\nChi2 Test\nlogistic regression\n\n\nUnordered categorical\nChi2 Test\nmultinomial logistic regression\n\n\nOrdered categorical\nWilcoxon-Mann-Whitney\nordinal logistic regression\n\n\nContinuous (normally distributed)\nT-test\nlinear regression\n\n\nCensored: time to event\nLog-rank test\nCox regression\n\n\n\n\n\n\n\nFor linear regression: additive change in outcome\nFor logistic regression: multiplicative change in odds of the outcome\nFor Cox regression: multiplicative change in the hazard of the outcome. \n\n\n\n\n\n\n\n\n\nCausal relationship of Splenectomy and CTEPH\n\n\n\n\n\nCausal diagram of Splenectomy, Prox_v_dist, and CTEPH",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regressions</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#objective-3-the-logic-of-frequentist-inferential-statistics",
    "href": "intro_stats.html#objective-3-the-logic-of-frequentist-inferential-statistics",
    "title": "5  Statistical Foundations",
    "section": "",
    "text": "David Hume: causation is never directly observed\n\n\n\nChance\nConfounding (some other factor influences the exposure and the outcome)\nBias\nOr, causation (meaning, a real effect)\n\n\n\nWhen you have eliminated the impossible, whatever remains, however improbable, must be the truth. \n\n\n\n\n\n\n\nSequence\nFlips\nP-value\n\n\nHH\n2 flips\n0.25\n\n\nHHH\n3 flips\n0.125\n\n\nHHHH\n4 flips\n0.0625\n\n\n\n4.32 flips\n0.05\n\n\nHHHHH\n5 flips\n0.03125\n\n\n\n\n\n\nIf 10 people = (1-0.03125)^10 = 0.727.  23.3% of at least 1 HHHHH -&gt; we’re back to weak evidence of a real effect. \nThis is obviously true when multiple tests are reported, but less obviously also true if you try several analyses and choose the “best one” after seeing the result. Hence, prespecification.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevel of measurement of outcome variable\nTwo Independent Groups\nThree or more Independent Groups\nTwo Correlated* Samples\nThree or more Correlated* Samples\n\n\nDichotomous\n\nchi-square or Fisher’s exact test\n\nchi-square or Fisher-Freeman-Halton test\nMcNemar test\nCochran Q test\n\n\nUnordered Categorical\n\nchi-square or Fisher-Freeman-Halton test\n\nchi-square or Fisher-Freeman-Halton test\nStuart-Maxwell test\nMultiplicity adjusted Stuart-Maxwell tests#\n\n\n\nOrdered categorical\nWilcoxon-Mann-Whitney (WMW) test\n\nOld School***: Kruskal-Wallis analysis of variance (ANOVA)\nNew School***: multiplicity adjusted WMW test\n\nWilcoxon sign rank test\n\nOld School# Friedman two-way ANOVA by ranks\nNew School# Mulitiplicity adjusted Wilcoxon sign rank tests\n\n\n\nContinuous\nindependent groups t-test\n\nOld school***: oneway ANOVA\nNew school***: multiplicity adjusted independent groups t tests\n\npaired t-test\nmixed effects linear regression\n\n\nCensored: time to event\nlog-rank test\nMultiplicity adjusted log-rank test\nShared-frailty Cox regression\nShared-frailty Cox regression",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#how-to-formulate-your-scientific-question-as-a-statistically-testable-hypothesis",
    "href": "intro_stats.html#how-to-formulate-your-scientific-question-as-a-statistically-testable-hypothesis",
    "title": "5  Statistical Foundations",
    "section": "\n5.3 How to formulate your scientific question as a statistically testable hypothesis?",
    "text": "5.3 How to formulate your scientific question as a statistically testable hypothesis?\n\nFraming the question: https://open.substack.com/pub/statsepi/p/sorry-what-was-the-question-again?r=12se9v&utm_medium=ios&utm_campaign=post is it about description, prediction, causation, or measurement?\n\nWithout a doubt, the most common error trainees make when formulating their scientific hypothesis as a (statistically-)testable hypothesis is not realizing that you must define a null hypothesis where there is no effect, then find evidence against it - so-called Null Hypotheses Significance Testing. It goes something like this:\n\nScientifically, I suspect that high PaCO2 levels indicate a patient is at higher risk of 30-day readmission\nThe null hypothesis is that PaCO2 levels are not associated with risk of 30-day readmission\nI then evaluate the numbers of readmissions across the range of PaCO2 levels and ask the question: is there a substantial enough excess in readmissions among patients with higher PaCO2 levels that it’s unlikely to be explainable just by random variation?\nif it’s more suspicious than getting heads 4.32 times in a row, we say that chance alone can’t explain it.\n\n(Note, I’ll need to make separate arguments that confounders or biased assessment of either PaCO2 or readmission could explain it, if I want to make a convincing argument is the PaCO2 per se - for more on that, Chapter 6)\nSignal vs noise:\n\n\n5.3.1 Severity:\nA last topic that I’ve included here - but would often usually be introduced later… but I think warrants early exposure - is statistical severity. If we zoom out - we see that we’re generating exclusionary evidence against “null” hypothesis - but we’re not necessarily summarizing the strength of evidence in favor of a hypothesis.\nHow should we summarize how much evidence for a hypothesis? The idea is that we have evidence in favor of a theory in proportion to the strength of challenges it has survived. For example, if theres a theory that makes a prediction that would be extremely unlikely to occur unless the theory is true -e.g. einstein predicting the gravity-induced curvature of light - but then the evidence does NOT disprove it… that’s pretty strong evidence - because that test was severe.\nSimilarly, if you prespecify an analysis - stick to the statistical analysis plan, and find an effect - that’s a much stronger challenge than if you have a lot of ways to slice the data or analyze the effect. Prespecification is more severe.\nApplied to your design - you want to balance power (see Chapter 4) and severity. ***",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "index.html#must-you-learn-to-do-your-own-stats",
    "href": "index.html#must-you-learn-to-do-your-own-stats",
    "title": "Introduction to Clinical Research",
    "section": "Must you learn to do your own stats?",
    "text": "Must you learn to do your own stats?\nStrictly speaking, no. Not every successful clinician-researcher masters statistics or study design. Research increasingly involves collaborative teams, where methodologists and/or statisticians can handle technical details. In fact, some advocate for involving a statistician in every research project, a model that is more feasible in well-resourced settings.\nHowever, relying exclusively on others for statistical expertise has important drawbacks:\n\nSpeed: Conducting at least basic analyses yourself substantially accelerates research.\nEffectiveness: Understanding key statistical concepts enables more productive interactions with the rest of the research team, as you’re more likely to understand key issues and understand the proposed solutions.\nIndependence: Dependence on external expertise restricts the range of research questions you can feasibly explore.\nEducational Value: A stated reason (see note) that many training programs retain research requirements is that familiarity with study design and statistics directly enhances clinical practice by improving your interpretation of research literature. Outsourcing related tasks will not help you critically evaluate evidence or apply findings effectively to patient care.\n\nNote: There are less complimentary reasons that explain why such requirements actually persist.\nFor these reasons and others, I strongly recommend acquiring statistical and study design skills, even though it’s not strictly required in all cases. I still recommend it even if you have access to statisticians and methodologists because it will help you interact with them, and it’ll help you appraise the research.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Introduction to Clinical Research",
    "section": "About the author",
    "text": "About the author\nI am an Assistant Professor of Research in the Department of Critical Care at Intermountain Medical Center and core faculty in the University of Utah Pulmonary and Critical Care Medicine Fellowship Program and closely involved with the University of Utah Internal Medicine Residency. I earned a Master of Science in Clinical Investigation in 2024. My academic research primarily addresses the epidemiology of hypercapnic respiratory failure, clinical reasoning, and research informatics. For more information about my current projects, visit reblocke.github.io",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#todo-list",
    "href": "index.html#todo-list",
    "title": "Introduction to Clinical Research",
    "section": "TODO List:",
    "text": "TODO List:\n\n# Trouble shoot regression and chi2 equivalence... may be easier with linear regression.\n# Create an example of using chatgpt to troubleshoot an error\n# Find statisticians in all projects citation - and set up references/bibliography\n\nIf you find errors or have suggestions (content, resources, corrections, anything), email me and {first}dot{last}at imail.org",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "project_selection.html#sec-mentor",
    "href": "project_selection.html#sec-mentor",
    "title": "1  How to Select a Project",
    "section": "\n1.2 What Kind of Mentor(s) Do You Need?",
    "text": "1.2 What Kind of Mentor(s) Do You Need?\nThe perfect mentor doesn’t exist, and even good mentors are challenging to identify.\nThe concept of “mentoring up” has a lot of value. Essentially, the idea is to be thoughtful about what you want from a mentor, and being proactive about facilitating that.\nI think there is a spectrum from “entirely under your mentors wing” (your mentor has a project in mind and you do it) to “free-range mentorship” (you have an idea, but enroll a mentors help). In the case where there is an available mentor that is working on something that is working on exactly what you’re interested in, there’s no trade-off. The classic arrangement is entirely under your mentors wing - and this has the highest chance of getting a worthwhile result for the least amount of trouble/work.\nHowever, trainee-driven research has it’s benefits and can work [ ]EBTapper Paper. First, the rate-limiting resource is enthusiasm, so choosing an idea you’re excited about (ie. addressing a gap you feel passionate about) may ultimately be a good strategy. Furthermore, the process of identifying a gap, and finding a feasible way to address that gap really is the most interesting bit of research - so it’s a shame to cede that roll to the mentor. There’s something fulfilling about seeing an idea through from start to finish. If you do opt for a proposal that is more free-range, ensure your mentor is still going to be adequately supportive and can help to get the resources you may need.\nConversely, you should know that a trainee-initiated research project is going to be riskier and more work, so make the choice informed by how motivated you are to drive your research forward. I’d advice you to not go all the way in either direction.\nFood for thought - but consider taking a bit more autonomy in project creation than mentors might suggest.\nGreat summarization of early career approach: https://www.aasurg.org/blog/moving-forward-going-faster-scaling-impact-strategies-to-develop-early-career-surgeon-scientists/\nFigure idea: How do you find a good study question? (unique dataset? unique analysis? - why hasn’t it been done before?) - there is always a balance between what you find interesting, what potential mentors are working on, and what will be feasible in a fellow timeline. [ ] venn diagram.\nMatthew Effect https://x.com/jenlovechem/status/1755590367477436695?s=46&t=5eJ6uoTQrbbYTlHIOnRYRg Yikes - Mobility, male, and institutional funding matter - https://x.com/elife/status/1755029870784954520?s=46&t=5eJ6uoTQrbbYTlHIOnRYRg\nMentee literature: https://hbr.org/2017/11/what-mentors-wish-their-mentees-knew\nClarify what you need in a mentor:\nLongitudinal research / career mentor vs 1x (negotiating a job offer, speaking at a national meeting, or finding a job at another hospital.)\n=&gt;coaches, sponsors, and connectors. specific, narrower challenges such as preparing for a speaking engagement often benefit from a coach \n=&gt; sponsor: senior physicians (such as chiefs, chairs, or deans) who have garnered substantial social and political capital over their careers. Sponsors use their cachet to help high-potential individuals join prestigious committees, study groups, or honorific societies.\n=&gt; connector, a seasoned guide who can help the mentor and mentee unite, or build a mentorship team (CMRs? APDs?)\nfind mentors they can see themselves becoming\n-Ideal mentees thus learn to underpromise (“I’ll have a first draft to you in one week”) and overdeliver (“I know it’s only been three days, but I have a first draft ready to share with you”). - give your mentor enough time to review work products (for example, one week for abstracts and at least two to three weeks for grants). -Define goals for meetings ahead of time by knowing what you want to discuss and accomplish during your meeting. -Importantly, avoid long, winding emails with little in the form of an answerable question. Rather, frame questions so that they can be answered with yes-or-no answers, while reserving longer concerns for face-to-face meetings\n(Converse article - https://hbr.org/2017/03/6-things-every-mentor-should-do)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "project_selection.html#todo-list",
    "href": "project_selection.html#todo-list",
    "title": "1  How to Select a Project",
    "section": "TODO List:",
    "text": "TODO List:\n\n# Find the EB Tapper citation on fellow-driven research \n# Set up citations and glossary\n#",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Select a Project</span>"
    ]
  },
  {
    "objectID": "study_design.html#variable-definitions",
    "href": "study_design.html#variable-definitions",
    "title": "4  Basics of Study Design",
    "section": "",
    "text": "4.1.1 Exposures\nThe ‘cause’ that we investigate is generally termed the exposure. This could represent a specific exposure (in the common language sense), like air pollution. But it is also used to refer to treatment group assignment of 0 or 1 in a trial, or things that aren’t modified per se, like race/ethnicity, if we’re interested on the influence of that variable on our outcome of interest.\n\n4.1.2 Outcome\nThe thing you’re interesting in measuring is referred to as the outcome. Choosing an outcome is not as simple as it would seem. Consider a trial of a medication for ARDS.\nA common argument is made for using “hard outcomes”, like whether a patient died or not 90-days after they were enrolled in the trial. A benefit to this type of outcome is that when an effect is found, there’s little doubt as to whether it’s important or how to quantify the magnitude. We can then choose one of a handful of measure to summarize the effect: a risk ratio (risk of death among exposed over risk of death among non-exposed), the risk difference (risk of death among exposed - risk of death in the unexposed), or the odds ratio ( [exposed alive / exposed dead] / [nonexposed alive / non-exposed dead] Odds ratio : https://onlinelibrary.wiley.com/doi/10.1111/test.12391 ).\nHowever, the downside is that the outcome doesn’t contain much information, so a binary outcome is not very efficient. All we get is whether a patient died or not at 90-days. For the exposure to be shown to have an effect, there must be enough patients where they would have died if they hadn’t received the treatment, but would survive with it. For everyone else (they were destined to live or destined to die regardless of the exposure), we don’t learn anything. Consequently, it takes a lot of patients in the trial to convincingly separate signal from noise.\nA variety of more informative end-point could be used: - you could define an ordinal outcome like: alive and independent at 90-days, alive but discharged to facility, alive but still in acute care, or dead - which increases the power as you now can learn something from patients who would potentially switch categories. Sometimes this effect is quantified using a win ratio or a proportional odds ratio - you could look at time to death, knowing that death won’t be observed in many patients (called right censoring). This means that you’d capture even the patients where the treatment kept them alive for longer, but not enough for their to be a counterfactual difference at 90-days (which generally increases the power, but might not mirror what we care about so much). This is a more efficient option when more people have the outcome. - you could model the duration of respiratory support on each day in the following period, such as using a longitudinal ordinal model. That way, you have many opportunities to learn whether the exposure is having an effect (each day, it can be influencing the amount of ventilator support). However, the outcome of the analysis may be harder to quantify and also may be less patient centered.\nEndpoint considerations: https://hbiostat.org/endpoint/\n\n4.1.3 Considerations in choosing an outcome\n‘Signal to noise’: [ ] maybe move this to the study design one? I think so - cover power at the same time.\nif the alternative hypothesis is that the coin is subtly imbalanced, it’ll be a much harder to detect signal. This is the logic of power analysis - if you’re looking for a subtle signal (e.g. small difference, noisy data, rare events), you’ll need a bigger study.\nHowever, there is no free lunch: a statistical tests makes assumptions about the data, and if those assumptions hold in reality, then the implications from the analysis follow.\nExample interpretation:\nIf the p-value from a Chi2 test is P=0.03 - we say it’s as unlikely this would occur from just chance as it would be to flip a fair coin heads 5 times in a row.\nIF observations are independent; both variables are categorical; there are enough observations of each, then it is unlikely chance alone explains the difference. \n[ ] cover signal to noise ratio: history? https://www.sensible-med.com/p/doing-statistics-can-be-difficult\nWhat’s power got to do with it?\nEverything is underpowered ***\nPower - https://onlinelibrary.wiley.com/doi/full/10.1111/test.12403?campaign=wolearlyview",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basics of Study Design</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#the-disjunctive-syllogism",
    "href": "intro_stats.html#the-disjunctive-syllogism",
    "title": "5  Statistical Foundations",
    "section": "",
    "text": "David Hume: Causation is never directly observed\n\n\n\nWhen you have eliminated the impossible, whatever remains, however improbable, must be the truth. \n\n\n\n\n\nConfounding (some other factor, the confounder, influences the likelihood of the exposure and the likelihood of the outcome through other mechanisms)\nBias (some non-random distortion of the measurements in a study)\nChance\nOr, causation (meaning, a real effect)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#whats-a-p-value-good-for",
    "href": "intro_stats.html#whats-a-p-value-good-for",
    "title": "5  Statistical Foundations",
    "section": "\n5.2 What’s a P-value good for?",
    "text": "5.2 What’s a P-value good for?\nTo provide evidence for cause and effect, we need compelling arguments against confounding, bias, and chance. Arguments against confounding are best done by by study design (e.g. randomization) but can be addressed using statistical methods (see Chapter 6). Bias is best addressed by careful choice of instruments/assessments, but can also be supported by elements of study design (e.g. blinding).\nWhether or not chance is a plausible explanation or not is where P-values come in. Repeated for emphasis: P-values ONLY address the plausibility of chance explaining an apparent association. Having a low p-value tells you nothing about whether confounding or bias could explain an association - you need other arguments for that.\n\n5.2.1 Intuitively, what is a p-value?\nIf there was no association, how surprising would it be to see the observed data?\n\nif there is a large p-value, it would not be very surprising to see the observed data by just the play of chance.\nif the p-value is small, then it would be quite surprising to see the observed data if there’s really nothing going on.\n\nNote: the P-value is not comparing one hypothesis to another (ie. an alternative hypothesis). Nor is it saying how likely it would be to find an effect if there is something going on - it’s just saying: “if there’s actually nothing going on here (ie. no causal effect, no confounding, no bias), how unlikely would this finding be?\nThis is a conditional probability: “IF nothing is going on” (or, assume nothing is going on), then, how unlikely would this be. The notation is P ( observed_data | nothing going on).\nBox: note that p-values are NOT a measure of how strong evidence is. A finding with p-value 0.03 isn’t necessarily less robust even than p-value 0.001 - even if there is no bias or confounding. Consider, a huge study can have a very small p-value (unlikely chance.. but could be a small amount of bias, confounding, or a trivial effect), or a smaller study could have a larger p-value. This is termed “Lindley’s Paradox”\n\n5.2.2 A note on significance\n‘Statistically significant’ is a bit of a tricky term, as it may seem to imply a sense of importance. However, a result that is statistically signficant does not necessarily need to be important. Consider a very large study that can very precisely estimate an effect. It might be very unlikely that finding is due to chance (signficant), but it might be so small that we don’t care much (unimportant). The converse is also true: an apparently large effect might not reach statistical significance (so we can’t be sure, to the usual standard, that its not chance) but be very important - as in the case where, say, ECMO may reduce 90-day mortality.\nSome have advocated for switching to an alternative term: compatability rather than significance https://x.com/lester_domes/status/1941165288617324828?s=46&t=5eJ6uoTQrbbYTlHIOnRYRg to emphasize it’s just about the ranges of data, not their imporance. I like this framing. If the data is compatable with the null hypothesis, it’s not a strong argument against the null being true. However, if it’s surpsising enough, that can be a strong argument.\n\n5.2.3 How surprising is too surprising?\nArguing by coincidence begs the question: how surprising does something have to be before you say “there must be something going on here!”.\nLuckily, you can convert P-values to coin flips to get an intuitive sense (look inside the box for the actual conversion, but feel free to skip of logarithms make you queasy).  \nBox: “Shannon Transform: S-value”\nIn a world where the null value is true (ie. either heads or tails is equally likely to occur; but it’s going to be one of those two), you can characterize how suprising it would be to see a particular sequence. this is calculated by S-value = -log_2_ P-value.\nHere’s the setup… say I start flipping a coin, how many consecutive “Heads” need to occur before you’ll suspect it’s not a fair coin? \n\n\nSequence\nFlips (S-Value)\nP-value\n\n\nHH\n2 flips\n0.25\n\n\nHHH\n3 flips\n0.125\n\n\nHHHH\n4 flips\n0.0625\n\n\n\n4.32 flips\n0.05\n\n\nHHHHH\n5 flips\n0.03125\n\n\nMore at: https://stat.lesslikely.com/s-values/\nIMPORTANT POINT: the p-value is NOT the probability that the coin is biased. It is the probability of seeing that result (or more extreme) ASSUMING the coin is biased. \nSo, to answer the question from the lede… the specific answer to “How surprising is too surprising for chance to be a compelling explanation? Since we, as a community, have agreed that P = 0.05 is the threshold… a bit more surprising than 4 consecutive heads with a fair coin is too suspicious.\nBox: Multiplicity understanding multiplicity: If we all flipped a coin 5 times, what is the chance that one of us would get 5 heads in a row? \n\nIf 10 people = (1-0.03125)^10 = 0.727.  23.3% of at least 1 HHHHH -&gt; we’re back to weak evidence of a real effect. \nThis is obviously true when multiple tests are reported, but less obviously also true if you try several analyses and choose the “best one” after seeing the result. Hence, prespecification.\n\nBox: What, then, do confidence intervals mean?\nNote - it’s NOT that there’s a 95% chance that the true value is within this range. It’s if you execute this method 100 times, you’d the value to be outside the range 5% of the time. [ ].gif\nHow do you choose the right test?\nWe have to go one step further to understand where the P-value comes from because of one sneaky clause that I snuck into the definition of the p-value. Quote: “P-values summarize how surprising it would be to see the observed data, assuming there is nothing going on and the assumptions of the test hold.”\nLike regression models (covered soon Chapter 6), there are many statistical tests, each of which requires assumptions to be made. The main characteristics of the data that allow one to choose an appropriate tests are how many groups, the size of the groups, the type of variable, and whether measurements are independent of each other (ie. knowing one observation tells you nothing about another) or correlated (e.g. observation from the same patient at two different times)?\n\n\n\n\n\n\n\n\n\n\nLevel of measurement of outcome variable\nTwo Independent Groups\nThree or more Independent Groups\nTwo Correlated* Samples\nThree or more Correlated* Samples\n\n\n\nDichotomous\n(e.g. yes/no)\n\n\nchi-square or Fisher’s exact test\n\nchi-square or Fisher-Freeman-Halton test\nMcNemar test\nCochran Q test\n\n\n\nUnordered Categorical\n(e.g. red/blue/grey)\n\n\nchi-square or Fisher-Freeman-Halton test\n\nchi-square or Fisher-Freeman-Halton test\nStuart-Maxwell test\nMultiplicity adjusted Stuart-Maxwell tests#\n\n\n\n\nOrdered categorical\n(e.g. bad, neutral, good)\n\nWilcoxon-Mann-Whitney (WMW) test\n\nOld School***: Kruskal-Wallis analysis of variance (ANOVA)\nNew School***: multiplicity adjusted WMW test\n\nWilcoxon sign rank test\n\nOld School# Friedman two-way ANOVA by ranks\nNew School# Mulitiplicity adjusted Wilcoxon sign rank tests\n\n\n\n\nContinuous\n(e.g. BMI)\n\nindependent groups t-test\n\nOld school***: oneway ANOVA\nNew school***: multiplicity adjusted independent groups t tests\n\npaired t-test\nmixed effects linear regression\n\n\n\nTime to event\n(e.g. survival)\n\nlog-rank test\nMultiplicity adjusted log-rank test\nShared-frailty Cox regression\nShared-frailty Cox regression\n\n\n\nFrom: From: Stoddard GJ. Biostatistics and Epidemiology Using Stata: A Course Manual. Salt Lake City, UT: University of Utah School of Medicine.\nSide-box: How can you collaborate effectively with a statistician? They will know these assumptions and can tell you when your analyses makes dubious assumptions (if you communicate the constraints of the problem correctly). THus, the reason to know about these things, even if you’re enrolling the help of a statistician, is to realize when you’re making a decision that bakes assumptions into your analysis - and you want to help the statistician understand the clinical situation so they can help match)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "intro_regression.html#what-use-are-regression-models",
    "href": "intro_regression.html#what-use-are-regression-models",
    "title": "6  Intro to Regressions",
    "section": "",
    "text": "Descriptive: in the sample, how much, on average, does an individual’s weight increase for each inch of height, after adjusting for age and sex? The goal here is generally to summarize the strength of association.\nPredictive: how much would you expect a 40 year old male who is 5’ 10” to weigh? The goal here is to make a prediction about something not observed (yet).\nCausal: how much would I weigh if I were 2 inches taller? The intention here is generally inferential (ie. to test a hypothesis)\n\n\n\n\n\n6.1.1 When do you, and when do you not want an adjusted estimate?\nTo answer this question, we need to differentiate a couple of related terms that describe the way that an Exposure (E) -&gt; Outcome (O) relationship can be changed by a third variable. To explain this, we’ll use Directed Acyclic Graphs, which can be created with tools like Dagitty (or the python package package GraphViz). [ ]\nThe idea with a DAG is that you draw your hypothesized causal connections between variables to define how you think they are related. These are assumptions of your statistical model and are not necessarily testable.\nIt’s helpful for our purposes because it can define the different ways that three variables may interrelate.\n\n6.1.1.1 Confounder (C)\nAs mentioned earlier, a confounder is something that influences both the likelihood of exposure and the likelihood of the outcome via other mechanisms than through the exposure.\nFor example, consider the risk of death in patients with elevated PaCO2 levels. Say that older patients are more likely to get hypercapnia. In an unadjusted analysis, PaCO2 levels might be associated with death because age is associated with likelihood of hypercapnia, and likelihood of death via a variety of mechanisms. In this case, what we’d really like to do is evaluate whether PaCO2 levels remain associated with the risk of death after controlling for age (a confounder).\n[ ] DAG. - DAG references: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7124493/ \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom matplotlib import style\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport statsmodels.formula.api as smf\nimport graphviz as gr\nfrom linearmodels.iv import IV2SLS\nfrom IPython.display import display, SVG, HTML\n\npd.set_option(\"display.max_columns\", 5)\nstyle.use(\"fivethirtyeight\")\n\n\ng = gr.Digraph()\n\ng.edge(\"ability\", \"educ\")\ng.edge(\"ability\", \"wage\")\ng.edge(\"educ\", \"wage\")\n\nsvg = g.pipe(format=\"svg\").decode(\"utf-8\")  \nHTML(svg)\n\nability\nabilityeduc\neducability-&gt;educ\nwage\nwageability-&gt;wage\neduc-&gt;wage\n\n\n\n\ng = gr.Digraph()\n\ng.edge(\"ability\", \"educ\")\ng.edge(\"ability\", \"wage\")\ng.edge(\"educ\", \"wage\")\ng.edge(\"instrument\", \"educ\")\ng\n\n&lt;graphviz.graphs.Digraph object at 0x15bdaa990&gt;\n\n\n\n6.1.1.2 Mediator (M)\nImagine you are starting someone on home bilevel positive airway pressure (BPAP) for chronic hypercapnic respiratory failure and you are interested in assessing how much it reduces their risk of readmission. Let’s also say that you measure how much the PaCO2 level improves 1 month after starting BPAP. One might be tempted to put PaCO2 at 1 month into the regression model because we know that PaCO2 influences readmission risk.\nHowever, change to PaCO2 during follow-up may be part of how BPAP reduces risk: if re-establishing a normal PaCO2 set point is an indicator of effective nocturnal ventilation. Thus, if adjust for change in PaCO2, you’re asking “What is the association between BPAP and readmissions independent of PaCO2 change”? This might be an interesting question - and is termed what is the ‘direct effect’ of BPAP. However, if what you’re interested in is the overall, or ‘total effect’ - e.g. to answer whether a patient should be placed on BPAP - you wouldn’t want to remove that ‘indirect effect’.. you’re interested in quantifying the benefit, whether or not it’s mediated by change in PaCO2 levels.\n[ ] mediator example\n\n6.1.1.3 Effect Modifier\nEffect modifiers refer to situations where a third variable influences the effect of the exposure on the outcome. Technically, effect modifiers refer to characteristics of the treatment that are, in principle, changable while interactions refer to the same concept that is not changeable.\nAs a hypothetical example, a bacterial pathogen is presumably an effect modifier for response to antibiotics on community acquired pneumonia: if bacterial CAP, antibiotics help, and if viral it probably doesn’t.\nThis might seem like a very distinct concept from mediators and confounders - and it is. But the reason I bring it up here is that it’ll appear similar in regression analyses if not specified. In fact, there’s no way ‘just from the data’ that you can differentiate between an Effect Modifier, a mediator, and a confounder - you must reason through what you think the relationship is between each of the variables and then treat them appropriately.\nNote: there is not a uniformly agreed upon way to represent an effect modifiers in DAG, though there are proposals (see ***).\nOne last note on effect modifiers and interactions: they are difficult to estimate in the data. This is because, in practice, interactions/effect modififiers are usually smaller than the main effects we aim to detect AND because we’re trying to isolate signal from noise of two (rather than 1) sources. This consequences of this is that it often takes a much largers ample to convincingly detect an interaction. However, if you have a reason to suspect one exists, you should probably include it in your DAG and modeling strategy regardless.\n\n6.1.2 So, what should go in a regression model?\nExtrapolating from the points made above, clearly sometimes the right thing to do is not include anything in a regression model (ie. just present the unadjusted effect estimate).\nOn the other end of the spectrum, it is rarely correct to “put everything” into the regression model. This is true for a few reasons:\n\nIt’s tricky to understand what you’re then estimating\nYou can introduce biases by adjusting for a collider: something that is an effect of two or more variables in the model. E.g. You can get into the NBA by being tall or a good shooter, so - if you look at NBA players (ie conditioning on NBA team membership), you’ll see an inverse relationship even if there is no association in the general population.\nUnless your dataset has many more observations than variables, you’ll overfit the data.\n\n[ ] collider example\nTHus, there’s guidance from the AJRCCM editors (and other documents in other fields) that generally recommend something like this:\n\nBefore looking at the values in your data, sit down and draw a DAG of how you think your variables (and variables that you wish you could measure, even if you can’t) are causally related. www.dagitty.net can be used for this\nConsider what it is that you want to estiamte - ie. the estimand: are you interested in the total effect? the direct effect? an unadjusted effect (ie for prediction), or an effect adjusting for all relevant confounders (ie. for causal effect detection?)\nLook for the paterns that identify the above relationships\nThen, come up with a list of the confounders you want to adjust for, whether you want to account for mediation (usually, no), and whether there are interactions or effect modifiers. And, to the extent possible, avoid unintended collider stratification.\n\nSo, in our example, we’d like to control for age and baseline paCO2, but not change (or follow-up PaCO2)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Regressions</span>"
    ]
  },
  {
    "objectID": "intro_regression.html#understand-the-logic-of-regression-analysis",
    "href": "intro_regression.html#understand-the-logic-of-regression-analysis",
    "title": "6  Intro to Regressions",
    "section": "\n6.2 Understand the logic of regression analysis",
    "text": "6.2 Understand the logic of regression analysis\nRecall, if there is an association between an ‘exposure’ and an ‘outcome’, there are 4 possible explanations\n\nChance\nConfounding (some other factor influences the exposure and the outcome)\nBias \nOr, causation (a real effect)\n\nRandomization = takes care of some (but not all) of the other reasons for an association = can convince a skeptic. However, observational research must make arguments (based on assumptions) and they must be explicit. Randomization addresses point 2 (essentially, converts it to point 1, in that only chance confounding can occur)\nFor non-randomized data, you must make an argument against point 2. This is the most common use of regression. \n[the methods section of your paper is the argument against point 3; pull in RECORD/STROBE recs]\n\nYou can use regressions to make predictions (e.g. using a linear regression to predict the FEV1 by age, sex, and height)\nYou can summarize the association between two variables after “adjusting” for the effect of other variables in the model (e.g. the effect of changing a treatment indicator from 0 to 1, correcting for other observed covariates)\n\nRegression comes with additional assumptions: \n\nIndependent observations (special “mixed models” can relax this)\nThe form of the output variable is correct* \nThe form of the predictor variables are correct\nThe relationship between the predictors are properly specified.**\nAdditional constraints (e.g. constant variance)\n\nThus the logic is: if the assumptions of the models hold in reality, then the described relationships are valid\n\nNo model is perfect, but some models are useful\n\n\nMorris moment(TM)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Regressions</span>"
    ]
  },
  {
    "objectID": "stat_software.html#sec-options",
    "href": "stat_software.html#sec-options",
    "title": "2  Statistical Software",
    "section": "",
    "text": "R\nPython\nStata\n\n\nCost\nFree\nFree\nRequires License\n\n\nIDE\nRStudio/Posit\nMany, Visual Code is good\nBuilt in editor\n\n\nStrengths\nBest libraries for epidemiology, trial statistics.\nBest libraries for text processing, machine learning, AI integration\nSimple syntax; powerful quasi-experimental/meta-analysis packages. Used by U of U MSCI.\n\n\nWeakness\nClunky syntax; many ‘dialects’\nOverkill for many, complex development environment\nClunkiest machine learning, explainable programing, cost.\n\n\nExplainable programmingWriting analytic code in a clear, structured, and understandable way so that readers (including reviewers, collaborators, and future researchers) can easily follow and replicate the steps of your analysis. In quantitative disciplines such as mathematics and computer science, providing such code is already standard practice. Medicine is beginning to adopt this approach, recognizing its importance for transparency and reproducibility.\nQuarto\nJupyter, Quarto\nNot native (though can use Jupyter)\n\n\n\n\n\n\n\nterm\n\n\ndefinition\n\n\n\n\nExplainable programming\n\n\nWriting analytic code in a clear, structured, and understandable way so that readers (including reviewers, collaborators, and future researchers) can easily follow and replicate the steps of your analysis. In quantitative disciplines such as mathematics and computer science, providing such code is already standard practice. Medicine is beginning to adopt this approach, recognizing its importance for transparency and reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#sec-install",
    "href": "stat_software.html#sec-install",
    "title": "2  Statistical Software",
    "section": "\n2.2 How to install",
    "text": "2.2 How to install\nChoose the tab for the language(s) you plan to use:\n\n2.2.1 Instructions:\n\n\nR\nPython\nStata\n\n\n\n\n\n1:\nInstall R Language\nhttps://cran.r-project.org/\nThis installs the base programming language\n\n\n2:\nInstall RStudio\nhttps://posit.co/downloads/\nRStudio is an IDEIntegrated Development Environment, a program that allows for writing, running, and debugging code within a single program. (integrated development environment) that allows you to write, execute, and debug code from within a single program.\n\n\n3:\nInstall Quarto (formerly Markdown)\nhttps://quarto.org/docs/get-started/\nFacilitates sharing and explaining your code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1:\n\n\n\n\nInstall Python Language and dependencies\n\n\n\n\nhttps://github.com/conda-forge/miniforge?tab=readme-ov-file\n\n\n\n\nThis (mini-forge) installs the base Python programming language, the things it depends on, and many useful packages\n\n\n\n\n\n\n2:\n\n\n\n\nCreate an environment\n\n\n\n\nExecute the following commands in a terminal:\n\n\nbash Miniforge3-MacOSX-&lt;arch&gt;.sh\n\n\nconda init “\\((basename \"\\)SHELL”)“\n\n\nexec “$SHELL”\n\n\nwhich mamba || conda install -n base -c conda-forge mamba\n\n\nconda activate stats && python -m ipykernel install –user –name stats –display-name Python (stats)\n\n\n\n\nThis sets up an environment stats with all needed packages, then activates a Kernel (controls the versions and packages that are used).\n\n\n\n\n\n\n3:\n\n\n\n\nInstall Visual Code\n\n\n\n\nhttps://code.visualstudio.com/download then add Python and Jupyter extensions.\n\n\nIn VS Code: Command Palette → Shell Command: Install ‘code’ command in PATH\n\n\n\n\nVisual Code is an IDE (integrated development environment) that allows you to write, execute, and debug code from within a single program.\n\n\n\n\n\n*Note: there are many IDEIntegrated Development Environment, a program that allows for writing, running, and debugging code within a single program.. Visual Code is a classic one with a lot of functionality, though there are AI enabled ones (e.g. Cursor) that may be more helpful depending on how much programming you plan to do and whether you want to bother with the added complexity (discussed more in Section 2.7).\nYou can also use Quarto for explainable programming in Python - but Jupyter is a more common workflow so we focus on that.\n\n\n\n\n1:\nGet a product key\nif U of U Trainee, contact me\nThis verifies you or your institutions’ purchase\n\n\n2:\nInstall STATA\nhttps://www.stata.com/install-guide/\nIncludes language and IDEIntegrated Development Environment, a program that allows for writing, running, and debugging code within a single program. (integrated development environment)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#sec-packages",
    "href": "stat_software.html#sec-packages",
    "title": "2  Statistical Software",
    "section": "\n2.3 Packages",
    "text": "2.3 Packages\nFor common statistical analyses in any of these languages, specialized packages already exist that handle these tasks efficiently. Whenever you find yourself manually calculating or coding a statistical procedure, consider that someone likely has already written reliable, tested code that will perform the analysis faster and more accurately. You’ll want to use these packages whenever possible.\nFirst, a few terms: functionA reusable piece of code that performs a specific task. Examples include calculating the mean of a dataset or running logistic regression., argumentsInputs provided to a function so it can perform its task. For instance, a function calculating a mean needs a dataset, while logistic regression requires data, the outcome variable, and predictor variables., packageA curated collection of functions designed to accomplish related tasks. Programming languages come pre-installed with basic packages, but you’ll often download additional packages to access specialized functions. Each language provides straightforward ways to locate and install new packages.\n\n\n\nterm\n\n\ndefinition\n\n\n\n\n\narguments\n\n\nInputs provided to a function so it can perform its task. For instance, a function calculating a mean needs a dataset, while logistic regression requires data, the outcome variable, and predictor variables.\n\n\n\n\nfunction\n\n\nA reusable piece of code that performs a specific task. Examples include calculating the mean of a dataset or running logistic regression.\n\n\n\n\npackage\n\n\nA curated collection of functions designed to accomplish related tasks. Programming languages come pre-installed with basic packages, but you’ll often download additional packages to access specialized functions. Each language provides straightforward ways to locate and install new packages.\n\n\n\n\n\n2.3.1 Finding Packages\n\n\nR\nPython\nStata\n\n\n\n\n\nWhere to find packages?\nhttps://cran.r-project.org/web/views/\n\n\nCommand to install packages\n\ninstall.packages(`package_name`)\n\n\n\nHow to access documentation file?\n\n?package_name or ?command_name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere to find packages?\n\n\n\n\n\n\nhttps://anaconda.org/conda-forge\n\n\n\n\nhttps://pypi.org/\n\n\n\n\nhttps://github.com/topics/python\n\n\n\n\n\n\n\n\nCommand to install packages\n\n\n\n\nmamba install package_name (rarely, packages that have not been compiled on conda-forge will require pip install package_name to be used)\n\n\n\n\n\n\nHow to access documentation file?\n\n\n\n\nThe project page on https://pypi.org/ or https://github.com/\n\n\n\n\n\n\n\n\n\nWhere to find packages?\nfindit package_name\n\n\nCommand to install packages\nssc install package_name\n\n\nHow to access documentation file?\nhelp package_name",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#sec-reproducible",
    "href": "stat_software.html#sec-reproducible",
    "title": "2  Statistical Software",
    "section": "\n2.4 Reproducible Research",
    "text": "2.4 Reproducible Research\nAs mentioned before, Explainable programmingWriting analytic code in a clear, structured, and understandable way so that readers (including reviewers, collaborators, and future researchers) can easily follow and replicate the steps of your analysis. In quantitative disciplines such as mathematics and computer science, providing such code is already standard practice. Medicine is beginning to adopt this approach, recognizing its importance for transparency and reproducibility. is an increasingly important idea because programming errors frequently lead to erroneous research results. Understandable analytic code is important for co-authors to understand and verify what you’ve done, as well as facilitating replication and improvements.\nImportantly (and, as will be discussed in Chapter 3), it’s not just the statistical design choices that can have an influence on the observed outcomes. Researchers must decide what data should be included, how the data should be cleaned, whether missing values will be imputed (and if so, how). All this occurs before talk of statistical tests, regressions, or presentation occur.\nBy some estimates, variation in how these pre-processing tasks are done accounts for more variability in findings than the design choices most readers focus on. Accordingly, the data processing code that is shared should include the entire process, from data cleaning to figure generation.\nStatistical Methods and analysis: Pitfalls of research: https://journals.stfm.org/primer/2022/van-smeden-2022-0059/#.YvQyN_kxOPE.twitter How to structure the project files: https://medium.com/the-stata-guide/the-stata-workflow-guide-52418ce35006 Reproducible code info: https://twitter.com/lisadebruine/status/1504063177012695047?s=11 Citation for need to release analytic code: https://academic.oup.com/jamia/article-abstract/30/5/859/7056675\n\n2.4.1 How to document and share code\n\n\nR\nPython\nStata\n\n\n\nIn R, the easiest way to put together understandable and sharable code is to use Quarto documents (this website is actually is generated in Quarto).\nQuarto creates notebooks that include segments that can contain text and pictures with other sections that contain executable R code. The segments containing R code can be executed in subsequent order - variables are shared between the boxes - thus allowing a long analytic pipeline to be broken up into small understandable chunks.\n\n\n\n\n\n\nTip\n\n\n\nTo create a new Quarto notebook, go to File-&gt;New-&gt;Quarto Document (*.qmd). Leave all the options in their default. Try it to make the below notebook.\n\n\nFor example, a hypothetical analysis might go something like:\n\n2.4.1.1 Example Simulation of Coin Flips:\nOur aim is to simulate the number of heads seen if a coin is flipped 50 times.\nFirst, we create a function that simulates a fair coin flip: 50% probability heads (coded as 1), 50% probability of tails (coded as 0)\n\n## one fair-coin flip: 1 = Heads, 0 = Tails\nflip_coin &lt;- function(n = 1) {\n  sample(c(0, 1), size = n, replace = TRUE)\n}\n\nThen, we write a program that repesents a single simulation: the coin is flipped 50 times\n\n## run one experiment of 50 flips and return #Heads\nsimulate_50 &lt;- function() {\n  sum(flip_coin(50))\n}\n\n## quick demo\nset.seed(42)   # reproducible example\nsimulate_50()\n\n[1] 27\n\n\nThen, we perform 1000 simulations and store the results in an array, which we then present summary statistics on.\n\nset.seed(42)                       # reproducible simulations\nn_sims  &lt;- 1000                    # how many experiments?\nresults &lt;- replicate(n_sims, simulate_50())\n\nsummary(results)                   # five-number summary\nhist(results,\n     breaks = 20,\n     main   = \"Distribution of heads in 1 000 × 50-flip experiments\",\n     xlab   = \"Number of heads\") \n\n\n\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  14.00   22.00   25.00   24.74   27.00   35.00 \n\n\n\n2.4.1.2 Nuts and Bolts\nObviously, this example is a bit contrived, but is meant to show how the text and code can be interleaved to make the resulting output easy to understand. This quarto file can then be shared with collaborators (or posted on a code sharing site, such as github - provided none of the protected health information is hard coded into the text.\n\n\n\n\n\n\nTip\n\n\n\nAs an exercise, see if you can create your own Quarto markdown file, and extend the analysis to generate the interquartile range for the mean number of Heads in 50 flips.\n\n\n\n\n\nIn Python, the easiest way to put together understandable and sharable code is to use Jupyter notebooks.\nJupyter (like Quarto) creates notebooks that include segments that can contain text and pictures with other sections that contain executable Python code. The segments containing Python code can be executed in subsequent order - variables are shared between the boxes - thus allowing a long analytic pipeline to be broken up into small understandable chunks.\n\n\n\n\n\n\nTip\n\n\n\nTo create a new Jupyter notebook, go to File -&gt; New Jupyter Notebook (*.ipynb). Try it to make the below notebook.\n\n\nFor example, a hypothetical analysis might go something like:\n\n2.4.1.3 Example Simulation of 50 Coin Flips\nOur aim is to simulate the number of heads seen if a coin is flipped 50 times.\nFirst, we create a function that simulates a fair coin flip: 50% probability heads (coded as 1), 50% probability of tails (coded as 0)\n\nimport random\n\ndef flip_coin(n: int = 1) -&gt; list[int]:\n    \"\"\"Return a list of 0 / 1 draws; 1 = heads, 0 = tails.\"\"\"\n    return random.choices([0, 1], k=n)\n\nThen, we write a program that repesents a single simulation: the coin is flipped 50 times\n\ndef simulate_50() -&gt; int:\n    \"\"\"Flip a coin 50 × and return the number of heads.\"\"\"\n    return sum(flip_coin(50))\n\nrandom.seed(42)          # reproducible example\nsimulate_50()\n\n25\n\n\nThen, we perform 1000 simulations and store the results in an array, which we then present summary statistics on.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrandom.seed(42)\n\nn_sims  = 1000\nresults = [simulate_50() for _ in range(n_sims)]\n\n# summary statistics\nprint(f\"min  : {min(results)}\")\nprint(f\"max  : {max(results)}\")\nprint(f\"mean : {np.mean(results):.2f}\")\nprint(f\"sd   : {np.std(results, ddof=1):.2f}\")\n\n# visualise\nplt.hist(results, bins=20, edgecolor=\"black\")\nplt.title(\"Heads in 1 000 × 50-flip experiments\")\nplt.xlabel(\"Number of heads\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nmin  : 13\nmax  : 36\nmean : 24.93\nsd   : 3.48\n\n\n\n\n\n\n\n\n\n2.4.1.4 Nuts and Bolts\nObviously, this example is a bit contrived, but is meant to show how the text and code can be interleaved to make the resulting output easy to understand. This Jupyter notebook can then be shared with collaborators (or posted on a code sharing site, such as github - provided none of the protected health information is hard coded into the text.\n\n\n\n\n\n\nTip\n\n\n\nAs an exercise, see if you can create your own Jupyter notebook file, and extend the analysis to generate the interquartile range for the mean number of Heads in 50 flips.\n\n\n\n\n\nStata does not have a native notebook-style way to share code, but instead you have *.do files that contain a script of the relevant commands, and you can add comments using either # , *, or //* *//*\n\n\n\n\n\n\nTip\n\n\n\nTo create a new STATA do file, go to File -&gt; New -&gt; Do File (*.do). Try it to make the below script.\n\n\nStata Code: *.do files dont have an easy way to execute within the web page, so you’ll have to run the below script on your own machine to see the output.\n*-----------------------------------------------------*\n* 1.  Set the number of simulations                  *\n*-----------------------------------------------------*\nlocal nsims   = 1000      // how many experiments?\nlocal nflips  = 50        // flips per experiment\n\nset obs `nsims'           // create `nsims' empty observations\n\n* Optional: reproducibility\nset seed 42\n\n*-----------------------------------------------------*\n* 2.  Simulate 50 fair-coin flips for each experiment *\n*     A \"head\" is coded 1 when runiform() &lt; .5        *\n*-----------------------------------------------------*\nforvalues j = 1/`nflips' {\n    generate byte flip`j' = runiform() &lt; .5\n}\n\n*-----------------------------------------------------*\n* 3.  Count heads in each experiment                  *\n*-----------------------------------------------------*\negen heads = rowtotal(flip1-flip`nflips')\n\n*-----------------------------------------------------*\n* 4.  Inspect the sampling distribution               *\n*-----------------------------------------------------*\nsummarize heads\n\nhistogram heads , ///\n    width(1) start(0.5)          /// each bin spans one integer count\n    title(\"Heads in `nsims' × `nflips'-flip experiments\") ///\n    xlabel(0(5)50)               /// tick every 5 heads; adjust as desired\n    ylabel(, angle(horizontal))\n\n\n2.4.1.5 Nuts and Bolts\nObviously, this example is a bit contrived, but is meant to show how the text in comments and code can be interleaved to make the resulting output easy to understand. This do file can then be shared with collaborators (or posted on a code sharing site, such as github - provided none of the protected health information is hard coded into the text.\n\n\n\n\n\n\nTip\n\n\n\nAs an exercise, see if you can create your own Stata do file, and extend the analysis to generate the interquartile range for the mean number of Heads in 50 flips.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#sec-llm",
    "href": "stat_software.html#sec-llm",
    "title": "2  Statistical Software",
    "section": "\n2.7 Large Language Models (LLMs)",
    "text": "2.7 Large Language Models (LLMs)\nRandomized trials (!) show the professional coders are more productive when using LLMs to assist with code. Novices see larger relative gains because LLMs supply boiler-plate, interface syntax, and “first drafts” of code.\nHowever, LLMs can hallucinate. They can also be tricky to use. Here’s some guidance that holds for any of the frontier company models (OpenAI: chatGPT, Anthropic Claude, Google Gemini)\n\n2.7.1 Minimal viable set up:\n\n\nStep\nWhy\nTool\n\n\n\nUse your local IDEIntegrated Development Environment, a program that allows for writing, running, and debugging code within a single program. with a plain chat tab or desktop app.\nAllows you to (quality)control inputs and outputs - which is harder to do with an integrated IDE (e.g. Cursor) or agentic models.\nBuilt-in browser tab or official desktop app\n\n\nDo NOT upload data to LLMs\nThe companies do not have Business Access Agreements with institutions, and this generally violates the consent/IRB authorization of most studies.\nKeep data local, only share schemas (descriptions of the variables) or mock data.\n\n\nSanity test all LLM-generated code\nHallucinations exist, and often task specifications are subtly wrong\nYou can writing coding tests (so called “Unit Tests”, but should also visualize and consider all code (this is true when not using LLMs)\n\n\nGive the LLM examples of what you want\nLLMs are VERY good at understanding what existing code does - and can often modify much better than create de-novo\nUse things like: here’s the code that generates this figure - modify it so that title is larger.\n\n\nGive the LLM specific instructions, often with steps\nThe more “context” you give the LLM for what you want, the more likely the associations it follows will be relevant\nDetailed instructions in the prompt. More on prompt engineering is available here: https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNever put PHI into a commercial LLM interface. Not even ‘publicly available’ data like MIMIC (which has terms of use that forbid this). Most companies market their LLMs as being able to act as data-analysts on your behalf (meaning, you give it the data and it analyzes it for you). Don’t do this. https://pmc.ncbi.nlm.nih.gov/articles/PMC10937180/\nInstead, use the LLM to help you write statistical code that you then run on your machine.\n\n\n\n\n\n\n\n\n\nMistake\nConsequence\nQuick fix\n\n\n\nSkipping LLM entirely\nSlower learning curve\nUse it for boiler-plate, explaining things, first drafts\n\n\nBlind trust\nSilent bugs ≈ misleading science\nUnit tests, peer-review, benchmark against known outputs\n\n\nVague prompt\nGeneric, unusable code\nInclude language, package, data schema, desired output\n\n\nManual debugging\nTime sink\nFeed the exact error message back to the model\n\n\nPasting PHI\nCompliance breach\nUse synthetic or sampled data; keep true identifiers offline",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#an-example-meta-analyzing-the-effect-of-steroids-in-cap",
    "href": "stat_software.html#an-example-meta-analyzing-the-effect-of-steroids-in-cap",
    "title": "2  Statistical Software",
    "section": "\n2.5 An Example: meta-analyzing the effect of *** steroids in CAP",
    "text": "2.5 An Example: meta-analyzing the effect of *** steroids in CAP\nThis example is chosen to demonstrate how to make a reproducible notebook, download a package, and execute the relevant commands.\nExample - simple meta-analysis in all 3 languages? (all default in STATA?)\ninstall the needed packages (meta, excel spreadsheet import)\ndisplay the data\nmeta-analyze\ngenerate a figure.\nRelevant packages: https://cran.r-project.org/web/views/MetaAnalysis.html\nThe ‘meta’ package looks good. Try using `install.packages(‘meta’)` to install it, then you can you can access the documentation using `?meta`\nLet’s try an example… I extracted data on all of the trials comparing high O2 to low O2 targets and uploaded to github.\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nhead(data_sheet)\nauthors &lt;- select(data_sheet, author)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nauthor\nyear\ndoi\nnum_randomized\nnum_patients\nnum_high_o2\nhigh_o2_died\nhigh_o2_alive\nnum_low_o2\nlow_o2_died\nlow_o2_alive\ntarget\noutcome\npopulation\n\n\n\nOxygen-ICU\nGirardis\n2016\n10.1001/jama.2016.11993\n460\n432\n216\n74.0\n142.0\n216\n52.000\n164.000\neither\nin-hosp\nAll\n\n\nCLOSE\nPanwar\n2016\n10.1164/rccm.201505-1019OC\n104\n103\n51\n19.0\n32.0\n52\n21.000\n31.000\nspo2\n90d\nAll\n\n\nHYPER2S\nAsfar\n2017\n10.1016/S2213-2600(17)30046-2\n442\n434\n217\n104.0\n113.0\n217\n90.000\n127.000\nsao2 vs fio2\n90d\nSeptic Shock\n\n\nLang2018\nLang\n2018\n10.1111/aas.13093\n65\n65\n38\n9.0\n29.0\n27\n8.000\n19.000\nfio2\n6m\nTBI\n\n\nCOMACARE\nJakkula\n2018\n10.1007/s00134-018-5453-9\n123\n120\n59\n20.0\n39.0\n61\n18.000\n43.000\npao2\n30d\nOHCA\n\n\nICU-ROX\nMackle\n2020\n10.1056/NEJMoa1903297\n1000\n965\n484\n157.3\n326.7\n481\n166.907\n314.093\nspo2\n90d\nunknown\n\n\n\n\n\n\nNow, let’s meta-analyze it:\n\nlibrary(meta)\n\nLoading required package: metadat\n\n\nLoading 'meta' package (version 8.1-0).\nType 'help(meta)' for a brief overview.\n\n#metabin takes events, total (rather than events, nonevents)\nm_ex1 &lt;- meta::metabin(low_o2_died, num_low_o2, high_o2_died, num_high_o2, data = data_sheet, studlab = paste(name, author, year), sm = \"OR\")\nmeta::forest(m_ex1, comb.random = FALSE, lab.c = \"High Oxygen\", lab.e = \"Low Oxygen\", label.left = \"Favors Low O2\", label.right = \"Favors High O2\")\n\nWarning: Use argument 'label.e' instead of 'lab.e' (deprecated).\n\n\nWarning: Use argument 'label.c' instead of 'lab.c' (deprecated).\n\n\n\n\n\n\n\n\nAnd if you want to get really cutting edge, you can do a trial sequential analysis (TSA) on it:\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.2     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWarning: Unknown or uninitialised column: `order`.\nUnknown or uninitialised column: `order`.\n\n\nWarning in RTSA(type = \"analysis\", data = rtsa_df, outcome = \"RR\", mc = 0.9, :\nNB. The required information size adjusted by Diversity (D^2). This might cause\nan under-powered analysis. Consider changing the argument `random_adj` from\n`D2` (default) to `tau2`.\n\n\n$study\n[1] \"character\"\n\n$author\n[1] \"character\"\n\n$year\n[1] \"numeric\"\n\n$doi\n[1] \"character\"\n\n$num_randomized\n[1] \"numeric\"\n\n$num_patients\n[1] \"numeric\"\n\n$nI\n[1] \"numeric\"\n\n$eI\n[1] \"numeric\"\n\n$high_o2_alive\n[1] \"numeric\"\n\n$nC\n[1] \"numeric\"\n\n$eC\n[1] \"numeric\"\n\n$low_o2_alive\n[1] \"numeric\"\n\n$target\n[1] \"character\"\n\n$outcome\n[1] \"character\"\n\n$population\n[1] \"character\"\n\n$study\ninteger(0)\n\n$author\ninteger(0)\n\n$year\ninteger(0)\n\n$doi\ninteger(0)\n\n$num_randomized\ninteger(0)\n\n$num_patients\ninteger(0)\n\n$nI\ninteger(0)\n\n$eI\ninteger(0)\n\n$high_o2_alive\ninteger(0)\n\n$nC\ninteger(0)\n\n$eC\ninteger(0)\n\n$low_o2_alive\ninteger(0)\n\n$target\ninteger(0)\n\n$outcome\ninteger(0)\n\n$population\ninteger(0)\n\n\n\nplot(an_rtsa)\n\nWarning in geom_segment(aes(x = 0, xend = max(sma_timing, na.rm = T), y = y_val1, : All aesthetics have length 1, but the data has 13 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, xend = 0, y = y_val1, yend = y_val2)): All aesthetics have length 1, but the data has 13 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\nMeaning, we’ve passed futility (at 90% power) for a 10% relative risk reduction a few trials ago. Cool.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#sec-utah_resources",
    "href": "stat_software.html#sec-utah_resources",
    "title": "2  Statistical Software",
    "section": "\n2.8 Local (and other) resources",
    "text": "2.8 Local (and other) resources\n\nOne Data Science Hub Workshops: https://utah-data-science-hub.github.io/education_archived.html  \nRequest CTSI help: https://ctsi.utah.edu/cores-and-services/triad \nR book (very good, but in-depth): https://r4ds.had.co.nz/index.html\n\nRules for statistical computing: https://journals.plos.org/ploscompbiol/article?id=10.1371%2Fjournal.pcbi.1005510\nHelpful R resources: tbl_summary: https://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html tbl_regression: https://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html\nHelpful Python Resources: Guide to Python for those familiar with STATA - https://github.com/aeturrell/coding-for-economists/blob/main/coming-from-stata.md Guide to Python for those familiar with R - https://www.emilyriederer.com/post/py-rgo/?utm_source=substack&utm_medium=email\nHelpful STATA resources:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#todo-list",
    "href": "stat_software.html#todo-list",
    "title": "2  Statistical Software",
    "section": "TODO List:",
    "text": "TODO List:\n\n# find citation on how frequently errors occur in scientific programming.  - https://www.nature.com/collections/prbfkwmwvz \n# find citation for data cleaning influence on stability of findings",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "stat_software.html#an-example-meta-analyzing-the-effect-of-corticosteroids-in-cap",
    "href": "stat_software.html#an-example-meta-analyzing-the-effect-of-corticosteroids-in-cap",
    "title": "2  Statistical Software",
    "section": "\n2.6 An Example: Meta-analyzing the effect of corticosteroids in CAP",
    "text": "2.6 An Example: Meta-analyzing the effect of corticosteroids in CAP\nLet’s use meta-analysis as a quick example of how to put this all into action.\nA meta-analysis typically accompanies a systematic review to comprehensively capture relevant studies on a question. Systematic reviews are complex, so we’ll skip the details here. Instead, we’ll perform a meta-analysis using a prepared spreadsheet of all known steroids-for-CAP studies (let me know if any studies are missing).\nDownload the data here: Steroid CAP Trials Spreadsheet\n\n# ---- display the table ------------------------------------------------------\nkable(steroids_pna, caption = \"Steroids PNa meta-analysis data\")\n\n\nSteroids PNa meta-analysis data\n\n\n\n\n\n\n\n\n\n\nstudy\nyear\nchest_ma\nint_death\nint_alive\npla_death\npla_alive\n\n\n\nWagner\n1956\n0\n1\n51\n1\n60\n\n\nMcHardy\n1972\n0\n3\n37\n9\n77\n\n\nMarik\n1993\n0\n1\n13\n3\n13\n\n\nConfalonieri\n2005\n0\n0\n23\n7\n16\n\n\nEl Ghamrawy\n2006\n0\n3\n14\n6\n11\n\n\nMikami\n2007\n0\n1\n15\n0\n15\n\n\nSnijders\n2010\n0\n6\n98\n6\n103\n\n\nMelvis\n2011\n0\n17\n134\n19\n134\n\n\nFerrandez-Serrano\n2011\n0\n1\n27\n1\n27\n\n\nSabry\n2011\n0\n2\n38\n6\n34\n\n\nNafae\n2013\n0\n4\n56\n6\n14\n\n\nBlum\n2015\n0\n16\n386\n13\n387\n\n\nTorres\n2015\n0\n8\n53\n11\n48\n\n\nLloyd\n2019\n0\n69\n338\n63\n362\n\n\nWittermans\n2021\n0\n3\n206\n7\n196\n\n\nESCAPe Study\n2023\n0\n47\n239\n50\n227\n\n\nDequin\n2023\n0\n25\n375\n47\n348\n\n\nREMAP-CAP\n2025\n1\n78\n443\n12\n110\n\n\n\n\n\nNow, we’ll cover how you’d meta-analyze these studies in each language as an exercise:\n\n\nR\nPython\nStata\n\n\n\nIn R, here’s the steps needed to perform the meta-analysis:\nFirst, set your working directory to wherever you downloaded the file. Then, we’ll need to make sure we have the right package to read in the spreadsheet\nWe’ll need the ‘readxl’ package to read in the *.xls file https://cran.r-project.org/web/packages/readxl/index.html\nThen, we write code to import the spreadsheet with study data\n\nlibrary(readxl)                           # install.packages(\"readxl\") if needed\n\n## --- OPTION A – read from a local copy ------------------------------------\n# setwd(\"~/your_dir\")                       # uncomment and edit this line\n# dat &lt;- read_xls(\"Steroids PNa MA.xls\", sheet = \"Sheet1\") |&gt;\n#        janitor::clean_names()\n\n## --- OPTION B – read directly from the GitHub raw file             ----\nurl  &lt;- \"https://raw.githubusercontent.com/reblocke/statistics_sandbox/\" |&gt;\n        paste0(\"1b87fb4e65da11a7f9541ea9c7f93b7e3947a13a/\",\n                \"Steroids%20PNa/Steroids%20PNa%20MA.xls\")\n\ntmp  &lt;- tempfile(fileext = \".xls\")          # download to a temp file\ndownload.file(url, tmp, mode = \"wb\")\n\ndat  &lt;- read_xls(tmp, sheet = \"Sheet1\") |&gt;\n        janitor::clean_names()                       # lower-case, snake_case\n\n# Inspect the key columns\ndat |&gt;\n  dplyr::select(study, year,\n                int_death, int_alive,\n                pla_death, pla_alive) |&gt;\n  print()\n\n# A tibble: 18 × 6\n   study              year int_death int_alive pla_death pla_alive\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Wagner             1956         1        51         1        60\n 2 McHardy            1972         3        37         9        77\n 3 Marik              1993         1        13         3        13\n 4 Confalonieri       2005         0        23         7        16\n 5 El Ghamrawy        2006         3        14         6        11\n 6 Mikami             2007         1        15         0        15\n 7 Snijders           2010         6        98         6       103\n 8 Melvis             2011        17       134        19       134\n 9 Ferrandez-Serrano  2011         1        27         1        27\n10 Sabry              2011         2        38         6        34\n11 Nafae              2013         4        56         6        14\n12 Blum               2015        16       386        13       387\n13 Torres             2015         8        53        11        48\n14 Lloyd              2019        69       338        63       362\n15 Wittermans         2021         3       206         7       196\n16 ESCAPe Study       2023        47       239        50       227\n17 Dequin             2023        25       375        47       348\n18 REMAP-CAP          2025        78       443        12       110\n\n\nNext, we’ll need to get a package to do the meta-analysis. Here’s a list of relevant packages: https://cran.r-project.org/web/views/MetaAnalysis.html\nThe ‘meta’ package looks good. Try using `install.packages(‘meta’)` to install it, then you can you can access the documentation using `?meta`\nNow, we’ll perform a random-effects meta-analysis (Using DL variance stimator)\n\nlibrary(meta)                             # install.packages(\"meta\")\n\nLoading required package: metadat\n\n\nLoading 'meta' package (version 8.1-0).\nType 'help(meta)' for a brief overview.\n\nm &lt;- metabin(\n  event.e     = int_death,\n  n.e         = int_death + int_alive,\n  event.c     = pla_death,\n  n.c         = pla_death + pla_alive,\n  studlab     = paste(study, year),\n  data        = dat,\n  sm          = \"OR\",      # odds ratio\n  method.tau  = \"DL\",      # &lt;-- DerSimonian–Laird estimator\n  method.random.ci        = FALSE\n)\n\nLastly, we’ll need to plot the result as a Forest Plot on a log scale (always log scale with ratio variables)\n\nforest(\n  m,\n  comb.fixed  = FALSE,\n  comb.random = TRUE,\n  print.tau2  = FALSE,\n  backtransf  = TRUE,\n  xlab        = \"Odds Ratio (death)\",\n  leftlabs    = c(\"Study (year)\", \"Steroid\", \"Placebo\"),\n  xlog        = TRUE,\n  at          = c(1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32),\n  label.e     = \"Steroid\",\n  label.c     = \"Placebo\",\n  col.diamond = \"navy\",\n  overall.lty = 2,\n  ## keep the default right-hand columns\n  # rightcols = FALSE,          # &lt;-- delete this line\n  main        = \"Steroids OR for Death, Random-Effects Meta-analysis\"\n)\n\n\n\n\n\n\n\n\n\nIn Python, here’s the steps needed to perform the meta-analysis:\nFirst, set your working directory to wherever you downloaded the file. Then, we’ll need to make sure we have the right package to read in the spreadsheet\nThen, we write code to import the spreadsheet with study data using pandas\n\nimport pandas as pd\n\n# --- OPTION A: read directly from disk -------------------------------\n# df = pd.read_excel(\"Steroids PNa MA.xls\", sheet_name=\"Sheet1\")\n\n# --- OPTION B:  For this demo we fetch the raw file from GitHub (same content as R block)\nurl = (\"https://raw.githubusercontent.com/reblocke/statistics_sandbox/\"\n       \"1b87fb4e65da11a7f9541ea9c7f93b7e3947a13a/\"\n       \"Steroids%20PNa/Steroids%20PNa%20MA.xls\")\ndf = pd.read_excel(url)           # GitHub serves raw file\n\n# clean column names to snake_case like janitor::clean_names()\ndf.columns = (df.columns\n                .str.strip()\n                .str.lower()\n                .str.replace(\" \", \"_\"))\n\n# quick sanity check\ndf.head(18)\n\n                study  year  chest_ma  ...  int_alive  pla_death  pla_alive\n0              Wagner  1956         0  ...         51          1         60\n1             McHardy  1972         0  ...         37          9         77\n2               Marik  1993         0  ...         13          3         13\n3        Confalonieri  2005         0  ...         23          7         16\n4         El Ghamrawy  2006         0  ...         14          6         11\n5              Mikami  2007         0  ...         15          0         15\n6            Snijders  2010         0  ...         98          6        103\n7              Melvis  2011         0  ...        134         19        134\n8   Ferrandez-Serrano  2011         0  ...         27          1         27\n9               Sabry  2011         0  ...         38          6         34\n10              Nafae  2013         0  ...         56          6         14\n11               Blum  2015         0  ...        386         13        387\n12             Torres  2015         0  ...         53         11         48\n13              Lloyd  2019         0  ...        338         63        362\n14         Wittermans  2021         0  ...        206          7        196\n15       ESCAPe Study  2023         0  ...        239         50        227\n16             Dequin  2023         0  ...        375         47        348\n17          REMAP-CAP  2025         1  ...        443         12        110\n\n[18 rows x 7 columns]\n\n\nNext, we’ll need to get a package to do the meta-analysis. We’ll use the statsmodels (statsmodels.stats.meta_analysis) package https://www.statsmodels.org/dev/examples/notebooks/generated/metaanalysis1.html\nNow, we’ll perform a random-effects meta-analysis (Using DL variance estimator - which takes some manual preparation work).\n\nimport numpy as np\nfrom statsmodels.stats.meta_analysis import (\n    effectsize_2proportions,    # log-odds-ratio + variance   \n    combine_effects             # pooling with DL            \n)\n\n# --- 2. per-study log(OR) & variance with 0.5 correction --------------------\na = df[\"int_death\"].to_numpy(dtype=float)\nb = df[\"int_alive\"].to_numpy(dtype=float)\nc = df[\"pla_death\"].to_numpy(dtype=float)\nd = df[\"pla_alive\"].to_numpy(dtype=float)\n\n# continuity correction if any cell is zero in that study\ncc = ((a == 0) | (b == 0) | (c == 0) | (d == 0)).astype(float) * 0.5\na += cc; b += cc; c += cc; d += cc\n\nlog_or = np.log((a * d) / (b * c))\nvar_or = 1 / a + 1 / b + 1 / c + 1 / d          # variance of log(OR)\n\n# --- 3. DerSimonian-Laird random-effects pooling ---------------------------\nres = combine_effects(log_or, var_or, method_re=\"dl\")   # DL estimator\n\nprint(\"Current statsmodels version:\", importlib.metadata.version(\"statsmodels\"))\n\nsf = res.summary_frame()          # still log-OR\nsf[\"eff\"]    = np.exp(sf[\"eff\"])\nsf[\"ci_low\"] = np.exp(sf[\"ci_low\"])\nsf[\"ci_upp\"] = np.exp(sf[\"ci_upp\"])\nprint(sf.round(3))\n\nCurrent statsmodels version: 0.14.4\n                     eff  sd_eff  ci_low  ci_upp   w_fe   w_re\n0                  1.176   1.427   0.072  19.285  0.005  0.010\n1                  0.694   0.696   0.177   2.714  0.020  0.036\n2                  0.333   1.219   0.031   3.638  0.006  0.013\n3                  0.047   1.495   0.002   0.878  0.004  0.009\n4                  0.393   0.814   0.080   1.936  0.014  0.028\n5                  3.000   1.672   0.113  79.499  0.003  0.007\n6                  1.051   0.594   0.328   3.369  0.027  0.046\n7                  0.895   0.356   0.446   1.796  0.075  0.092\n8                  1.000   1.440   0.059  16.822  0.005  0.010\n9                  0.298   0.850   0.056   1.578  0.013  0.026\n10                 0.167   0.711   0.041   0.672  0.019  0.035\n11                 1.234   0.380   0.586   2.600  0.066  0.085\n12                 0.659   0.506   0.245   1.774  0.037  0.059\n13                 1.173   0.190   0.808   1.702  0.264  0.150\n14                 0.408   0.697   0.104   1.599  0.020  0.036\n15                 0.893   0.223   0.576   1.383  0.191  0.137\n16                 0.494   0.258   0.297   0.819  0.142  0.123\n17                 1.614   0.328   0.849   3.069  0.089  0.100\nfixed effect       0.861   0.098   0.711   1.042  1.000    NaN\nrandom effect      0.785   0.145   0.590   1.044    NaN  1.000\nfixed effect wls   0.861   0.121   0.678   1.092  1.000    NaN\nrandom effect wls  0.785   0.146   0.590   1.045    NaN  1.000\n\n\nLastly, we’ll need to plot the result as a Forest Plot on a log scale (always log scale with ratio variables)\n\nimport matplotlib.pyplot as plt\n\n# draw the default forest plot (no extra kwargs)\nfig = res.plot_forest(use_exp=True)\n\n# post-process the axis with Matplotlib\nax = fig.axes[0]          # the single Axes returned by plot_forest\nax.set_xscale(\"log\")\nax.set_xlim(0.03125, 32)\nax.set_xlabel(\"Odds Ratio (death)\")\nax.set_xticks([1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32])\nax.set_xticklabels(\n    [\"1/32\", \"1/16\", \"1/8\", \"1/4\", \"1/2\",\n     \"1\", \"2\", \"4\", \"8\", \"16\", \"32\"]\n)\n\nplt.title(\"Steroids OR for Death, Random-Effects Meta-analysis (DL)\")\nplt.tight_layout()\nplt.show()\n\n(0.03125, 32)\n\n\n\n\n\n\n\n\n\n\nAs before, Stata does not have a native notebook-style way to execute code within this webpage, so you’ll have to run the below script on your own machine to see the output.\nHere’s the steps:\n\n*-----------------------------------------------------*\n* 1. Set your_dir to wherever you downloaded the doc  *\n*-----------------------------------------------------*\ncd your_dir\n\n*-----------------------------------------------------*\n* 2. Import the Spreadsheet with Study Data           *\n*-----------------------------------------------------*\nclear\nimport excel \"Steroids PNa MA.xls\", sheet(\"Sheet1\") firstrow case(lower)\nlist study year int_death int_alive pla_death pla_alive\nlabel variable study \"Study\"\n\n*-----------------------------------------------------*\n* 3. Meta-analyze the studies using REML              *\n*-----------------------------------------------------*\nmeta esize int_death int_alive pla_death pla_alive, random(reml) esize(lnor) studylabel(study year) \n\n*-----------------------------------------------------*\n* 4. Create a forest plot with the result             *\n*-----------------------------------------------------*\nmeta forestplot, eform  nullrefline(lcolor(gs3)) esrefline(lcolor(gs3) lpattern(dash_dot)) title(\"Steroids OR for Death, Random-Effects Meta-analysis\")  xsize(11) ysize(7) xscale(log range(0.01 64)) xlabel(0.03125 \"1/32\" 0.0625 \"1/16\" 0.125 \"1/8\" 0.25 \"1/4\" 0.5 \"1/2\" 1 \"1\" 2 \"2\" 4 \"4\" 8 \"8\" 16 \"16\" 32 \"32\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "data.html#getting-data",
    "href": "data.html#getting-data",
    "title": "3  Data",
    "section": "",
    "text": "Each row is an observation (usually a patient)\nEach column contains only 1 type of data (more below)\nNo free text (if you need to, categorize responses)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-get_data",
    "href": "data.html#sec-get_data",
    "title": "3  Data",
    "section": "",
    "text": "Database\nFeatures\nLink\n\n\n\nMIMIC III\nEHR, notes, high-frequency physiology; ICU\nhttps://physionet.org/content/mimiciii/1.4/\n\n\nMIMIC IV\nEHR, notes, high-frequency physiology, electrocardiograms, radiologic images, EEG, echocardiograms; Emergency department, hospital, ICU\nhttps://physionet.org/content/mimiciv/2.2/\n\n\neICU\nEHR; ICU\nhttps://physionet.org/content/eicu-crd/2.0/\n\n\nAmsterdamUMCdb\nEHR; ICU\nhttps://amsterdammedicaldatascience.nl/amsterdamumcdb/\n\n\nHiRID\nEHR, high-frequency physiology; ICU; COVID-19 focused\nhttps://physionet.org/content/hirid/1.1.1/\n\n\nSICdb\nEHR; high-frequency physiology; ICU\nhttps://physionet.org/content/sicdb/1.06/\n\n\nZhejiang\nEHR; ICU\nhttps://physionet.org/content/zhejiang-ehr-critical-care/1.0/\n\n\nPediatric Intensive Care\nEHR; ICU\nhttps://physionet.org/content/picdb/1.1.0/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nServices prioritized if they involve a grant or a grant application (4h, or requires seed function). Also can be prioritized as short queue (4-5 hours or less).\n  In the future, there will be a merit reward to prioritize your project.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-formatting",
    "href": "data.html#sec-formatting",
    "title": "3  Data",
    "section": "\n3.2 Formatting",
    "text": "3.2 Formatting\nStep 0: Save yourself a headache and collect your data in a processable format https://open.substack.com/pub/statsepi/p/simple-tips-for-recording-data-in \nFormat article: https://journals.plos.org/ploscompbiol/article?id=10.1371%2Fjournal.pcbi.1012604&utm_source=substack&utm_medium=email\nnaming: https://benharrap.com/post/2025-03-03-variable-naming-convention/\nData abstraction: \nData from web tables  https://twitter.com/asmith83/status/1549373680496656385?s=21&t=4SAl-DHtn3zREP_avr6XaA\nData collection with excel - https://www.youtube.com/watch?v=Ry2xjTBtNFE Also https://twitter.com/blakeaburge/status/1540666548616036353?s=11&t=Dl6FGUjENZiqAT0eXQ_I6A \nStep 1: Data Wrangling\nnaming variables: https://emilyriederer.netlify.app/post/column-name-contracts/ \n\n\nEach row is an observation (usually a patient)\nEach column contains only 1 type of data (more below)\nNo free text (if you need to, categorize responses)\n\nClean tabular format etc.\nPresentation on Cleaning - https://cghlewis.github.io/ncme-data-cleaning-workshop/slides.html\nUse excel like a boss, if you’re going to: More excel data https://cghlewis.com/blog/excel_entry/\nFlat files: Flat files: https://evidence.dev/blog/what-is-a-flat-file?utm_campaign=Data_Elixir&utm_source=Data_Elixir_526\n[ ] data checklist - find attribution for this\n### **Data Quality Indicator Checklist**\n#### **✓ Analyzable**\n- Dataset is in a rectangular (rows and columns), machine-readable format. Variable names are the first row only. The remaining data should be made up of values in cells.\n- One or more columns uniquely identify rows in the data (i.e., primary key).\n- All column values are explicit.\n  - No values are indicated by color coding.\n  - All non-missing values are explicitly defined (e.g., if a blank is assumed to be 0, it is filled with a 0).\n- Only one piece of information is contained in a variable.\n#### **✓ Complete**\n- Everyone in your sample (e.g., consented, included in the study, completed the instrument) is accounted for.\n- If you collected it, it should exist in the file.\n- There should be no rows with duplicate primary keys (e.g., study unique identifiers).\n#### **✓ Interpretable**\n- Variable names are machine-readable (i.e., no special characters or spaces) AND human-readable (consistently formatted and clear to humans).\n- Variable and value labels are added if sharing in SPSS, Stata, or SAS format.\n  - Consider sharing in at least one non-proprietary format (e.g., CSV).\n#### **✓ Valid**\n- Variables conform to the constraints that you have laid out in your data dictionary.\n  - Planned variable types (e.g., numeric, character, date).\n  - Allowable variable values and ranges (e.g., 1-50).\n  - Item-level missingness aligns with variable universe and skip patterns.\n#### **✓ Accurate**\n- There are no glaring errors in the data that you have not acknowledged.\n  - Based on any implicit knowledge you have.\n  - Based on a comparison of information within and across sources.\n#### **✓ Consistent**\n- Variable values are consistently measured, formatted, or categorized within a column.\n- Variables are consistently measured across collections of the same form.\n#### **✓ De-identified**\n- Disclosure risks have been addressed (both direct and indirect identifiers have been removed/altered as needed).\n\n3.2.0.1 Document Codebooks:\nhttps://cghlewis.com/talk/rladies_nyc/ comparison of ways to do this in R",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data_types",
    "href": "data.html#sec-data_types",
    "title": "3  Data",
    "section": "\n3.3 Primer on Data types",
    "text": "3.3 Primer on Data types\nStep 2: For each data element, consider the data type\n\nBinary (aka dichotomous scale): e.g. Yes or No, 0 or 1\nUnordered Categorical (nominal scale): e.g. Utah, Colorado, Nevada, Idaho\nOrdered Categorical (ordinal scale): e.g. Room air, nasal cannula, HFNC, intubated, ECMO, dead\nContinuous (interval & ratio scales - differ by whether 0 is special): e.g. Temperature (Celsius or Kelvin, respectively)\n\n\n\n\n\n\n\n\n\n\n\n\ndichotomous\nnominal\nordinal\ninterval\n\n\na.ka.\nbinary\ncategorical\nordered categorical\ncontinuous\n\n\nn\nX\nX\nX\nX\n\n\n%\nX\nX\nX\nX\n\n\nmin\n\n\nX\nX\n\n\nmax\n\n\nX\nX\n\n\nrange\n\n\nX\nX\n\n\nmode\nX\nX\nX\nX\n\n\nmean\n\n\n\nX\n\n\nmedian\n\n\nX\nX\n\n\nIQR\n\n\nX\nX\n\n\nStd. dev.\n\n\n\nX\n\n\nStd. err.\n\n\n\nX\n\n\n\nFrom: Stoddard GJ. Biostatistics and Epidemiology Using Stata: A Course Manual. Salt Lake City, UT: University of Utah School of Medicine.\nTODO: not sure this stuff should live here vs elsewhere:\nStep 3: Visualize the distribution of each data-point (detect outliers, data entry errors, etc.)\nDarren’s hypothetical code lives in a spreadsheet “darren_proj.xlsx”:\nHere is some code that loads the excel spreadsheet into R (we’ll revisit)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nIt’s already (mostly) clean.\nLet’s summarize it:\n\nsummary(darren_data_sheet)\n\n   patient_id    splenectomy        prox_v_dist           qanadli     \n Min.   : 1.00   Length:20          Length:20          Min.   : 2.00  \n 1st Qu.: 5.75   Class :character   Class :character   1st Qu.: 3.75  \n Median :10.50   Mode  :character   Mode  :character   Median :10.00  \n Mean   :10.50                                         Mean   :10.30  \n 3rd Qu.:15.25                                         3rd Qu.:15.00  \n Max.   :20.00                                         Max.   :25.00  \n   got_cteph?       hosp          \n Min.   :0.00   Length:20         \n 1st Qu.:0.00   Class :character  \n Median :0.00   Mode  :character  \n Mean   :0.25                     \n 3rd Qu.:0.25                     \n Max.   :1.00                     \n\n\nHmmm.. what’s wrong with this?\nR need to be told that the binary variables are binary (and not characters)\n\nlibrary(dplyr)\n\n# Convert 'y'/'n' in the splenectomy column to TRUE/FALSE\ndarren_data_sheet &lt;- darren_data_sheet %&gt;%\n  mutate(splenectomy = ifelse(splenectomy == \"y\", TRUE, FALSE))\n\n# Assuming darren_data_sheet is your dataframe\ndarren_data_sheet &lt;- darren_data_sheet %&gt;%\n  mutate(`got_cteph?` = ifelse(`got_cteph?` == 1, TRUE, FALSE))\n\nLet’s visualize each element:\n\nlibrary(ggplot2)\n\n# First, the binary ones\n\n# Plot for splenectomy\nggplot(darren_data_sheet, aes(x = factor(splenectomy))) +\n  geom_bar() +\n  labs(title = \"Distribution of Splenectomy\", x = \"Splenectomy\", y = \"Count\")\n\n\n\n\n\n\n# Plot for prox_v_dist\nggplot(darren_data_sheet, aes(x = factor(prox_v_dist))) +\n  geom_bar() +\n  labs(title = \"Distribution of Proximal vs. Distal\", x = \"Proximal vs Distal\", y = \"Count\")\n\n\n\n\n\n\n# Plot for got_cteph?\nggplot(darren_data_sheet, aes(x = factor(`got_cteph?`))) +\n  geom_bar() +\n  labs(title = \"Distribution of CTEPH Diagnosis\", x = \"Got CTEPH?\", y = \"Count\")\n\n\n\n\n\n\n\nThe categorical one:\n\n# Bar chart for hosp\nggplot(darren_data_sheet, aes(x = factor(hosp))) +\n  geom_bar(fill = \"coral\", color = \"black\") +\n  labs(title = \"Distribution of Hospital\", x = \"Hospital\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Adjust text angle for better readability if needed\n\n\n\n\n\n\n\nand finally, the continuous one:\n\n# Histogram for qanadli\nggplot(darren_data_sheet, aes(x = qanadli)) +\n  geom_histogram(bins = 30, fill = \"blue\", color = \"black\") +\n  labs(title = \"Histogram of Qanadli Scores\", x = \"Qanadli Score\", y = \"Frequency\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "webexercises.html#example-question-types",
    "href": "webexercises.html#example-question-types",
    "title": "Appendix B — Webexercises",
    "section": "",
    "text": "B.1.1 Fill-In-The-Blanks (fitb())\nCreate fill-in-the-blank questions using fitb(), providing the answer as the first argument.\n\n2 + 2 is \n\n\nYou can also create these questions dynamically, using variables from your R session.\n\nThe square root of 9 is: \n\n\nThe blanks are case-sensitive; if you don’t care about case, use the argument ignore_case = TRUE.\n\nWhat is the letter after D? \n\n\nIf you want to ignore differences in whitespace use, use the argument ignore_ws = TRUE (which is the default) and include spaces in your answer anywhere they could be acceptable.\n\nHow do you load the tidyverse package? \n\n\nYou can set more than one possible correct answer by setting the answers as a vector.\n\nType a vowel: \n\n\nYou can use regular expressions to test answers against more complex rules.\n\nType any 3 letters: \n\n\nB.1.2 Multiple Choice (mcq())\n\n“Never gonna give you up, never gonna: \nlet you go\nturn you down\nrun away\nlet you down”\n“I \nbless the rains\nguess it rains\nsense the rain down in Africa” -Toto\n\nB.1.3 True or False (torf())\n\nTrue or False? You can permute values in a vector using sample(). \nTRUE\nFALSE\n\n\nB.1.4 Longer MCQs (longmcq())\nWhen your answers are very long, sometimes a drop-down select box gets formatted oddly. You can use longmcq() to deal with this. Since the answers are long, It’s probably best to set up the options inside an R chunk with echo=FALSE.\nWhat is a p-value?\n\nthe probability that the null hypothesis is truethe probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is truethe probability of making an error in your conclusion\n\nWhat is true about a 95% confidence interval of the mean?\n\nthere is a 95% probability that the true mean lies within this rangeif you repeated the process many times, 95% of intervals calculated in this way contain the true mean95% of the data fall within this range",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "stat_software.html#sec-steroids_cap",
    "href": "stat_software.html#sec-steroids_cap",
    "title": "2  Statistical Software",
    "section": "\n2.6 An Example: Meta-analyzing the effect of corticosteroids in CAP",
    "text": "2.6 An Example: Meta-analyzing the effect of corticosteroids in CAP\nLet’s use meta-analysis as a quick example of how to put this all into action.\nA meta-analysis typically accompanies a systematic review to comprehensively capture relevant studies on a question. Systematic reviews are complex, so we’ll skip the details here. Instead, we’ll perform a meta-analysis using a prepared spreadsheet of all known steroids-for-CAP studies (let me know if any studies are missing).\nDownload the data here: Steroid CAP Trials Spreadsheet\n\n# ---- display the table ------------------------------------------------------\nkable(steroids_pna, caption = \"Steroids PNa meta-analysis data\")\n\n\nSteroids PNa meta-analysis data\n\n\n\n\n\n\n\n\n\n\nstudy\nyear\nchest_ma\nint_death\nint_alive\npla_death\npla_alive\n\n\n\nWagner\n1956\n0\n1\n51\n1\n60\n\n\nMcHardy\n1972\n0\n3\n37\n9\n77\n\n\nMarik\n1993\n0\n1\n13\n3\n13\n\n\nConfalonieri\n2005\n0\n0\n23\n7\n16\n\n\nEl Ghamrawy\n2006\n0\n3\n14\n6\n11\n\n\nMikami\n2007\n0\n1\n15\n0\n15\n\n\nSnijders\n2010\n0\n6\n98\n6\n103\n\n\nMelvis\n2011\n0\n17\n134\n19\n134\n\n\nFerrandez-Serrano\n2011\n0\n1\n27\n1\n27\n\n\nSabry\n2011\n0\n2\n38\n6\n34\n\n\nNafae\n2013\n0\n4\n56\n6\n14\n\n\nBlum\n2015\n0\n16\n386\n13\n387\n\n\nTorres\n2015\n0\n8\n53\n11\n48\n\n\nLloyd\n2019\n0\n69\n338\n63\n362\n\n\nWittermans\n2021\n0\n3\n206\n7\n196\n\n\nESCAPe Study\n2023\n0\n47\n239\n50\n227\n\n\nDequin\n2023\n0\n25\n375\n47\n348\n\n\nREMAP-CAP\n2025\n1\n78\n443\n12\n110\n\n\n\n\n\nNow, we’ll cover how you’d meta-analyze these studies in each language as an exercise:\n\n2.6.1 Example Code\n\n\nR\nPython\nStata\n\n\n\nIn R, here’s the steps needed to perform the meta-analysis:\nFirst, set your working directory to wherever you downloaded the file. Then, we’ll need to make sure we have the right package to read in the spreadsheet\nWe’ll need the ‘readxl’ package to read in the *.xls file https://cran.r-project.org/web/packages/readxl/index.html\nThen, we write code to import the spreadsheet with study data\n\nlibrary(readxl)                           # install.packages(\"readxl\") if needed\n\n## --- OPTION A – read from a local copy ------------------------------------\n# setwd(\"~/your_dir\")                       # uncomment and edit this line\n# dat &lt;- read_xls(\"Steroids PNa MA.xls\", sheet = \"Sheet1\") |&gt;\n#        janitor::clean_names()\n\n## --- OPTION B – read directly from the GitHub raw file             ----\nurl  &lt;- \"https://raw.githubusercontent.com/reblocke/statistics_sandbox/\" |&gt;\n        paste0(\"1b87fb4e65da11a7f9541ea9c7f93b7e3947a13a/\",\n                \"Steroids%20PNa/Steroids%20PNa%20MA.xls\")\n\ntmp  &lt;- tempfile(fileext = \".xls\")          # download to a temp file\ndownload.file(url, tmp, mode = \"wb\")\n\ndat  &lt;- read_xls(tmp, sheet = \"Sheet1\") |&gt;\n        janitor::clean_names()                       # lower-case, snake_case\n\n# Inspect the key columns\ndat |&gt;\n  dplyr::select(study, year,\n                int_death, int_alive,\n                pla_death, pla_alive) |&gt;\n  print()\n\n# A tibble: 18 × 6\n   study              year int_death int_alive pla_death pla_alive\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 Wagner             1956         1        51         1        60\n 2 McHardy            1972         3        37         9        77\n 3 Marik              1993         1        13         3        13\n 4 Confalonieri       2005         0        23         7        16\n 5 El Ghamrawy        2006         3        14         6        11\n 6 Mikami             2007         1        15         0        15\n 7 Snijders           2010         6        98         6       103\n 8 Melvis             2011        17       134        19       134\n 9 Ferrandez-Serrano  2011         1        27         1        27\n10 Sabry              2011         2        38         6        34\n11 Nafae              2013         4        56         6        14\n12 Blum               2015        16       386        13       387\n13 Torres             2015         8        53        11        48\n14 Lloyd              2019        69       338        63       362\n15 Wittermans         2021         3       206         7       196\n16 ESCAPe Study       2023        47       239        50       227\n17 Dequin             2023        25       375        47       348\n18 REMAP-CAP          2025        78       443        12       110\n\n\nNext, we’ll need to get a package to do the meta-analysis. Here’s a list of relevant packages: https://cran.r-project.org/web/views/MetaAnalysis.html\nThe ‘meta’ package looks good. Try using `install.packages(‘meta’)` to install it, then you can you can access the documentation using `?meta`\nNow, we’ll perform a random-effects meta-analysis (Using DL variance stimator)\n\nlibrary(meta)                             # install.packages(\"meta\")\n\nLoading required package: metadat\n\n\nLoading 'meta' package (version 8.1-0).\nType 'help(meta)' for a brief overview.\n\nm &lt;- metabin(\n  event.e     = int_death,\n  n.e         = int_death + int_alive,\n  event.c     = pla_death,\n  n.c         = pla_death + pla_alive,\n  studlab     = paste(study, year),\n  data        = dat,\n  sm          = \"OR\",      # odds ratio\n  method.tau  = \"DL\",      # &lt;-- DerSimonian–Laird estimator\n  method.random.ci        = FALSE\n)\n\nLastly, we’ll need to plot the result as a Forest Plot on a log scale (always log scale with ratio variables)\n\nforest(\n  m,\n  comb.fixed  = FALSE,\n  comb.random = TRUE,\n  print.tau2  = FALSE,\n  backtransf  = TRUE,\n  xlab        = \"Odds Ratio (death)\",\n  leftlabs    = c(\"Study (year)\", \"Steroid\", \"Placebo\"),\n  xlog        = TRUE,\n  at          = c(1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32),\n  label.e     = \"Steroid\",\n  label.c     = \"Placebo\",\n  col.diamond = \"navy\",\n  overall.lty = 2,\n  ## keep the default right-hand columns\n  # rightcols = FALSE,          # &lt;-- delete this line\n  main        = \"Steroids OR for Death, Random-Effects Meta-analysis\"\n)\n\n\n\n\n\n\n\n\n\nIn Python, here’s the steps needed to perform the meta-analysis:\nFirst, set your working directory to wherever you downloaded the file. Then, we’ll need to make sure we have the right package to read in the spreadsheet\nThen, we write code to import the spreadsheet with study data using pandas\n\nimport pandas as pd\n\n# --- OPTION A: read directly from disk -------------------------------\n# df = pd.read_excel(\"Steroids PNa MA.xls\", sheet_name=\"Sheet1\")\n\n# --- OPTION B:  For this demo we fetch the raw file from GitHub (same content as R block)\nurl = (\"https://raw.githubusercontent.com/reblocke/statistics_sandbox/\"\n       \"1b87fb4e65da11a7f9541ea9c7f93b7e3947a13a/\"\n       \"Steroids%20PNa/Steroids%20PNa%20MA.xls\")\ndf = pd.read_excel(url)           # GitHub serves raw file\n\n# clean column names to snake_case like janitor::clean_names()\ndf.columns = (df.columns\n                .str.strip()\n                .str.lower()\n                .str.replace(\" \", \"_\"))\n\n# quick sanity check\ndf.head(18)\n\n                study  year  chest_ma  ...  int_alive  pla_death  pla_alive\n0              Wagner  1956         0  ...         51          1         60\n1             McHardy  1972         0  ...         37          9         77\n2               Marik  1993         0  ...         13          3         13\n3        Confalonieri  2005         0  ...         23          7         16\n4         El Ghamrawy  2006         0  ...         14          6         11\n5              Mikami  2007         0  ...         15          0         15\n6            Snijders  2010         0  ...         98          6        103\n7              Melvis  2011         0  ...        134         19        134\n8   Ferrandez-Serrano  2011         0  ...         27          1         27\n9               Sabry  2011         0  ...         38          6         34\n10              Nafae  2013         0  ...         56          6         14\n11               Blum  2015         0  ...        386         13        387\n12             Torres  2015         0  ...         53         11         48\n13              Lloyd  2019         0  ...        338         63        362\n14         Wittermans  2021         0  ...        206          7        196\n15       ESCAPe Study  2023         0  ...        239         50        227\n16             Dequin  2023         0  ...        375         47        348\n17          REMAP-CAP  2025         1  ...        443         12        110\n\n[18 rows x 7 columns]\n\n\nNext, we’ll need to get a package to do the meta-analysis. We’ll use the statsmodels (statsmodels.stats.meta_analysis) package https://www.statsmodels.org/dev/examples/notebooks/generated/metaanalysis1.html\nNow, we’ll perform a random-effects meta-analysis (Using DL variance estimator - which takes some manual preparation work).\n\nimport numpy as np\nfrom statsmodels.stats.meta_analysis import (\n    effectsize_2proportions,    # log-odds-ratio + variance   \n    combine_effects             # pooling with DL            \n)\n\n# --- 2. per-study log(OR) & variance with 0.5 correction --------------------\na = df[\"int_death\"].to_numpy(dtype=float)\nb = df[\"int_alive\"].to_numpy(dtype=float)\nc = df[\"pla_death\"].to_numpy(dtype=float)\nd = df[\"pla_alive\"].to_numpy(dtype=float)\n\n# continuity correction if any cell is zero in that study\ncc = ((a == 0) | (b == 0) | (c == 0) | (d == 0)).astype(float) * 0.5\na += cc; b += cc; c += cc; d += cc\n\nlog_or = np.log((a * d) / (b * c))\nvar_or = 1 / a + 1 / b + 1 / c + 1 / d          # variance of log(OR)\n\n# --- 3. DerSimonian-Laird random-effects pooling ---------------------------\nres = combine_effects(log_or, var_or, method_re=\"dl\")   # DL estimator\n\nprint(\"Current statsmodels version:\", importlib.metadata.version(\"statsmodels\"))\n\nsf = res.summary_frame()          # still log-OR\nsf[\"eff\"]    = np.exp(sf[\"eff\"])\nsf[\"ci_low\"] = np.exp(sf[\"ci_low\"])\nsf[\"ci_upp\"] = np.exp(sf[\"ci_upp\"])\nprint(sf.round(3))\n\nCurrent statsmodels version: None\n                     eff  sd_eff  ci_low  ci_upp   w_fe   w_re\n0                  1.176   1.427   0.072  19.285  0.005  0.010\n1                  0.694   0.696   0.177   2.714  0.020  0.036\n2                  0.333   1.219   0.031   3.638  0.006  0.013\n3                  0.047   1.495   0.002   0.878  0.004  0.009\n4                  0.393   0.814   0.080   1.936  0.014  0.028\n5                  3.000   1.672   0.113  79.499  0.003  0.007\n6                  1.051   0.594   0.328   3.369  0.027  0.046\n7                  0.895   0.356   0.446   1.796  0.075  0.092\n8                  1.000   1.440   0.059  16.822  0.005  0.010\n9                  0.298   0.850   0.056   1.578  0.013  0.026\n10                 0.167   0.711   0.041   0.672  0.019  0.035\n11                 1.234   0.380   0.586   2.600  0.066  0.085\n12                 0.659   0.506   0.245   1.774  0.037  0.059\n13                 1.173   0.190   0.808   1.702  0.264  0.150\n14                 0.408   0.697   0.104   1.599  0.020  0.036\n15                 0.893   0.223   0.576   1.383  0.191  0.137\n16                 0.494   0.258   0.297   0.819  0.142  0.123\n17                 1.614   0.328   0.849   3.069  0.089  0.100\nfixed effect       0.861   0.098   0.711   1.042  1.000    NaN\nrandom effect      0.785   0.145   0.590   1.044    NaN  1.000\nfixed effect wls   0.861   0.121   0.678   1.092  1.000    NaN\nrandom effect wls  0.785   0.146   0.590   1.045    NaN  1.000\n\n\nLastly, we’ll need to plot the result as a Forest Plot on a log scale (always log scale with ratio variables)\n\nimport matplotlib.pyplot as plt\n\n# draw the default forest plot (no extra kwargs)\nfig = res.plot_forest(use_exp=True)\n\n# post-process the axis with Matplotlib\nax = fig.axes[0]          # the single Axes returned by plot_forest\nax.set_xscale(\"log\")\nax.set_xlim(0.03125, 32)\nax.set_xlabel(\"Odds Ratio (death)\")\nax.set_xticks([1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32])\nax.set_xticklabels(\n    [\"1/32\", \"1/16\", \"1/8\", \"1/4\", \"1/2\",\n     \"1\", \"2\", \"4\", \"8\", \"16\", \"32\"]\n)\n\nplt.title(\"Steroids OR for Death, Random-Effects Meta-analysis (DL)\")\nplt.tight_layout()\nplt.show()\n\n(0.03125, 32)\n\n\n\n\n\n\n\n\n\n\nAs before, Stata does not have a native notebook-style way to execute code within this webpage, so you’ll have to run the below script on your own machine to see the output.\nHere’s the steps:\n\n*-----------------------------------------------------*\n* 1. Set your_dir to wherever you downloaded the doc  *\n*-----------------------------------------------------*\ncd your_dir\n\n*-----------------------------------------------------*\n* 2. Import the Spreadsheet with Study Data           *\n*-----------------------------------------------------*\nclear\nimport excel \"Steroids PNa MA.xls\", sheet(\"Sheet1\") firstrow case(lower)\nlist study year int_death int_alive pla_death pla_alive\nlabel variable study \"Study\"\n\n*-----------------------------------------------------*\n* 3. Meta-analyze the studies using REML              *\n*-----------------------------------------------------*\nmeta esize int_death int_alive pla_death pla_alive, random(reml) esize(lnor) studylabel(study year) \n\n*-----------------------------------------------------*\n* 4. Create a forest plot with the result             *\n*-----------------------------------------------------*\nmeta forestplot, eform  nullrefline(lcolor(gs3)) esrefline(lcolor(gs3) lpattern(dash_dot)) title(\"Steroids OR for Death, Random-Effects Meta-analysis\")  xsize(11) ysize(7) xscale(log range(0.01 64)) xlabel(0.03125 \"1/32\" 0.0625 \"1/16\" 0.125 \"1/8\" 0.25 \"1/4\" 0.5 \"1/2\" 1 \"1\" 2 \"2\" 4 \"4\" 8 \"8\" 16 \"16\" 32 \"32\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Statistical Software</span>"
    ]
  },
  {
    "objectID": "intro_stats.html#resources",
    "href": "intro_stats.html#resources",
    "title": "5  Statistical Foundations",
    "section": "\n5.4 Resources",
    "text": "5.4 Resources\nVisualizations: - https://sites.google.com/view/ben-prytherch-shiny-apps/shiny-apps\n(of compatable intervals)\nBayesian Framework:\nLikelihoodist aka Evidential framework - https://www.atsjournals.org/doi/abs/10.1164/rccm.202504-0809TR",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical Foundations</span>"
    ]
  },
  {
    "objectID": "project_management.html#key-figures",
    "href": "project_management.html#key-figures",
    "title": "7  Getting your project done",
    "section": "",
    "text": "Use vector graphics such as eps or pdf. These scale properly and do not look fuzzy when enlarged. Do not use jpeg, bmp or png files as these will look fuzzy when enlarged, or if saved at very high resolutions will be enormous files. Jpegs in particular are designed for photographs not statistical graphics.\nUse readable fonts. For graphics I prefer sans-serif fonts such as Helvetica or Arial. Make sure the font size is readable after the figure is scaled to whatever size it will be printed.\nAvoid cluttered legends. Where possible, add labels directly to the elements of the plot rather than use a legend at all. If this won’t work, then keep the legend from obscuring the plotted data, and make it small and neat.\nIf you must use a legend, move it inside the plot, in a blank area.\nNo dark shaded backgrounds. Excel always adds a nasty dark gray background by default, and I’m always asking authors to remove it. Graphics print much better with a white background. The ggplot for R also uses a gray background (although it is lighter than the Excel default). I don’t mind the ggplot version so much as it is used effectively with white grid lines. Nevertheless, even the light gray background doesn’t lend itself to printing/photocopying. White is better.\nAvoid dark, dominating grid lines (such as those produced in Excel by default). Grid lines can be useful, but they should be in the background (light gray on white or white on light gray).\nKeep the axis limits sensible. You don’t have to include a zero (even if Excel wants you to). The defaults in R work well. The basic idea is to avoid lots of white space around the plotted data. \nMake sure the axes are scaled properly. Another Excel problem is that the horizontal axis is sometimes treated categorically instead of numerically. If you are plotting a continuous numerical variable, then the horizontal axis should be properly scaled for the numerical variable. \nDo not forget to specify units. \nTick intervals should be at nice round numbers.\nAxes should be properly labelled.\nUse linewidths big enough to read. 1pt lines tend to disappear if plots are shrunk. \nAvoid overlapping text on plotting characters or lines.\nFollow Tufte’s principles by removing chart junk and keeping a high data-ink ratio.\nPlots should be self-explanatory, so include detailed captions.\nUse a sensible aspect ratio. I think width:height of about 1.6 works well for most plots.\nPrepare graphics in the final aspect ratio to be used in the publication. Distorted fonts look awful.\nUse points not lines if element order is not relevant.\nWhen preparing plots that are meant to be compared, use the same scale for all of them. Even better, combine plots into a single graph if they are related.\n\nAvoid pie-charts. Especially 3d pie-charts. Especially 3d pie-charts with exploding wedges. I promise all my students an instant fail if I ever see anything so appalling.\nVisualizations\n\n\n\n\n\n\n\nfind attribution.\n\n\n\n\n\n\nfind attribution\n\n\n\n7.1.1 Introduction\nFormulating Hypothesis - specific, testable. Create a sheet synopsis with collaborators, roles, and author status\nStart with hypothesis,\nThen create a background with introduction structure to justify the study Checklist\n\n\nWHAT IS THE GENERAL TOPIC AREA OR BROAD ISSUE YOUR RESEARCH ADDRESSES?\n\n\nWHAT IS THE CRITICAL GAP IN THE LITERATURE YOUR RESEARCH ADDRESSES?\n\n\nWHAT IS THE NEXT STEP NEEDED TO ADDRESS THE CRITICAL GAP?\n\n\nWHAT IS YOUR RESEARCH QUESTION AND HYPOTHESIZED OUTCOMES (SPECIFY YOUR INDEPENDENT AND DEPENDENT VARIABLES)?\n\n\nWHAT IS THE BEST TITLE FOR THIS RESEARCH?\n\n\n\nWHAT IMPORTANT BACKGROUND INFORMATION DOES A NAÏVE READER NEED TO KNOW (INCLUDING TERMINOLOGY, ACRONYMS) TO UNDERSTAND WHAT YOUR RESEARCH DOES, WHY IT IS IMPORTANT, AND THE CRITICAL GAP YOU WILL ADDRESS)\n\nINTRODUCTION checklist:\n\n\nIs there a statement of theory, what is known, current state of the field (i.e. context of the study subject)?\n\n\nIs there a statement of what is unknown, the problem, and the crucial missing link?\n\n\nIs relevant background clearly described with all pertinent literature?\n\n\nIs there a clear goal of the planned research that will address an existing gap in ability or knowledge?\n\n\nAre there specific questions/issues to be addressed clearly identified and stated?\n\n\nIs it clear what will be known at the conclusion of the proposed research?\n\n\nIs it clear why that knowledge is important?\n\n\nIs the innovation, or unique contribution of this study or proposal clear?\n\n\n\n7.1.2 Methods: \nSubsections: Design, sampling, intervention, data collection, data analysis\nIn principle, make it so that it’s possible to replicate. \nMake sure info re: compliance to regulatory boards (e.g. IRB)\nStatistics pitfalls checklist: https://biostat.app.vumc.org/wiki/Main/ManuscriptChecklist \nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5994954/\nBox 1. Design Considerations for EHR Based Studies\n\nWhere in the health system are the data collected?\nWhat is the coverage/catchment area of your health system?\nIs the patient population receiving care across multiple institutions/centers?\nDo the data constitute different catchments? (Admixture\nHow are you defining exposures and outcomes? (Phenotyping)\n\nHow are you defining person-time?\n\nWhat is an appropriate ‘burn-in’ period to define a cohort\nIs a ‘burn-out’ period necessary to define censoring\n\n\nDo different populations produce more information (i.e. sicker patients have more encounters)?\n\n\nDo the methods and analyses follow from the introduction, theoretical framework, and goals of the study?\n\nParticipants:\n\nWill the subject groups permit a test of the aims/hypotheses?\n\n\n\nWhat is the evidence that the subjects are appropriate categorized for the study?  How are the subjects selected and identified?\n\n\n What is the evidence that the planned number of subjects is adequate?\nProcedures:\n\n\nAre the methods suited to the study goals/aims?\n\n\n\nAre adequate controls for bias and other confounding factors implemented?\nAre the procedures described in adequate detail that the experiment could be duplicated?\nAre the independent and dependent variables clearly identified and defined?\nDo the use of figures or tables improve understanding of procedure equipment and procedures conducted?10) Are the statistical analysis procedures justified and considered appropriate for addressing the aims/hypotheses?  11) Are references cited to support proposed methods, particularly those with precedence in the literature\n\n\nRESULTS:\n\n\nAnswer question. Number of subjects. Include p-value and confidence intervals. This should be longest. \nMake sure that figures and tables are not generally duplicative. Summarize in the text. \n-worthwhile to double check the figures, and do the figure legends adequately explain the figure (stand alone), which includes all acronyms. \n This description begins with a detailed review of selection and retention of study subjects in comparison with the source population they are meant to represent, data collection methods, and opportunities for confounding, selection bias and measurement error\n\n\n\n\n1) Are the results presented in a factual and logical manner (i.e. from most to least important findings or from simple to complex findings, first to last hypothesis)?\n\n\nAre the results reported in a sequence that is aligned with the goals/hypotheses of the study?\nAre adequate tables and figures utilized to enable the reviewer to make independent conclusions without reading the statistical findings?\nAre tables and figures essential?  Or could the information provided in the tables and figures better be summarized within the text?\nAre the tables and graphs easy to understand and interpret?\nAre the same data illustrated in graphics as in the text (i.e. unnecessary duplication of data presentation)\n\n\n\n7.1.3 Conclusions/Discussion\nattribution\n\nChopra - thoughts here: https://twitter.com/vineet_chopra/status/1537843032287870977?s=21&t=hs9Ti21F5xOY5rPLtrceug\nTake home message. \nhttps://discourse.datamethods.org/t/language-for-communicating-frequentist-results-about-treatment-effects/934\n“What is the story we want to tell?”\n-no new data\n-no rehash\n-Avoid causation If not designed for that\nShould explicitly say what our a-priori hypotheses were? We hypothesized a reduction in blood pressure and healthcare use that was “effect modified?” by adherence strata.  \n“Such assumptions involve mechanisms that have created the data and are related to sampling, measurement and treatment assignment (in observational studies common causes of factor and outcome) and, as a consequence, the bias this may produce”\nSelection bias arises from biased subject sampling, losses to follow-up, subject nonresponse, subject selection\nafter susceptibles have left the pool of subjects and other mechanisms.\n\n\n“such as where more accurate measurements, validation studies or more confounder measurements are needed. We recognize that general calls for further research are of little utility, but these specific avenues for further research are a directproduct of bias analysis, so somewhat different from general statements.”\n“For instance, to account for missing values, approaches include likelihood based methods (3,4), Bayesian models (5), and multiple imputation (6,7); to account for measurement error or misclassification, likelihood based methods (8), Bayesian models (9), multiple imputation (10,11), and regression calibration (12) have been proposed; and to account for confounding one could apply matching, stratification (13), propensity score methods (14), and multivariable regression adjustment (15,16). “\nFor EHR based studies specifically: https://catalogofbias.org/biases/informed-presence-bias/\n\n\n\n\n\nDoes the discussion progress from a specific goal (e.g. reiterate the study goals) to a more general application of the findings?\n\n\n\nDoes the discussion summarize and interpret the findings related to the data collected and associated findings? Are all possible alternative explanations for the findings presented?\n\n\nAre patterns, principles, relationships identified and described relevant to the findings without inclusion of speculation?\n\n\nDoes the discussion explain how the findings related to hypothesized outcomes as well as relevant literature on the topic?\n\n\nAre the theoretical implications of the findings addressed?\n\n\nAre explanations provided regarding unexpected or expected findings as well as agreement or contradictions of the findings to relevant literature?\n\n\nAre applications of the findings identified or described?\n\n\nAre generalizations to other contexts, species, or conditions identified or described without overgeneralizing or making conclusions that do not follow from the data or methods?\n\n\nDoes the discussion identify limitations to the current work, or future work that is needed to resolve contradictions or to explain unexpected findings?\n\n\nIs there an explanation regarding the contribution of the findings to the broader topic or context to circle back to the big picture and gap in the literature being addressed?\n\n\n\n\nTitle:  As few words as possible. Active tense title that conveys what you found is ideal.\nInclude:  PICO OR SPICED\nSetting - if relevant\nPopulation - who was studied\nIntervention - independent variable\nCondition - patients, as well as modifiers\nEndpoints - what was measured\nDesign per CONSORT; then study type\nCut down to ~12 words, lead with most important terms\nhttps://x.com/scientistswrite/status/1699080043337503133?s=46&t=5eJ6uoTQrbbYTlHIOnRYRg\n3 types: \n-interrogative title: states research question, may be sensationalist (e.g. editorials)\n-declarative: states main findings  - may come off as biased, overly definitive\n-neutral: describe the study without stating causal relationship (most common). \nAvoid acronyms. \nTITLE:  Does the title concisely reflect the article’s main focus?\n***Example 1***:  Effectiveness of Cloposcopic Cervical Screenings\n\n***Example 2***:  Prevalence of thyroid disorders in obstetrics patients\n\n***Example 3***:  Chronic obstructive pulmonary disease: molecular and cellular mechanisms\n\n***Example 4***:  Gastrointestinal Lymphoma:  Radiologic-Pathologic Correlation\n\n7.1.4 Abstract\n\n7.1.5 \nAbstract: Do last. generally 200-300 words. 2nd most read part (to title). End with the strongest possible statement of the major findings  but don’t overstate results. Have external reviewer to look for key information that didn’t make it in to the abstract (or vice versa)\nKnowing the magnitude and standard error of an empirical estimate is much more important than simply knowing the estimate’s sign and whether it is statistically significant. Yet, we find that even in top journals, when empirical social scientists choose their headline results - the results they put in abstracts - the vast majority ignore this teaching and report neither the magnitude nor the precision of their finding\n\n7.1.6 \nSubmission:\nPublishing Process:\nFiguring out where to publish: \nhttps://jane.biosemantics.org/\nhttps://www.scimagojr.com/ \nJournal Selection: \n-Do they publish stuff like this? (Shortcut is to assess what journal you are citing from)\nImpact factor vs likelihood of acceptance\n-Aim high - shoot for the prestigious journal that you might have a realistic chance at. \n\nConsiderations: \n-read author instructions carefully (formats, tables/figure numbers, different categories of submission. Will save time in the long term (avoiding). \nHow to disseminate clinical research: doi:10.1111/anae.15781\n—&gt; choosing journal based on social media promotion and presence in addition to IF (which reigns because some institutions tie promotions to this)\n—&gt; see if your institution or funder will pay for open access publication\n—&gt; ask people for promotional opportunities\nManuscript checklist:\nGood manuscript checklist: https://www.wemjournal.org/article/S1080-6032(22)00161-2/fulltext \nCover letter: -Author agreements -Not being considered for publication elsewhere -Description, novelty, and significance of the findings. -Special considerations (if there might be confusion about what is new and what isn’t, etc.) -Select reviewers / associate editors (who might have expertise, include this - why you suggested them) - try to take advantage of this.\nhttps://twitter.com/vineet_chopra/status/1483855989652656128?s=21\nResponding to reviewers: Sleep on the manuscript for a week, then look at it with fresh eyes. Rejections: incorporate feedback and don’t be discouraged. -cool off for a day off, then look at the reviewer comments to look at comments First Decision (minor revision, major revision, reject w full review, reject w/o review e.g. by editor/assoc editor). Second decision (accept v reject - still mostly head to reject).\nHow to not screw up a request for revision: -clear and comprehensive response (respectful of reviewers). Do it, point by point. Take as much space as needed to clarify the revision. -don’t make them look for revisions: make it as easy as possible for them to see if you’ve addressed (“often cut and paste the revised statements right under the bullet point”) -if you disagree - rebuttal should be detailed and can include citation -if question about perceived novelty and significance of the findings: this is an especially important to respond to.\nStatistical issues: https://discourse.datamethods.org/t/reference-collection-to-push-back-against-common-statistical-myths/1787\nSupplementary: Is that material a useful adjunct? Is it worth including in the main paper?\n\n7.1.7 Post-publication/submission: \nVisual abstract: \nhttps://doi.org/10.1136/bmjebm-2023-112784 RIVA checklsit\nTemplates at https://simplesage.io/resources \nFind phi https://github.com/jacobpstein/pii#readme\nGuide to licenses: https://opensource.org/licenses\nWhere to share data after: - https://zenodo.org/records/7946938",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Getting your project done</span>"
    ]
  },
  {
    "objectID": "intro_regression.html#what-type-of-regression-is-needed",
    "href": "intro_regression.html#what-type-of-regression-is-needed",
    "title": "6  Intro to Regressions",
    "section": "\n6.3 What type of regression is needed?",
    "text": "6.3 What type of regression is needed?\nOutput variable (aka the dependent variable, predicted variable) form determines the type of regression : \n\n\n\n\n\n\n\n\nLevel of measurement of outcome variable\nTwo Independent Groups without Confounding Adjustment\nTwo Independent Groups without Confounding Adjustment\n\n\nDichotomous\nChi2 Test\nlogistic regression\n\n\nUnordered categorical\nChi2 Test\nmultinomial logistic regression\n\n\nOrdered categorical\nWilcoxon-Mann-Whitney\nordinal logistic regression\n\n\nContinuous (normally distributed)\nT-test\nlinear regression\n\n\nCensored: time to event\nLog-rank test\nCox regression\n\n\n\nFrom: From: Stoddard GJ. Biostatistics and Epidemiology Using Stata: A Course Manual. Salt Lake City, UT: University of Utah School of Medicine.\nInterpretation:\nRegression coefficient = What change in the outcome do you expected if you change the predictor by 1 unit, holding all other variables constant\n\nFor linear regression: additive change in outcome\nFor logistic regression: multiplicative change in odds of the outcome\nFor Cox regression: multiplicative change in the hazard of the outcome. \n\nExample:\nConsider, if we want to test whether ‘splenectomy’ and ‘got_cteph?’ are associated, we could use a chi2 test:\n\n#chi2_test_result &lt;- chisq.test(darren_data_sheet$splenectomy, darren_data_sheet$`got_cteph?`)\n#print(chi2_test_result)\n\nAlternatively you could specify a logistic regression\n(“GLM” standards for ‘general linear model’. Logistic regression is a type of glm where the family is binomial)\n\n#logistic_model &lt;- glm(`got_cteph?` ~ splenectomy, data = darren_data_sheet, family = binomial())\n\n# Output the summary of the model to see coefficients and statistics\n#summary(logistic_model)\n\n\n\nCausal relationship of Splenectomy and CTEPH\n\n(https://www.dagitty.net/dags.html Daggity is a tool to specify such diagrams)\n\n#logistic_model_updated &lt;- glm(`got_cteph?` ~ splenectomy + prox_v_dist, data = darren_data_sheet, family = binomial())\n#summary(logistic_model_updated)\n\n\n\nCausal diagram of Splenectomy, Prox_v_dist, and CTEPH\n\nConsider: do you want the adjusted or the unadjusted estimate? \nHint: it depends….\nDistributions:\n\nRegressions -\n[ ] create the linear regression interpretation and example.\nlogic of different choices… ie. “under the following assumptions this is the estimate” - thus, if you make different assumptions, you make a different answers. Therefore, knowing the assumptinos are very important for knowing whether the result is believable.\nModels - you choose a way to distill the relationships that are contingent on certain assumptions - and if those assumptions hold, your conclusions follow.\nDAGs What does it mean to control? https://idlhy0218.github.io/page%20building/blog.html#control\nConceptually - you are modeling something… all models are wrong, some models are useful. You just need to know the assumptions you are implying by your choice, so that you can make an argument about whether the assumptions are warranted or not .\nFunctional forms *** dichotomization vs flexible models\nWhat type of questions can regression be used? - controlling for the effect of one thing on another. - prediction\n— how to do in python: python resources for how to do that section of it: https://ajthurston.com/predprobs?utm_source=substack&utm_medium=email",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Regressions</span>"
    ]
  },
  {
    "objectID": "data.html#exploratory-data-analysis",
    "href": "data.html#exploratory-data-analysis",
    "title": "3  Data",
    "section": "\n3.4 Exploratory Data analysis:",
    "text": "3.4 Exploratory Data analysis:\nPackages to expedite:\nR - https://cosimameyer.com/post/exploratory-data-analysis-in-r/?utm_source=substack&utm_medium=email\nPython - https://cosimameyer.com/post/exploratory-data-analysis-in-python/?utm_source=substack&utm_medium=email",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#additional-references",
    "href": "data.html#additional-references",
    "title": "3  Data",
    "section": "\n3.5 Additional references:",
    "text": "3.5 Additional references:\nhttps://johnborghi.github.io/Supporting_Scientific_Data/\nYoutube video from Dahly - https://www.youtube.com/watch?v=Ry2xjTBtNFE&t=145s&utm_source=substack&utm_medium=email\nData cleaning resources: https://www.anyamemensah.com/blog/recoding-cols-python?utm_source=substack&utm_medium=email",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "intro_regression.html#what-can-regression-models-do-for-you",
    "href": "intro_regression.html#what-can-regression-models-do-for-you",
    "title": "6  Intro to Regressions",
    "section": "",
    "text": "Descriptive: ““For every extra inch of height, how much heavier is someone on average (after adjusting for age and sex)?” (Goal: summarize patterns you’ve already observed)\nPredictive: “If I meet a 40-year-old man who is 5′10″, how much should I expect them to weigh?” (Goal: make reliable guesses about unseen data..)\nCausal: “If I became two inches taller, how would my weight change?” (Goal: Test a hypothesis about cause-and-effect.)\n\n\n\n\n\n\n6.1.1 When should you adjust your estimate?\nTo understand when “adjusting” for other variables helps and when it doesn’t, it’s essential to distinguish clearly between three related concepts: confounding, effect modification (or, similarly, interactions), and mediation. All three describe ways a third variable can influence the relationship between an Exposure (E) and an Outcome (O).\nWe’ll use Directed Acyclic Graphs (DAGs) to illustrate these concepts. DAGs visually represent your assumptions about how variables relate to each other. These are assumptions you assume for the purpose of your analysis, rather than directly test. You can create DAGs using tools like Dagitty (www.dagitty.com), the dagitty/diagrameR packages (R), dag (STATA) or Python’s Graphviz package.)\n[ ] DAG. - DAG references: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7124493/ \n\n6.1.1.1 Confounder (C)\nA confounder is a variable that is associated with the exposure, precedes it, and also independently affects the outcome.\nConsider this example: you’re interested in how the severity of hypoxemia in acute respiratory failure influences mortality risk (a causal question).\n\nExposure (E) = severity of hypoxemia (P/F ratio)\nOutcome (O) = risk of death at 28-days\nConfounder (C) = illness severity (e.g. APACHE II Score)\n\nFor example, consider the risk of death in patients with elevated PaCO2 levels. Say that older patients are more likely to get hypercapnia. In an unadjusted analysis, PaCO2 levels might be associated with death because age is associated with likelihood of hypercapnia, and likelihood of death via a variety of mechanisms. In this case, what we’d really like to do is evaluate whether PaCO2 levels remain associated with the risk of death after controlling for age (a confounder).\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom matplotlib import style\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport statsmodels.formula.api as smf\nimport graphviz as gr\nfrom linearmodels.iv import IV2SLS\nfrom IPython.display import display, SVG, HTML\n\npd.set_option(\"display.max_columns\", 5)\nstyle.use(\"fivethirtyeight\")\n\n\ng = gr.Digraph()\n\ng.edge(\"ability\", \"educ\")\ng.edge(\"ability\", \"wage\")\ng.edge(\"educ\", \"wage\")\n\nsvg = g.pipe(format=\"svg\").decode(\"utf-8\")  \nHTML(svg)\n\nability\nabilityeduc\neducability-&gt;educ\nwage\nwageability-&gt;wage\neduc-&gt;wage\n\n\n\n\ng = gr.Digraph()\n\ng.edge(\"ability\", \"educ\")\ng.edge(\"ability\", \"wage\")\ng.edge(\"educ\", \"wage\")\ng.edge(\"instrument\", \"educ\")\ng\n\n&lt;graphviz.graphs.Digraph object at 0x157dae990&gt;\n\n\n\n6.1.1.2 Mediator (M)\nImagine you are starting someone on home bilevel positive airway pressure (BPAP) for chronic hypercapnic respiratory failure and you are interested in assessing how much it reduces their risk of readmission. Let’s also say that you measure how much the PaCO2 level improves 1 month after starting BPAP. One might be tempted to put PaCO2 at 1 month into the regression model because we know that PaCO2 influences readmission risk.\nHowever, change to PaCO2 during follow-up may be part of how BPAP reduces risk: if re-establishing a normal PaCO2 set point is an indicator of effective nocturnal ventilation. Thus, if adjust for change in PaCO2, you’re asking “What is the association between BPAP and readmissions independent of PaCO2 change”? This might be an interesting question - and is termed what is the ‘direct effect’ of BPAP. However, if what you’re interested in is the overall, or ‘total effect’ - e.g. to answer whether a patient should be placed on BPAP - you wouldn’t want to remove that ‘indirect effect’.. you’re interested in quantifying the benefit, whether or not it’s mediated by change in PaCO2 levels.\n[ ] mediator example\n\n6.1.1.3 Effect Modifier\nEffect modifiers refer to situations where a third variable influences the effect of the exposure on the outcome. Technically, effect modifiers refer to characteristics of the treatment that are, in principle, changable while interactions refer to the same concept that is not changeable.\nAs a hypothetical example, a bacterial pathogen is presumably an effect modifier for response to antibiotics on community acquired pneumonia: if bacterial CAP, antibiotics help, and if viral it probably doesn’t.\nThis might seem like a very distinct concept from mediators and confounders - and it is. But the reason I bring it up here is that it’ll appear similar in regression analyses if not specified. In fact, there’s no way ‘just from the data’ that you can differentiate between an Effect Modifier, a mediator, and a confounder - you must reason through what you think the relationship is between each of the variables and then treat them appropriately.\nNote: there is not a uniformly agreed upon way to represent an effect modifiers in DAG, though there are proposals (see ***).\nOne last note on effect modifiers and interactions: they are difficult to estimate in the data. This is because, in practice, interactions/effect modififiers are usually smaller than the main effects we aim to detect AND because we’re trying to isolate signal from noise of two (rather than 1) sources. This consequences of this is that it often takes a much largers ample to convincingly detect an interaction. However, if you have a reason to suspect one exists, you should probably include it in your DAG and modeling strategy regardless.\n\n6.1.2 So, what should go in a regression model?\nExtrapolating from the points made above, clearly sometimes the right thing to do is not include anything in a regression model (ie. just present the unadjusted effect estimate).\nOn the other end of the spectrum, it is rarely correct to “put everything” into the regression model. This is true for a few reasons:\n\nIt’s tricky to understand what you’re then estimating\nYou can introduce biases by adjusting for a collider: something that is an effect of two or more variables in the model. E.g. You can get into the NBA by being tall or a good shooter, so - if you look at NBA players (ie conditioning on NBA team membership), you’ll see an inverse relationship even if there is no association in the general population.\nUnless your dataset has many more observations than variables, you’ll overfit the data.\n\n[ ] collider example\nTHus, there’s guidance from the AJRCCM editors (and other documents in other fields) that generally recommend something like this:\n\nBefore looking at the values in your data, sit down and draw a DAG of how you think your variables (and variables that you wish you could measure, even if you can’t) are causally related. www.dagitty.net can be used for this\nConsider what it is that you want to estiamte - ie. the estimand: are you interested in the total effect? the direct effect? an unadjusted effect (ie for prediction), or an effect adjusting for all relevant confounders (ie. for causal effect detection?)\nLook for the paterns that identify the above relationships\nThen, come up with a list of the confounders you want to adjust for, whether you want to account for mediation (usually, no), and whether there are interactions or effect modifiers. And, to the extent possible, avoid unintended collider stratification.\n\nSo, in our example, we’d like to control for age and baseline paCO2, but not change (or follow-up PaCO2)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Regressions</span>"
    ]
  },
  {
    "objectID": "intro_regression.html#sec-reg_purpose",
    "href": "intro_regression.html#sec-reg_purpose",
    "title": "6  Intro to Regressions",
    "section": "",
    "text": "Purpose\nExample Question\nGoal\n\n\n\nDescriptive\n“For every extra inch of height, how much heavier is someone on average (after adjusting for age and sex)?”\nSummarize patterns you’ve already observed\n\n\nPredictive\n“If I meet a 40-year-old man who is 5′10″, how much should I expect him to weigh?”\nMake reliable guesses about unseen data\n\n\nCausal\n“If I became two inches taller, how would my weight change?”\nTest a hypothesis about cause-and-effect\n\n\n\n\n\n6.1.1 When should you adjust your estimate?\nTo understand when “adjusting” for other variables helps and when it doesn’t, it’s essential to distinguish clearly between three related concepts: confounding, effect modification (or, similarly, interactions), and mediation. All three describe ways a third variable can influence the relationship between an Exposure (E) and an Outcome (O).\nWe’ll use Directed Acyclic Graphs (DAGs) to illustrate these concepts. DAGs visually represent your assumptions about how variables relate to each other. These are assumptions you assume for the purpose of your analysis, rather than directly test. You can create DAGs using tools like Dagitty (www.dagitty.com), the dagitty/diagrameR packages (R), dag (STATA) or Python’s Graphviz package.)\nDAG reference: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7124493/ \n\n6.1.1.1 Confounders\nA confounder (abbreviated, C) is a variable that is associated with the exposure, precedes it, and also independently affects the outcome.\nConsider this example: you’re interested in how the severity of hypercapnia during acute hypercapnic respiratory failure influences readmission risk (a causal question).\n\n\n\n\n\n\n\nRole\nVariable\nDescription\n\n\n\nExposure (E)\nSeverity of hypercapnia\nHigher PaCO₂ indicates more severe hypercapnia\n\n\nOutcome (O)\nReadmission risk\nWhether the patient is readmitted within 28 days\n\n\nConfounder (C)\nDisease severity\nOverall illness severity (e.g., FEV₁)\n\n\n\nUsing the python GraphViz package, this DAG can be visualized as\n\ng = gr.Digraph()\n\ng.edge(\"Disease Severity (C)\", \"Hypercapnia Severity (E)\")\ng.edge(\"Disease Severity (C)\", \"Readmission Risk (O)\")\ng.edge(\"Hypercapnia Severity (E)\", \"Readmission Risk (O)\")\n\nsvg = g.pipe(format=\"svg\").decode(\"utf-8\")  \nHTML(svg)\n\nDisease Severity (C)\nDisease Severity (C)Hypercapnia Severity (E)\nHypercapnia Severity (E)Disease Severity (C)-&gt;Hypercapnia Severity (E)\nReadmission Risk (O)\nReadmission Risk (O)Disease Severity (C)-&gt;Readmission Risk (O)\nHypercapnia Severity (E)-&gt;Readmission Risk (O)\n\n\n\nIn this example, you can imagine an example where there appears to be an association between hypercapnia severity and risk of readmission, even if - hypothetically - there isn’t one, if patients who have more severe lung disease tend to have higher PaCO₂ levels. Thus, to isolate the effect of hypercapnia itself, you need to account for the severity of lung disease.\n\n6.1.1.2 Mediators\nNext, consider CO₂ narcosis (altered mental status from severe hypercapnia). Should we adjust for CO₂ narcosis when estimating the impact of hypercapnia on outcomes like readmission risk?\nCO₂ narcosis clearly meets some criteria for adjustment: it’s associated with both hypercapnia (the exposure) and readmission risk (the outcome). But, CO₂ narcosis doesn’t cause hypercapnia; it’s a consequence of it. This makes CO₂ narcosis a mediator (abbreviated M), rather than a confounder.\nA mediator is a variable that is on the causal pathway between exposure and outcome. Mediators never cause the exposure, nor do they result from the outcome. Instead, they represent a step through which the exposure affects the outcome.\n\n\n\n\n\n\n\nRole\nVariable / Example\nDescription\n\n\n\nExposure (E)\nSeverity of hypercapnia\nBaseline PaCO₂ level on admission\n\n\nMediator (M)\nCO₂ narcosis (altered mental status)\nDeveloped after hypercapnia; lies on path between E and O\n\n\nOutcome (O)\nReadmission risk\nWhether the patient is readmitted within 28 days\n\n\n\n\ng = gr.Digraph()\n\ng.edge(\"Hypercapnia Severity (E)\", \"Altered Mental Status (M)\")\ng.edge(\"Altered Mental Status (M)\", \"Readmission Risk (O)\")\ng.edge(\"Hypercapnia Severity (E)\", \"Readmission Risk (O)\")\n\nsvg = g.pipe(format=\"svg\").decode(\"utf-8\")  \nHTML(svg)\n\nHypercapnia Severity (E)\nHypercapnia Severity (E)Altered Mental Status (M)\nAltered Mental Status (M)Hypercapnia Severity (E)-&gt;Altered Mental Status (M)\nReadmission Risk (O)\nReadmission Risk (O)Hypercapnia Severity (E)-&gt;Readmission Risk (O)\nAltered Mental Status (M)-&gt;Readmission Risk (O)\n\n\n\nIf we include CO₂ narcosis in our regression model, what happens?\nThink of it this way: hypercapnia may influence readmission risk in two distinct ways:\n\nIndirect pathway: Hypercapnia → CO₂ narcosis → readmission. This is the effect that is mediated through M)\nDirect pathway: Hypercapnia → readmission through other mechanisms (not involving M)\n\nIncluding CO₂ narcosis in the model removes (“controls out”) the indirect pathway. In other words, the regression result would represent only the effect of hypercapnia independent of CO₂ narcosis (the direct effect*).\nTypically, we’re interested in the entire impact of hypercapnia on outcomes, the total effect, which includes both direct and indirect pathways. Thus, you usually should avoid including mediators in your regression models.\n\n\n\n\n\n\nAdditional assumptions for valid mediation analysis\n\n\n\n\n\nIn the case where you really are interested in splitting apart the direct and indirect pathways, there are a few additional assumptions for a valid mediation analysis, collectively termed sequential ignorability. They include:\n\nNo unmeasured confounding for the relationships:\n\nE → O\nE → M\nM → O\n\n\nNo confounders of the M → O relationship are caused by E\n\n\n\n\nSimilarly, there are other variables which are neither confounders nor mediators (such as variables merely correlated with the mediator or variables that result from the outcome) that will cause confusion and yield misleading or uninterpretable results if they’re included in regression models.\n\n6.1.1.3 Effect Modifier\nAn effect modifier (often abbreviated Z) is a third variable that changes (modifies) the strength or direction of the relationship between an exposure (E) and an outcome (O).\nReturning to our example, imagine that the relationship between hypercapnia and readmission risk depends on the patient’s primary diagnosis:\n• In COPD, elevated PaCO₂ may cause persistent impairment of respiratory drive, increasing the likelihood of readmission despite therapy.\n• In Obesity Hypoventilation Syndrome (OHS), elevated PaCO₂ may respond effectively to nocturnal BPAP therapy, making readmissions less likely.\nThus, the impact of hypercapnia on readmission risk differs depending on whether the patient has COPD or OHS. In other words, the primary diagnosis modifies the effect of hypercapnia on readmission risk, making it an effect modifier.\n\n\n\n\n\n\n\nRole\nVariable\nDescription\n\n\n\nExposure (E)\nSeverity of hypercapnia\nPaCO₂ level at admission\n\n\nEffect modifier (Z)\nPrimary diagnosis\nCOPD vs OHS alters how PaCO₂ influences readmission risk\n\n\nOutcome (O)\nReadmission Risk\nWhether the patient returns within 28 days\n\n\n\n\ng = gr.Digraph()\n\ng.edge(\"Hypercapnia Severity\", \"Altered Mental Status\")\ng.edge(\"Altered Mental Status\", \"Readmission Risk\")\ng.edge(\"Hypercapnia Severity\", \"Readmission Risk\")\n\nsvg = g.pipe(format=\"svg\").decode(\"utf-8\")  \nHTML(svg)\n\n# Note: there is not a uniformly agreed upon way to represent an effect modifiers in DAG, though there are proposals\n\nHypercapnia Severity\nHypercapnia SeverityAltered Mental Status\nAltered Mental StatusHypercapnia Severity-&gt;Altered Mental Status\nReadmission Risk\nReadmission RiskHypercapnia Severity-&gt;Readmission Risk\nAltered Mental Status-&gt;Readmission Risk\n\n\n\n\n\n\n\n\n\nScale matters in effect modification\n\n\n\n\n\nWhether there’s effect modification depends on scale:\n\n\n\n\n\n\n\nScale\nQuestion answered\nPossible outcome\n\n\n\n\nRelative (e.g., risk ratio, odds ratio)\n“Does the proportional effect differ by subgroup?”\nSubgroups may differ on the RR scale but not on the additive scale.\n\n\n\nAdditive (e.g., risk difference)\n“Does the absolute effect differ by subgroup?”\nEffect modifiers can disappear (or appear) when you switch scales.\n\n\n\nThere is not consensus about which is preferred. Generalizing, relative effect modification better captures biologic interactions while absolute effect modification better captures the implications for decision-making. Always state the scale (relative effect modification vs. absolute effect modification) and remember that a modifier on one scale may vanish on another.\n\n\n\nWhat happens if you include an effect modifier as a regular predictor in your regression model?\nUnfortunately, you don’t capture the relationship you’re interested in, as the model would provide:\n• An estimate of how diagnosis (COPD vs OHS) influences readmission risk, after adjusting for PaCO₂. • A single, averaged estimate of how PaCO₂ influences readmission risk, after adjusting for the primary diagnosis.\nWhat you really want is to represent how the effect of PaCO₂ differs depending on the diagnosis: two distinct estimates rather than a single averaged effect. To accomplish that, you must explicitly include an interaction term (e.g., PaCO2 × diagnosis) in your regression model.\nBecause regression models represent effect modification through interaction terms, the terms effect modification and interaction are often used semi-interchangeably.\n\n\n\n\n\n\nInteractions need more power\n\n\n\n\n\nExpect to need much larger sample sizes (4-16 ×) to detect interactions with the same confidence you’d expect for main effects:\n\n\nInteractions are usually smaller than main effects.\n\nYou’re separating the signal from two variables at once (exposure × modifier), so standard errors inflate.\n\nEven so, you should include the interaction term in your plan if biological or clinical reasoning suggests effect modification… just remember that it’s not very strong evidence against effect modification if the interaction term is not statistically significant.\n\n\n\n\n6.1.2 So, what should go in your regression model?\nWe’ve discussed three ways that a third variable can influence the relationship between an exposure and an outcome:\n•   Confounding (C)\n•   Mediation (M)\n•   Effect Modification (Z)\nEach scenario changes the meaning of your regression results. Whether and how you include these variables depends entirely on what you’re trying to measure—your estimand—and your assumptions about how each variable fits into the causal structure.\nHere’s the critical takeaway: You can’t determine the role of variables just by looking at the data.*\nConfounders, mediators, and modifiers can all appear very similar in raw data. Instead, you must carefully reason through the role each variable plays in your scenario, clearly define your estimand, and then build a model that matches these assumptions. Directed Acyclic Graphs (DAGs) help you explicitly lay out and communicate these assumptions, clarifying your model-building decisions.\nThe need to explicitly define assumptions has important implications for how you should build regression models. Most importantly:\nDo NOT simply include all available variables in your model and hope for the best.\nEven with large datasets, including too many predictors without careful thought can obscure what your results actually mean, or worse, mislead you entirel. To emphasize the point clearly:\nDo NOT just throw everything into your regression model.\nTo drive this home, the next section (optional) introduces a fourth type of variable, called a collider, illustrating even more clearly why careful selection of variables matters.\n\n\n\n\n\n\nWhy do tall NBA players appear to be bad at free throws?\n\n\n\n\n\nThis surprising observation illustrates collider(-stratification) bias. A collider is a variable influenced by two (or more) other variables. Adjusting for (or restricting your analysis to a certain value of) a collider can create artificial associations that don’t exist in reality.\nExample: NBA Player Selection\n\nBeing tall → more likely to become an NBA player\n\nHaving excellent free-throw accuracy → more likely to become an NBA player\n\nThus, “NBA player” is a collider—it results from both height and shooting accuracy:\n\ng = gr.Digraph()\n\ng.edge(\"Being very tall\", \"Drafted into NBA\")\ng.edge(\"Excellent free-throw accuracy\", \"Drafted into NBA\")\n\nsvg = g.pipe(format=\"svg\").decode(\"utf-8\")  \nHTML(svg)\n\nBeing very tall\nBeing very tallDrafted into NBA\nDrafted into NBABeing very tall-&gt;Drafted into NBA\nExcellent free-throw accuracy\nExcellent free-throw accuracyExcellent free-throw accuracy-&gt;Drafted into NBA\n\n\n\nWhen you look only at NBA players (essentially, ‘adjusting for’ the collider), you create an artificial, inverse relationship: it looks like taller players are poorer free-throw shooters, even if no real-world correlation exists. In reality, what you’re finding is: “there’s two ways to get into the NBA. So, folks who got in because they’re good shooters are probably less tall and people who got in because they’re tall are likely worse shooter.”\nTakeaway: Conditioning on a collider distorts reality. Carefully identify colliders in your DAG, and avoid adjusting for them.\nThis may seem niche, but collider-stratification is everywhere, once you look for it. The “obesity paradox” is one medical example. Consider ARDS.\nYou must be very hypoxemic to have ARDS. There are two reasons you might be hypoxemic:\n\nobesity, leading to atelectasis and shunting\nsevere lung injury\n\n\ng = gr.Digraph()\n\ng.edge(\"Obesity\", \"Hypoxemia\")\ng.edge(\"Lung Injury\", \"Hypoxemia\")\n\nsvg = g.pipe(format=\"svg\").decode(\"utf-8\")  \nHTML(svg)\n\nObesity\nObesityHypoxemia\nHypoxemiaObesity-&gt;Hypoxemia\nLung Injury\nLung InjuryLung Injury-&gt;Hypoxemia\n\n\n\nSo, if you look at people with ARDS (essentially, conditioning on hypoxemia), you expect a negative relationship between BMI and the degree of lung injury. When people unfamiliar with collider-stratification find that relationship, they look for explanations about “the protective effect of obesity”. Doubtful there is such a thing.\n\n\n\nWhat you should do instead (as endorsed by the editors of all the main PCCM journals) is before you even look at your data, sit down and think through the relationship between your exposure of interest and your outcome of interest and start creating a DAG.\n\nWhat things are confounders (even if you can’t measure them, including them in the diagram because it’ll help clarify what residual confounders will be left after you’ve constructed your best model, which will undoubtedly be imperfect)?\nConsider what the thing you’re interested in estiamting is (is it the total effect?), and what things are mediators?\nWhat things might modify the effect of interest?\n\nOnce your diagram is done, it implies what your ideal regression model might be. Include only the variables that seem important in your DAG. Then, you have to further consider the limitations of the data (ie. size, how well each can be measured), and decide which confounders can be adjusted for under the actual constraints in your project.\n\n\n\n\n\n\nHow does sample size limit what can be in a regression?\n\n\n\n\n\nThink of regression as trying to pull signal (the actual underlying relationships) out of noise (randomness from comes from only having a few, imperfectly measured data points).\nConsider the case where there’s one predictor, two points. With just two data points you can always draw a line (fit a model) that fits perfectly… but the “fit” is meaningless. Same thing occurs if there’s two predictors, three points. In three-dimensional space you can always fit a plane exactly through three points—for the same reason. In both cases, the perfect fit just reflects that if you give yourself enough degrees of freedom, you can perfectly fit your data, but it may not reflect the underlying trends. The model memorizes random noise instead of learning the underlying relationship.\nMany rules of thumb (and more rigorous appraoches) exist to determine how many predictors is too many.\nIt’s better to err on the side of including less.\n\n\n\nThen, finally, you can run your regression.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Regressions</span>"
    ]
  },
  {
    "objectID": "intro_regression.html#sec-regression_logic",
    "href": "intro_regression.html#sec-regression_logic",
    "title": "6  Intro to Regressions",
    "section": "\n6.2 The logic of using regression for causal inference",
    "text": "6.2 The logic of using regression for causal inference\nRecall that when we observe an association between an exposure and an outcome, there are four possible explanations:\n1.  Chance\n2.  Confounding (another factor influencing both exposure and outcome)\n3.  Bias (systematic error resulting from measurement or design)\n4.  Causation (a true effect)\nWhen we use regression for causal inference, we’re typically making an argument by exclusion (the disjunctive syllogism):\n\nIf chance, confounding, and bias can’t explain the association, it must be causal.\n\nThe probability of chance alone explaining an observed observation is the p-value. The plausibility that bias could explain it is addressed by the methods of the paper (how the study was designed and how the effects were measured).\nIn randomized studies, random assignment makes confounding improbable (if the sample is big enough). However, in observational (non-randomized) research, statistical adjustments to address the possibility of confounding are required. This is the most common use of regression.\nHowever, for regression to adequately adjust for confounders (ie. confounders are )\nRegression comes with additional assumptions: \n\nIndependent observations (special “mixed models” can relax this)\nThe form of the output variable is correct* \nThe form of the predictor variables are correct\nThe relationship between the predictors are properly specified.**\nAdditional constraints (e.g. constant variance)\n\nLastly, all potential confounders must be in the model (an assumption called ‘conditional exchangeability’)\nThus the logic is: if the assumptions of the models hold in reality, then the described relationships are valid\n\nNo model is perfect, but some models are useful\n\n\nMorris moment(TM)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Regressions</span>"
    ]
  },
  {
    "objectID": "intro_regression.html#sec-reg_type",
    "href": "intro_regression.html#sec-reg_type",
    "title": "6  Intro to Regressions",
    "section": "\n6.3 What type of regression is needed?",
    "text": "6.3 What type of regression is needed?\nOutput variable (aka the dependent variable, predicted variable) form determines the type of regression : \n\n\n\n\n\n\n\n\nLevel of measurement of outcome variable\nTwo Independent Groups without Confounding Adjustment\nTwo Independent Groups without Confounding Adjustment\n\n\nDichotomous\nChi2 Test\nlogistic regression\n\n\nUnordered categorical\nChi2 Test\nmultinomial logistic regression\n\n\nOrdered categorical\nWilcoxon-Mann-Whitney\nordinal logistic regression\n\n\nContinuous (normally distributed)\nT-test\nlinear regression\n\n\nCensored: time to event\nLog-rank test\nCox regression\n\n\n\nFrom: From: Stoddard GJ. Biostatistics and Epidemiology Using Stata: A Course Manual. Salt Lake City, UT: University of Utah School of Medicine.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Regressions</span>"
    ]
  },
  {
    "objectID": "intro_regression.html#sec-reg_interp",
    "href": "intro_regression.html#sec-reg_interp",
    "title": "6  Intro to Regressions",
    "section": "\n6.4 How do you interpret the regression results?",
    "text": "6.4 How do you interpret the regression results?\nRegression coefficient = What change in the outcome do you expected if you change the predictor by 1 unit, holding all other variables constant\n\nFor linear regression: additive change in outcome\nFor logistic regression: multiplicative change in odds of the outcome\nFor Cox regression: multiplicative change in the hazard of the outcome. \n\nExample:\nConsider, if we want to test whether ‘splenectomy’ and ‘got_cteph?’ are associated, we could use a chi2 test:\n\n#chi2_test_result &lt;- chisq.test(darren_data_sheet$splenectomy, darren_data_sheet$`got_cteph?`)\n#print(chi2_test_result)\n\nAlternatively you could specify a logistic regression\n(“GLM” standards for ‘general linear model’. Logistic regression is a type of glm where the family is binomial)\n\n#logistic_model &lt;- glm(`got_cteph?` ~ splenectomy, data = darren_data_sheet, family = binomial())\n\n# Output the summary of the model to see coefficients and statistics\n#summary(logistic_model)\n\n\n\nCausal relationship of Splenectomy and CTEPH\n\n(https://www.dagitty.net/dags.html Daggity is a tool to specify such diagrams)\n\n#logistic_model_updated &lt;- glm(`got_cteph?` ~ splenectomy + prox_v_dist, data = darren_data_sheet, family = binomial())\n#summary(logistic_model_updated)\n\n\n\nCausal diagram of Splenectomy, Prox_v_dist, and CTEPH\n\nConsider: do you want the adjusted or the unadjusted estimate? \nHint: it depends….\nDistributions:\n\nRegressions -\n[ ] create the linear regression interpretation and example.\nlogic of different choices… ie. “under the following assumptions this is the estimate” - thus, if you make different assumptions, you make a different answers. Therefore, knowing the assumptinos are very important for knowing whether the result is believable.\nModels - you choose a way to distill the relationships that are contingent on certain assumptions - and if those assumptions hold, your conclusions follow.\nDAGs What does it mean to control? https://idlhy0218.github.io/page%20building/blog.html#control\nConceptually - you are modeling something… all models are wrong, some models are useful. You just need to know the assumptions you are implying by your choice, so that you can make an argument about whether the assumptions are warranted or not .\n\n6.4.1 Functional Forms\nFunctional forms *** dichotomization vs flexible models\nWhat type of questions can regression be used? - controlling for the effect of one thing on another. - prediction\n— how to do in python: python resources for how to do that section of it: https://ajthurston.com/predprobs?utm_source=substack&utm_medium=email\n\nTODO:\n\npull in definition code.\nmake links automatically.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Regressions</span>"
    ]
  }
]